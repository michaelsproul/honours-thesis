\documentclass[]{unswthesis}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{multicol}
\usepackage{proof}
\usepackage{centernot}
\usepackage{backref}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}
\usepackage{csquotes}
\usepackage{underscore}
\usepackage{tabto}

% Nicely coloured links.
\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={blue!80!black},
    urlcolor={blue!80!black}
}

% Put boxes around figures? Maybe.
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}

% Abstract env.
\newenvironment{abstract}
 {
  \begin{center}
  \bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  \end{center}
  \list{}{
    \setlength{\leftmargin}{.5cm}%
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}

% Tick symbol.
\def\tick{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\newcommand{\cross}{$\times$}

%%% Class options:

%  undergrad (default)
%  hdr

%  11pt (default)
%  12pt

%  final (default)
%  draft

%  oneside (default for hdr)
%  twoside (default for undergrad)


%% Thesis details

% TODO: Choose a title...
% Re-usable lemmas for the formalisation of linearly-typed languages
% A library-based approach to context splitting
% Towards a framework for the mechanised verification of substructural type systems
% A library based approach to the verification of linear type systems
% Code: library, lemmas, definitions, engineering.
% Effective verification of linear types in Coq.
% Proof engineering for formalisations of languages with linear types
\thesistitle{A library based approach to the verification of languages with linear types}
\thesisschool{School of Computer Science and Engineering}
\thesisauthor{Michael Alexander Sproul}
\thesisZid{z3484357}
\thesistopic{3680}
\thesisdegree{Bachelor of Science (Honours)}
\thesisdate{May 2016}
\thesissupervisor{Dr. Ben Lippmeier}

%% My own LaTeX macros, definitions, etc.
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\newcommand{\case}{\text{ case }}
\newcommand{\of}{\text{of }}
\newcommand{\yields}{\multimap}
\newcommand{\steps}{\Rightarrow}
\newcommand{\lquine}{\left\ulcorner}
\newcommand{\rquine}{\right\urcorner}
\newcommand{\capa}{\text{cap}}
\newcommand{\ptr}{\text{ptr }}
\newcommand{\rgnUL}{$\lambda^\text{rgnUL}$\text{ }}
\newcommand{\SSPHS}{\text{SSPHS }}
\newcommand{\types}{\vdash}
\newcommand{\Francois}{Fran\frenchc{c}ois }
\newcommand{\lolly}{\multimap}

% Text formatting.
\let\frenchc\c
\let\c\texttt
\let\oldi\i
\let\i\textit
\let\oldb\b
\let\b\textbf

\begin{document}

%% Roman numeral numbering in this section.
\frontmatter
\maketitle

% Mention:
% DbLib, context splitting, DILL, L^3.

\begin{abstract}
We present a library of proofs to aid in the mechanical verification of languages with substructural type systems. We identify \i{context splitting} as a widely applicable technique in the verification of these languages, and argue that our proofs are relevant to any language that uses multiple contexts to separate linear assumptions from non-linear ones, in the style of Dual Intuitionistic Linear Logic. Our work is implemented using the Coq interactive theorem prover and builds upon \Francois Pottier's \c{DbLib} library for variable binding using De Bruijn indices. We provide a proof of type soundness for a Linear Lambda Calculus (LLC) and highlight how similar techniques can be applied to \i{The Linear Language with Locations}, $L^3$ -- which is representative of a class of more complex languages with destructive updates.
\end{abstract}

\tableofcontents

%% Chapter numbers and normal page numbering.
\mainmatter

\chapter{Introduction}
\label{ch:intro}

Computer systems form an integral part of modern society, both in the form of personal devices and critical infrastructure. Ensuring the correct operation of computer hardware and software is therefore a worthwhile endeavour. One emerging technique for the construction of robust software systems is the use of mathematical formalisations and proofs of correctness. In this paradigm, desirable properties of the software can be proven true using a computer-based \i{proof assistant}, which itself relies on a minimal amount of trusted code. For software formalisation and verification to be truly effective, the objects under consideration must have precise mathematical models associated with them. Typically these models are created based on the \i{semantics} (meaning) of the programming language that the software is written in. Unfortunately for the would-be software verifier, most popular programming languages lack formal semantics and are therefore not amenable to verification techniques.

The C Programming Language, in which large amounts of low-level \i{systems software} is written, is an example of a language that is difficult to reason about because of its murky semantics. According to the language specification the results of some operations are classified as \i{undefined behaviour}, meaning that the compiler has no obligation to produce a specific result (REF). Ideally, we would like to rule out undefined behaviour at compile time through the use of a type system. In this thesis we focus on the formal semantics of languages with strong type systems that are more well-suited to low-level software verification than C.

In particular, we are interested in \i{resource-aware} languages which exploit \i{substructural} type systems based on linear logic (REF) to track how values are consumed and destroyed. This is useful in the context of systems software as it can enable every piece of heap-allocated memory to be automatically de-allocated without the use of a garbage collector, for example, by performing de-allocation when the unique reference to the memory goes out of scope. We begin by discussing the Linear Lambda Calculus (LLC), which is a simple extension of $\lambda$-calculus with a linear type system. We then go on to describe more expressive languages that include features like shared pointers and destructive updates.

The \b{main contribution} of this thesis is a software library for the interactive theorem prover Coq. The library includes definitions and lemmas primarily about \i{context splitting}, which is a technique used in many substructural type systems. Our code is an extension of \Francois Pottier's \c{DbLib} library which aims to de-duplicate the effort required to reason about variable binding, just as we hope to de-duplicate the effort required to reason about linear typing.

As a proof-of-concept for our approach we have undertaken a proof of \i{type soundness} for the Linear Lambda Calculus. Type soundness is a key result in establishing the sensibility of a language and we argue that our approach is also applicable to soundness proofs for more complex languages with the same foundations. Specifically, we argue that a proof of soundness for \i{The Linear Language with Locations} ($L^3$) could make effective use of our library, and that $L^3$ is representative of languages that build upon the Linear Lambda Calculus by adding destructive updates and shared pointers.

\section{Coq and the Curry-Howard Correspondence}
\label{sec:curry_howard}

Recall that the Curry-Howard correspondence establishes an equivalence between logical systems and type systems for programming languages. By considering logical propositions as types, the proof of a proposition $P$ can be given by constructing a value of type $P$ in the equivalent programming language. The Coq proof assistant provides a dependently typed programming language with inductive data-types that allows complex propositions to be expressed and proved in this manner. Coq proofs make use of \i{tactics} which abstract over repetitive reasoning.

% Background.
\chapter{Background and Previous Work}
\label{ch:background}

\section{The Unsuitability of C}

TODO: Flesh this out (or delete it?).

Historically, low-level \textit{systems software} has been written primarily in the C programming language, which was originally designed without a formal semantics. Attempts to assign semantics to C have resulted in numerous impressive verification projects -- notably the CompCert verified compiler \cite{leroy09} and the seL4 micro-kernel \cite{klein14}.

\section{Overview of Linear and Affine Typing}

Linear, affine and uniqueness typing are closely-related features of type systems that enforce rules about the number of times values may be used and referenced. These restrictions are motivated by several desirable properties that can be obtained by enforcing them. The Clean programming language (introduced in \cite{clean87}) uses uniqueness typing to ensure that values in memory have at most one reference to them, thus enabling \i{destructive updates} whilst preserving referential transparency. The Rust programming language \cite{rustWeb} uses uniqueness typing to track and free heap-allocated memory, thus allowing it to achieve memory safety without garbage collection. This makes it suitable for writing systems software where a garbage collector isn't available, like a garbage collector itself, or an operating system.

At their core, all of these systems enforce their constraints using typing rules derived from linear logic \cite{girard87}. To get a feel for linear logic, we first turn our attention to the Linear Lambda Calculus. We then continue by discussing more complex languages and the techniques used in their mechanical formalisation.

\section{The Linear Lambda Calculus}

\begin{figure}[h]
\caption{Typing Rules for DILL Linear Lambda Calculus}
\begin{displaymath}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{cc}
\infer[\text{(Int-Var)}]{\Gamma, x : A; \emptyset \types x : A}{} &
\infer[\text{(Lin-Var)}]{\Gamma; x : A \types x : A}{} \\
\infer[\text{(Unit-I)}]{\Gamma; \emptyset \types * : I}{} &
\infer[\text{(Unit-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \text{let $*$ be $t$ in $u$} : A}
  {\Gamma; \Delta_1 \types t : I  &  \Gamma; \Delta_2 \types u : A} \\
\infer[\text{($\otimes$-I)}]
  {\Gamma; \Delta_1, \Delta_2 \types t \otimes u : A \otimes B}
  {\Gamma; \Delta_1 \types t : A  &  \Gamma; \Delta_2 \types u : B} &
\infer[\text{($\otimes$-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \text{let $x \otimes y : A \otimes B$ be $u$ in $t$} : C}
  {\Gamma; \Delta_1 \types u : A \otimes B  &  \Gamma; \Delta_2, x : A, y : B \types t : C} \\
\infer[\text{($\lolly$-I)}]
  {\Gamma; \Delta \types (\lambda x : A. t) : A \lolly B}
  {\Gamma; \Delta, x : A \types t : B} &
\infer[\text{($\lolly$-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types (u \; t) : B}
  {\Gamma; \Delta_1 \types u : A \lolly B  &  \Gamma; \Delta_2 \types A} \\
\infer[\text{(!-I)}]
  {\Gamma; \emptyset \types !t : !A}
  {\Gamma; \emptyset \types t : A} &
\infer[\text{(!-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \text{let $!x : A$ be $u$ in $t$} : B}
  {\Gamma; \Delta_1 \types u : !A  &  \Gamma, x : A; \Delta_2 \types t : B} \\
\end{array}
\end{displaymath}
\end{figure}

In classical and intuitionistic logic there are structural deduction rules equivalent to the following, called \i{Contraction} and \i{Weakening} \cite{wadler90, wadler93}.

\begin{eqnarray*}
\infer[\text{Contraction}]{\Gamma, A \vdash B}{
	\Gamma, A, A \vdash B
}
\qquad
\infer[\text{Weakening}]{\Gamma, A \vdash B}{
    \Gamma \vdash B
}
\end{eqnarray*}

Contraction allows duplicate assumptions to be discarded, whilst Weakening allows a non-vital extra assumption to be introduced from nowhere. Linear logic \cite{girard87} selectively restricts the use of both rules, such that every \i{linear} assumption is used \i{exactly once}. Affine logic on the other hand restricts only the use of Contraction, resulting in each linear assumption being used \i{at most once}. Both are \i{substructural} in that they restrict the use of structural rules.

\i{Non-linear} assumptions can still be used an unlimited number of times. Hence, variants of Contraction and Weakening that operate only on non-linear assumptions are present in linear logic, affine logic and similar substructural systems.

When transformed into typing rules (as per Section \ref{sec:curry_howard}), Contraction and Weakening take the form:

\begin{eqnarray*}
\infer[\text{Contraction}]{\Gamma, x : A \vdash u[x/y, x/z] :: B}{
	\Gamma, y : A, z : A \vdash u :: B
}
\qquad
\infer[\text{Weakening}]{\Gamma, x : A \vdash y :: B}{
    \Gamma\vdash y :: B
}
\end{eqnarray*}

Note that the use of substitutions in the rule for Contraction is required so that no variable appears more than once in a typing context $\Gamma$. Further, substructural type theories include context-splitting operations that allow the linear variables of a typing context to be split between two new contexts which can be used to type-check subterms.

As in linear and affine logic, linear type theory restricts the use of both Contraction and Weakening, whilst affine type theory restricts just the use of Contraction. Our previous observations about the number of times an assumption is used, now translate into observations about the number of times variables are used.

Without Contraction, \i{variables can only appear once in a term}. The dual substitution of $x$ for $y$ and $x$ for $z$ allows a term containing one $y$ and one $z$ to become a term containing two occurrences of $x$. None of the other rules of intuitionistic type theory allow this \cite{wadler93}.

Similarly, without Weakening, \i{all variables in the context must be used in the term}. This follows from the fact that Weakening introduces an unused variable to the context and no other rule of intuitionistic type theory allows this \cite{wadler93}.

From these two observations we can conclude that linear type theory requires all variables to be used \i{exactly once} in terms, whilst affine type theory requires all variables to be used \i{at most once} in terms.

As in substructural logics, substructural type theories selectively permit Contraction and Weakening for select non-linear values. This is quite practical in the context of programming language implementation as it allows trivially copyable values like integers to be easily duplicated. The mechanism employed by Wadler \cite{wadler93} involves differentiating between linear and non-linear assumptions, and adding type and value-level bang (!) operators to make types and values non-linear. The function for duplication of non-linear values (of type $!A$) is expressed as follows in Wadler's system:

\begin{eqnarray*}
\varnothing \vdash \lambda \langle x' \rangle . \case x' \of !x \rightarrow \langle !x, !x\rangle :\text{ } !A \yields (!A \otimes !A)
\end{eqnarray*}

See \cite{wadler93} for full notational details.

\section{Uniqueness Typing}

Although linear and affine type theory capture the essence of uniquely referenced values, they are insufficient to describe the concept of \i{uniqueness} as it appears in languages like Clean. In his 2007 paper, de Vries \cite{deVries07} notes that terms of a unique type should be \i{guaranteed to never have been shared}, which is sufficient to guarantee a unique pointer at runtime. In contrast, terms of linear (or affine) type are \i{guaranteed not to be shared in the future}, which is insufficient to guarantee a unique pointer.

The distinctness of linearity and uniqueness is further highlighted by the \i{dereliction} rule present in some versions of linear type theory, and the rule that we'll refer to as \i{uniqueness removal} present in Clean's type system. Let $\alpha^\odot$ and $\alpha^\otimes$ stand for linear and non-linear versions of a base type $\alpha$. Similarly, let $\alpha^\bullet$ and $\alpha^\times$ stand for unique and shared versions of a base type $\alpha$. We have:

\begin{eqnarray*}
(\lambda x. \; x) \; : & \alpha^\otimes \rightarrow \alpha^\odot & \text{(linear dereliction)}\\
(\lambda x. \; x) \; : & \alpha^\bullet \rightarrow \alpha^\times & \text{(uniqueness removal)}
\end{eqnarray*}

The dereliction rule allows a non-linear value to be transformed into a linear one. As noted by de Vries \cite{deVriesPhD08}, an analogous rule for unique types that converts shared values to unique ones cannot possibly be sound. The ``unique" value resulting from such a rule would not necessarily be unique because other shared references may still exist.

Conversely, uniqueness removal only makes sense in the context of uniqueness types. A unique value may sacrifice its uniqueness to become shared, but a linear value which models the existence of a single resource should not be transformed into an unlimited supply of that resource.

Note that dereliction need not form a part of type systems based on linear logic, and that it is absent from Wadler's presentations \cite{wadler90, wadler93} and all systems considered in the next sections. Further note that if uniqueness is to be exploited to make garbage collection unnecessary -- as in the case of Rust -- then the uniqueness removal rule is undesirable as it prevents values from having a unique owner.

Due to the non-equivalence of linearity and uniqueness, de Vries constructed a distinct set of semantics and typing rules to model Clean's type system \cite{deVries07}.

One key component of his approach is the use of the \i{kind} (type of types) system to track uniqueness and non-uniqueness. As in Haskell, de Vries' uniqueness system includes a kind for data (\c{*}) which is the kind of all inhabited types (and \c{Void}). In addition, there is a uniqueness kind $\mathcal{U}$ inhabited by two types $\bullet$ and $\times$ representing uniqueness and non-uniqueness respectively. A third kind, $\mathcal{T}$ is the kind of base types (like \c{Int}). These kinds are brought together by a type constructor $\c{Attr} ::_k \mathcal{T} \rightarrow \mathcal{U} \rightarrow *$ which applies a uniqueness attribute to a base type to form a type that is inhabited. For example, \c{Attr} $\bullet$ \c{Int} or $\c{Int}^\bullet$ is the type of uniquely referenced integers.

The other main technique employed by de Vries' model is the use of arbitrary boolean expressions as uniqueness attributes, with $\bullet$ as true and $\times$ as false. Clean's type system allows uniqueness polymorphism, which results in constraint relationships between uniqueness variables, which are represented in de Vries' system as simple boolean expressions that can be handled by a standard unification algorithm.

Importantly, de Vries' work includes the only known mechanical verification of a type system similar to Clean's. The formalisation uses the Coq proof assistant, and the \i{locally nameless} approach to variable naming that is discussed in Section \ref{sec:de_bruijn}.

% Linear + affine logic and control over contraction and weakening. DONE.

% Uniqueness typing as an alternative (mention dereliction, non guarantees). DONE.

% de Vries formalisation of Clean's uniqueness typing using attributes.

\section{Type Soundness}

Type systems for programming languages are said to be \i{sound} if well-typed programs are guaranteed not to get stuck when evaluated according to the language's operational semantics. Soundness is GREAT.

In this thesis, we aim to construct a mechanical proof of type soundness for $L^3$ using the Coq proof assistant. The proof will be carried out in the syntactic style of Wright and Felleisen \cite{wright94}.

\section{The Linear Language with Locations, $L^3$}

%the mechanical verification of a small language -- \i{The Linear Language with Locations}, $L^3$. Although formal semantics and semi-formal proofs of correctness exist for $L^3$, no computer-based proofs exist. We verify $L^3$ because it is typical of modern resource-aware calculi in its use of capabilities -- whilst remaining simple. $L^3$'s concepts of ownership make its mechanisation relevant to the complete verification of low-level systems like garbage collectors and operating systems, which are at the cutting edge of verification research.

The Linear Language with Locations $L^3$ \cite{ahmed05}, makes use of concepts from linear typing to manage \i{capabilities} -- values that represent the permission to read or write an area of memory. By treating capabilities linearly, pointers themselves become duplicable and can be stored in numerous locations freely.

$L^3$ is a \i{low-level} language by design and features primitive operations for allocating and deallocating memory. By virtue of linear capabilities, \i{strong updates} are supported, whereby the type of a value in a memory location may change upon writing. Strong updates enable staged value initialisation, and simulation of register type-changes throughout program execution. For example, a pointer initially pointing to an \c{Int} can be overwritten with a \c{Bool} and the type system can statically track this change.

$L^3$'s definition includes a description of syntax, operational semantics and typing rules. Notably, the operational semantics include a store type which maps locations to values. Similarly, the typing rules include a location context that tracks which locations are in scope.

\begin{eqnarray*}
(\sigma, \text{new } v) \Rightarrow (\sigma \uplus \{l \mapsto v\},
	\lquine l, \langle \capa, \ptr l \rangle \rquine)
\\
(\sigma, \text{let } \lquine \rho, x \rquine = \lquine l, v \rquine \text{in } e)
	\Rightarrow
	(\sigma, e[l/\rho][v/x])
\end{eqnarray*}

These two rules from the operational semantics show the behaviour of the \c{new} keyword for allocating memory, and the \c{let} construct for unpacking pointers and capabilities. Note how the store $\sigma$ is extended with a mapping from a new location $l$ to the value $v$ in the case of the rule for \c{new}. The $\lquine l, \langle \capa, \ptr l \rangle \rquine$ notation represents a value of \i{existential type} that witnesses the existence of a pair containing a capability for, and pointer to, some location $l$ which is hidden from the programmer.

$L^3$'s rules for managing capabilities are closer to standard linear typing than de Vries' rules for modelling Clean's uniqueness typing. Typing assumptions are treated linearly, there are context-splitting operations and contraction and weakening are permitted for values of bang (!) type. This is in contrast to the use of kinds and attribute type constructors in de Vries' model.

Conceptually, because the rules that introduce capabilities are baked into the type system, capabilities are guaranteed \i{not to have been shared} (uniqueness) and \i{not to be shared in the future} (linearity). The use of capabilities also yields all of the same properties yielded by substructural and uniqueness typing -- notably destructive updates and the potential to eliminate garbage collection. It is typical of many of the later systems which we cover in the next section.

\section{Systems of Capabilities}

\subsection{Cyclone}

Several systems extend and generalise the capability-based approach employed in $L^3$. Fluet, Morrisett and Ahmed followed up their paper on $L^3$ with a region-based system that borrows many ideas from $L^3$, called \rgnUL \cite{fluet06}. Notably, it makes use of linear capabilities to provide safe access to \i{dynamic regions}, which are first-class abstractions for the allocation of memory. Dynamic regions extend simpler lexical regions by allowing regions to exist independent of lexical scopes. Accompanying the \rgnUL paper is a mechanised proof of type soundness using the Twelf proof assistant \cite{pfenning99}.

The same authors are also responsible for the Cyclone project \cite{grossman05}, which extends the C programming language with regions and uniqueness typing in order to achieve safe memory management without garbage collection or manual intervention. The \rgnUL calculus models Cyclone's core features, and there exists a translation from Cyclone to \rgnUL via an intermediate language $F^\text{RGN}$ which makes use of a generalised ST monad \cite{fluet06, fluet04}. No mechanised proofs of correctness for this work exist, although an earlier semi-formal proof of type soundness for Cyclone \cite{jim01} is structured in a way that looks amenable to mechanised verification. On their webpage \cite{cycloneWeb}, the creators of Cyclone note that work on the project has stopped, with many of the ideas living on in Rust. Future formalisations of Rust can hopefully make use of this work.

\subsection{Pottier's type-and-capability system with hidden state}

A mechanical formalisation for a system even more similar to $L^3$ than \rgnUL is given in a 2013 article by \Francois Pottier \cite{pottier13}. Pottier's system, \SSPHS, uses affine capabilities in the style of $L^3$, but adds polymorphism and support for \i{hidden state}. Hidden state allows an object to completely conceal mutable internal state from its clients. Pottier gives a memory manager as an example where such a feature is useful -- clients care only about the memory allocated or de-allocated, and not about internal data-structures modified in the process. Hidden state is realised via a typing rule called the \i{anti-frame rule}, which makes terms with hidden state subtypes of the type sans hidden state.

The concept of hidden state is distinct from, yet related to, the existential types that $L^3$ uses to conceal exact locations. \SSPHS also employs hidden state for the purpose of general resource management, rather than just memory management. The ability to express memory management in the language obsoletes $L^3$ and similar systems' explicit rules for memory management, which Pottier describes as ``magic" \cite{pottier13}.

All of $L^3$'s features, including strong updates, are covered by Pottier's system. It also subsumes \rgnUL, with support for polymorphism and regions. Unlike previous systems it also guarantees the runtime-irrelevance of capabilities, which are proved to be erasable.

Pottier's formalisation is done within the Coq proof assistant and makes use of de Bruijn indices for variable binding (a pre-cursor to his \c{dblib} library, discussed in Section \ref{sec:de_bruijn}). The formalisation consists of 20,000 lines of Coq source and follows the syntactic approach to proving type soundness via progress and preservation. Pottier notes that the formalisation took around 6 months to complete.

\subsection{Mezzo}

Together with Thibaut Balabonski and Jonathan Protzenko, Pottier is also responsible for the Mezzo programming language and its associated Coq formalisation \cite{mezzo14}. Mezzo differs from \SSPHS and \rgnUL in that it is designed to be high-level and expressive. Like the other systems examined, its system of ownership is based around linear \i{permissions}, which allow programmers to design diverse usage \i{protocols} for functions and data. Mezzo's model of concurrency leverages ownership to guarantee that well-typed programs do not contain data-races, a property that is also formalised in Coq.

Mezzo includes mechanisms for deferring permissions checks to runtime in order to gain more expressive power, at the cost of some synchronisation overhead. Its surface syntax is also designed to be more minimal than languages like $L^3$ which favour explicit annotations. Both of these aspects reflect Mezzo's ambition to be a user-facing programming language that provides control over resources.

The prototypical compiler for Mezzo uses untyped OCaml as its target language and as such requires garbage collection at runtime. Further, due to OCaml's lack of parallelism, concurrent and race-free Mezzo programs are currently unable to take advantage of multiple cores. One can imagine further work to compile Mezzo to a low-level language with similar semantics, in order to take advantage of its full feature set.

Mezzo's Coq formalisation consists of 14,000 lines of code and makes use of a 2000 line library called \c{dblib} for handling de Bruijn indices. Like the proof for \SSPHS, it uses progress and preservation to prove type soundness.

\section{Typed assembly languages and trustworthy compilers}

Strong updates can be used to model the storage of type-distinct values in a single register through-out program execution. As such, low-level calculi like $L^3$ and \rgnUL are conceptually linked to \i{typed assembly languages} (TALs), which extend regular assembly languages with type annotations.

Well-typed TAL programs typically guarantee memory safety given an axiomatisation of a machine architecture. In the TALx86 \cite{morrisett99, crary99} system, blocks are annotated with pre-conditions that place requirements on the types of registers. This approach to typing is substantially different from the operational semantics and inductive typing judgements used to describe the semantics of the other languages we've surveyed ($L^3$, \rgnUL, \SSPHS). However, recent work by Amal Ahmed \i{et al.}  has successfully resulted in a more traditional model for typed assembly languages \cite{ahmed10}. This model still differs from the others considered in this thesis in that it uses denotational semantics, Hoare logic and several interconnecting layers in order to minimise the number of axioms required. Ahmed's paper includes a Twelf formalisation of soundness for the TAL semantic framework and an example language.

Another take on the typed assembly language concept, is Bedrock from Adam Chlipala's research group \cite{chlipala11}. Bedrock uses a domain-specific assembly language embedded within Coq to express low-level programs. Aided by user-provided annotations, Bedrock can prove properties about these assembly programs in an automated way using custom Coq tactics. The block annotations resemble the block pre-conditions of TALx86.

More broadly it is worth noting the contribution of the CompCert \cite{leroy09} project to program verification. Through a series of semantics-preserving translations through intermediate languages, CompCert compiles a variant of C to multiple assembly languages. CompCert is programmed and verified in Coq. Verification of programs written in a low-level linearly-typed language could use parts of CompCert, perhaps with a language like $L^3$ or \SSPHS as an intermediate language.

\section{Summary of Mechanisation Techniques}

The following table contains a summary of languages and type systems and their mechanisations. A tick (\tick) indicates that a property is true for a given language, a cross (\cross) indicates that it is false and a dash (-) indicates that the property is not applicable. The * indicates work to be completed as part of this thesis. Note that we also write ``Clean" here to mean Edsko de Vries' uniqueness typing system \cite{deVries07}.

\vspace{10mm}

\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{System} & \textbf{DU} & \textbf{SU} & \textbf{Cp} & \textbf{Poly} & \textbf{Other} & \textbf{Mechanised?} & \textbf{Naming}\\
\hline
Clean \cite{deVries07} & \tick & \cross & \cross & \tick & - & \tick (Coq) & LN\\
\hline
Rust \cite{rustWeb} & \tick & \cross & - & \tick 	& No GC & \cross & - \\
\hline
$L^3$ \cite{ahmed05} & \tick & \tick & \tick & $\times$ & - & Yes* (Coq) & DB*\\
\hline
\rgnUL \cite{fluet06} & \tick & - & \tick & \tick & Cyclone base & \tick (Twelf) & HOAS\\
\hline
\SSPHS \cite{pottier13} & \tick & \tick & \tick & \tick & Hidden state & \tick (Coq) & DB\\
\hline
Mezzo \cite{mezzo14} &  \tick & \tick & \tick & \tick & Data-race free & \tick (Coq) & DB\\
\hline
\end{tabular}

Key: DU=Destructive Updates, SU=Strong Updates, Cp=Capabilities, Poly=Polymorphism, LN=Locally Nameless, DB=De Bruijn Indices, HOAS=Higher-order Abstract Syntax.

\section{Summary of Previous Work}

In summary, previous work on the formalisation of resource-aware type systems has culminated in the wide-spread use of capabilities. The basic ideas of linear and affine logic have been adapted to form the core of these systems, with some extra features and approaches mixed in (e.g. hidden state and de Vries' use of kinds). The use of mechanical verification in proofs of type soundness has gained popularity, with most recent works including a formalisation in Coq or Twelf. Other mainstream proof assistants like Isabelle seem to be less used in this space, but we suspect this is primarily due to the limited number of research groups performing this kind of research, and their personal preferences. The notable exception to the verification trend is the $L^3$ language, which remains unverified but has had all of its salient features mechanically verified as part of other projects. Several capability systems mention Alias Types \cite{smith00} and separation logic \cite{reynolds02} as foundational concepts, but we defer discussion of these to future work.

\section{Evaluation Framework}

\subsection{Conceptual Goals}

\subsection{Implementation Goals}

The construction of a large Coq proof, as in the development of any complex piece of software, demands attention to quality of design and implementation. Due to the lack of well established engineering techniques for interactive proofs we describe and justify some of our own criteria here. Discussion of Adam Chlipala's automated style of theorem proving is given as part of a larger discussion about proof automation in the evaluation section.

Firstly, our Coq proofs should be \b{easy to read and understand}. By this we mean that additional complexity or obfuscation that detracts from the \i{intent} of the proof should be kept to a minimum. Coq proofs differ from most other bits of code in that they are often difficult to read without knowing the goal and hypotheses at each step, so our criteria for readability must take this into account.

Some heuristics we can use to assess readability are:

\begin{itemize}
\item Different levels of indentation for goals and sub-goals.
\item Minimal nesting of cases and sub-cases; prefer lemmas.
\item Meaningful variable names at every step; avoid auto-generated names.
\item Short proofs of lemmas.
\end{itemize}

These heuristics are adapted from common software engineering practice (references?), and should be familiar to anyone with a programming background. Short proofs of lemmas are like short functions, and preferring a small lemma to more code in the same lemma is like preferring a utility function to more code in the same function body. The meaningful naming of variables \i{at every step} links back to our altered definition of readability where we imagine that the reader is stepping through the proof examining the goals and hypotheses at each step.

One aim of readable proofs is to convey key insights, but readability also aids \b{maintainability}. Proofs, particularly libraries of proofs, are not static entities and must be constructed so that they can be updated as easily as possible as features are added and new versions of related software are released. Between different versions of the theorem prover the behaviour of tactics can change in ways that are not backwards compatible. References to automatically named variables are considered fragile as a change in the automatic name generation can invalidate the reference and all subsequent proof steps.

\begin{itemize}
\item Markers to enforce different cases and sub-cases.
\item No repetition of proof script.
\end{itemize}

TODO: Keep expanding this section.

\chapter{Proposal}
\label{ch:proposal}

The aim of this thesis is to formalise the semantics of the Linear Language with Locations (Core $L^3$) using the Coq proof assistant. The first step will be to translate the operational semantics and typing rules from the paper \cite{ahmed05} into inductive Coq definitions. As part of this, we will have to define two substitution functions -- one for the term substitution and one for location substitution. We will use the \c{dblib} library \cite{dblib13} for both substitution functions. The justification for this choice and a summary of the associated problems and alternatives is given in Section \ref{sec:de_bruijn}.

With the operational semantics and typing rules defined, we will proceed with a syntactic proof of type soundness via progress and preservation. Experience with simpler languages from \i{Software Foundations} \cite{pierce15} suggests that the proof of progress will be less involved than the proof of preservation. The choice to aim for a syntactic proof is motivated by the semi-formal syntactic proof given in the $L^3$ paper, and the efficacy of syntactic soundness proofs for languages surveyed in the literature review. Considerations of how the proof will be undertaken are given in Section \ref{sec:proof_proposal}.

Time permitting, the work will conclude with proofs of other properties of $L^3$. One interesting proof may be of the runtime irrelevance of capabilities. As in Pottier's  work on \SSPHS \cite{pottier13} this could be done by defining a version of the operational semantics in which capabilities do not appear, and proving that the two sets of semantics are equivalent.

Section \ref{sec:research_questions} includes research questions that this thesis hopes to address, whilst Section \ref{sec:timeline} gives an approximate timeline for the completion of the work.

\section{Variable naming and binding}
\label{sec:var_naming}

One problem that arises frequently in the formalisation of language semantics is that of \i{capture-avoiding substitution}. Substitution operations, whereby a value is substituted for a variable in a term, form the core computational component of the operational semantics in many languages. In the simply-typed (and untyped) lambda calculus, the $\beta$-rule uses substitution (denoted $[v/x]e$) to describe the semantics of function application:

\begin{eqnarray*}
(\lambda x : \tau. \; e) \; v \Longrightarrow_\beta [v/x]e
\end{eqnarray*}

The problem of \i{variable capture}, which we wish to avoid, is demonstrated by the following example:

\begin{eqnarray*}
(\lambda x. \; \lambda y. \; x + y) \; y \; {\centernot\Longrightarrow}_\beta \; (\lambda y. \; y + y)
\end{eqnarray*}

Here the parameter $y$ is a free variable acting as a place-holder for a value in the environment. After substitution however, the $y$ replacing $x$ in the abstraction body $x + y$ becomes bound due to the name collision between the free $y$ and the binder $y$. This altering of the meaning of terms during substitution is something we would like to avoid.

One way to avoid variable capture is to forbid the substitution of any terms containing free variables. In such a system, free variables like $y$ are never considered values and as such cannot be used in variable capturing substitutions. This is the approach taken by \i{Software Foundations} \cite{pierce15} in formalisations of the simply-typed lambda calculus and its variants. A further consequence of this approach is that globally-shared integers or strings for variable names are sufficient to guarantee soundness. Although it's tempting to embrace this approach for its simplifying properties, the $L^3$ specification requires \i{``standard capture-avoiding substitution"} \cite{ahmed05}.

% Extra: It doesn't accurately capture the behaviour of common functional languages like Haskell and ML, which perform substitutions whilst avoiding variable capture.

In our Coq formalisation of $L^3$ we would therefore like to include capture-avoiding substitution as part of the definitions of variable names and substitution operations. For this we consider three main approaches from the literature which all exploit the observation that the exact names of bound variables are insignificant at the level of language formalisation. In other words, although the names of variables may hold meaning for the authors of programs, they do not impact the meanings of programs themselves.

\subsection{Higher-order Abstract Syntax}

When using Higher-order Abstract Syntax (HOAS) to handle variable binding, the binders of the host language (in our case Coq) are used to represent binding constructs in the object language. Twelf encourages use of HOAS through its light-weight syntax (this example adapted from \cite{twelf08}):

\begin{verbatim}
exp : type.
let : exp -> (exp -> exp) -> exp.
\end{verbatim}

The full definition for \c{exp} is omitted, but this example demonstrates that a let-binding in the \i{object language}, can be considered in the \i{meta-language} as a value representing the expression being bound, and a function that accepts that bound expression as input. For example, the object language expression \c{let x = 1 + 2 in x + 3} would be encoded as \c{let (plus 1 2) ([x] plus x 3)}, where \c{plus : nat -> nat -> expr} and \c{([x] e)} is syntax for $(\lambda x. \; e)$.

This sort of encoding becomes problematic in Coq due to the difficulty of encoding types involving \i{negative occurrences} inductively. A type appears as a negative occurrence if it would appear below an odd number of negations in a translation to classical logic \cite{tapl}. In our example, the argument to \c{let}'s higher-order function is a negative occurrence: \c{exp -> (\underline{exp} -> exp) -> exp}. An (invalid) inductive Coq definition for the above Twelf example would be:

\begin{verbatim}
Inductive exp : Set :=
  | exp_plus : nat -> nat -> exp
  | exp_let : exp -> (exp -> exp) -> exp.
\end{verbatim}

Coq rejects this definition with the error: \c{Non strictly positive occurrence of "exp" in
 "exp -> (exp -> exp) -> exp"}, as expected.

There are ways to simulate HOAS-like systems in Coq by either limiting the expressiveness and defining filters on the inductive types obtained \cite{despeyroux95} or by mixing de Bruijn indices and HOAS \cite{capretta07}. As HOAS is entirely absent from the Coq formalisations surveyed we choose to look past it in favour of plain de Bruijn indices.

\subsection{De Bruijn indices and the locally nameless approach}
\label{sec:de_bruijn}

Building on the idea that the exact names of bound variables are irrelevant, de Bruijn indices represent variables as \i{distances from their binding occurrence} \cite{deBruijn72}. For example, the identity function $(\lambda x. \; x)$ is encoded as $(\lambda . \; 0)$ -- assuming without loss of generality that there are no integer literals in the object language that could be confused for de Bruijn indices.

For terms that contain free variables, a fixed naming context is used to map free variables to indices \cite{tapl}. For example, with the naming context $\Gamma = x, y, z$ which maps $\{x \mapsto 2, y \mapsto 1, z \mapsto 0\}$, the term $(\lambda x. \; (x \; y) \; z)$ would be encoded as $(\lambda. \; (0 \; 2) \; 1)$. We can imagine the context prepended to the term as an ordered list of binders, so that the use of $z$ ends up being separated from its binding occurrence by \i{1} -- the lambda.

Capture-avoiding substitution with de Bruijn indices can be defined as a recursive function that makes use of a \i{shifting} operation. Shifting a term by $d$ conceptually renumbers free variables for the introduction of $d$ elements at the end of the naming context. To avoid renumbering bound variables, a cut-off parameter $c$ is threaded through the computation. We denote shifting a term $t$ by $d$ with cut-off $c$ as $\uparrow^d_c t$.

\begin{eqnarray*}
\uparrow^d_c k & = &
	\begin{cases}
	k \quad & \text{if} \quad k < c \\
	k + d \quad & \text{if} \quad k \geq c
	\end{cases}\\
\uparrow^d_c (\lambda. \; t_1) & = & \lambda. \; \uparrow^d_{c + 1} t_1\\
\uparrow^d_c (t_1 \; t_2) & = & (\uparrow^d_c t_1) \; (\uparrow^d_c t_2)
\end{eqnarray*}

With shifting defined, the definition of substitution is straight-forward -- we simply shift the free variables of the substituted term each time we move under a lambda.

\begin{eqnarray*}
[s/j]k & = &
	\begin{cases}
	s \quad & \text{if} \quad j = k \\
	k \quad & \text{otherwise}
	\end{cases}\\
\left[s/j\right](\lambda. \; t_1) & = & \lambda. \; [(\uparrow^1_0 s)/(j + 1)] \; t_1\\
\left[s/j\right](t_1 \; t_2) & = & ([s/j] \; t_1) \; ([s/j] \; t_2)
\end{eqnarray*}

These equations for shifting and substitution are due to \cite{tapl}.

Unlike HOAS, the recursive functions for de Bruijn indices are well-suited for use with the Coq proof assistant. Of the Coq formalisations surveyed in our literature review, two of the largest and most similar to our planned formalisation use de Bruijn indices. The first, \SSPHS \cite{pottier13} defines a module with several lemmas about substitution, while the Mezzo formalisation \cite{mezzo14} makes use of a stand-alone library called \c{dblib} \cite{dblib13}. This library uses Coq's type-classes to provide useful substitution lemmas, given the definitions of a few basic operations on terms of the object language. This is an appealing prospect, and we hope that by using \c{dblib} for our formalisation we will be able to assess its suitability as a generic library for de Bruijn indices.

The alternative to \c{dblib} would have been to use Arthur Chargu\'{e}raud's \i{Engineering Formal Metatheory} library \cite{aydemir08} for binding using a \i{locally nameless} (LN) representation. The locally nameless representation uses de Bruijn indices for bound variables and traditional names for free variables. In his formalisation of uniqueness typing Edsko de Vries notes that use of the LN library \i{``meant that little of our subject reduction proof needs to be concerned with alpha-equivalence or freshness"} \cite{deVries07}.

However, LN does depend on Chargu\'{e}raud's TLC library for \i{non-constructive} logic within Coq \cite{tlc15}. We elect not to use this library, in order to keep the set of axioms minimal and to aid compatibility with other proofs. Further, the formalisations of \SSPHS and Mezzo make successful use of de Bruijn indices and they are considerably closer to $L^3$ than de Vries' uniqueness typing system.

\section{Proof of Type Soundness for $L^3$}
\label{sec:proof_proposal}

To aid in the syntactic proof of type soundness for $L^3$ several sources will be drawn upon for inspiration. The paper proof of soundness for $L^3$ should provide some hints of lemmas to prove and techniques for proving them, like which variable to do induction on. The paper proof of soundness for core $L^3$ spans only 8 pages, so the work could always expand to verify the extended version of $L^3$ which requires a further 14 pages of paper proof.

Other sources of inspiration will be the proofs of soundness for Mezzo and \SSPHS -- particularly for Coq-specific verification techniques. The Twelf proof of soundness for \rgnUL might also be useful for more general techniques.

\section{Research Questions}
\label{sec:research_questions}

By conducting the above research, we hope to answer the following research questions:

\begin{itemize}
\item Does the $L^3$ type system admit straight-forward verification in the style of \rgnUL, Mezzo and other modern capability systems, or do the generalisations and simplifications or these systems make verification simpler?
\item Is the \c{dblib} library for de Bruijn indices general enough to allow the verification of languages it wasn't designed for? Specifically, is it possible to define term and location substitution for $L^3$ using an unmodified version of \c{dblib}?
\item How much effort is involved in the translation of syntactic soundness proofs from paper to the Coq proof assistant?
\end{itemize}

To quantify the effort involved in the proofs, we will make use of a simple script that records every minute spent inside the Coq IDE.

\section{Timeline}
\label{sec:timeline}

I plan to complete most of the practical work over the summer so that the thesis write-up can be completed in Semester 1 2016. In case of delays, semester 1 can also act as an emergency buffer.

\textbf{Over summer (3 months $\approx$ 12 weeks total):}

\begin{itemize}
\item $L^3$'s operational semantics and type system in Coq (2 weeks).
\item Prove progress (4 weeks).
\item Prove preservation, and related lemmas (6 weeks).
\end{itemize}

We expect that some definitions will need to be adjusted along the way.

\textbf{Semester 1 2016 (12 weeks)}:

\begin{itemize}
\item Finalise proofs completed over the summer break (6 weeks).
\item Write-up results and findings (6 weeks).
\end{itemize}

Time permitting, the previously discussed extra proofs could be undertaken either during summer, or after the completion of the main part of the thesis write-up.

\section{Summary of Proposal}

By drawing inspiration from the paper proof of soundness for $L^3$, and similar proofs from the literature, we will prove type soundness for Core $L^3$. We will use the syntactic soundness technique first defined by Wright and Felleisen \cite{wright94}. We will make use of the \c{dblib} library \cite{dblib13} for reasoning about de Bruijn indices. Time permitting, proofs of soundness for extended $L^3$, or of other properties of Core $L^3$ may be attempted.

\chapter{Own Work}

TODO: Some sort of intro paragraph.

\section{Research Questions}

The primary research question that this thesis attempts to answer relates to the efficient development of verified proofs about low-level substructural languages. Specifically,

\begin{itemize}
\item What are the common properties of all proofs about substructural languages, and can these properties be exploited to avoid duplicated work in the context of interactive theorem proving?
\end{itemize}

In the context of Coq, this is can concretely be phrased:

\begin{itemize}
\item Is it possible to write Coq lemmas and definitions which are useful for the verification of numerous substructural type systems?
\end{itemize}

%TODO: Maybe a note about DbLib here.

%TODO: If I do stuff about $L^3$ I might have to nest some stuff under a specific LLC section.

\section{Outline of Own Work}

\begin{itemize}
\item Development of general definitions and lemmas about context splitting and other actions on typing environments.
\item Proof of type soundness for a linear lambda calculus.
\end{itemize}

The proof of type soundness for the linear lambda calculus consists of a collection of lemmas and definitions, culminating in proof of \textbf{preservation} and \textbf{progress} which together establish soundness.

We say that a lemma or definition is \i{universal} if it is applicable to the verification of other linearly typed languages (using the DbLib library). Conversely, we say that a lemma is \i{language-specific} if it is not universal, and is concerned with some specific part of the concrete language being verified.

As is somewhat standard in Coq proofs about programming languages [REF], we use inductive definitions for the terms, types and typing judgements of the language. These definitions are language-specific, and all subsequent lemmas that reference them are also necessarily language-specific.

%These definitions are language-specific, as attempting to make them universal results in numerous other problems.

\section{Syntax and Types}

To define the syntax of the linear lambda calculus (LLC) we must first define the set of types. The reason for this is that lambda abstractions are explicitly annotated with the type of their parameter in order to avoid type-inference when writing proofs. A lambda abstraction $(\lambda x : \tau. e)$ is represented by the Coq expression \c{TAbs $\tau$ e}, which hides the name of the binding variable $x$ through the use of de Bruijn indices. The syntax for types is:

\begin{minted}{coq}
Inductive ty : Set :=
    | TyUnit
    | TyFun : ty -> ty -> ty
    | TyBool.
\end{minted}

Now, with LLC's types defined, we can define the syntax of terms:

\begin{minted}{coq}
Inductive term : Set :=
    | TUnit | TTrue | TFalse
    | TVar : nat -> term
    | TAbs : ty -> term -> term
    | TApp : term -> term -> term.
\end{minted}

FIXME: Should I drop bools? They're not particularly useful.

Variables are represented by the \c{TVar} constructor which takes a de Bruijn index representing the variable and constructs a \c{term}. Later when defining the typing judgement we will see that a variable \c{TVar i} is well-typed only if the typing context contains a type at index $i$.

In order to state the progress and preservation lemmas, Coq also needs an idea of which terms are considered \i{values}, i.e. those terms in a form that can not be simplified further. For this, we use an inductive predicate \c{value t} with the following definition:

\begin{minted}{coq}
Inductive value : term -> Prop :=
    | VUnit : value TUnit
    | VVar : forall x, value (TVar x)
    | VAbs : forall t e, value (TAbs t e)
    | VTrue : value TTrue
    | VFalse : value TFalse
\end{minted}

Note that the type of \c{value} is \c{term -> Prop}. Given a \c{term}, \c{t}, a Coq term with type \c{value t} \i{witnesses} the truth of the proposition \c{value t}, which itself has type \c{Prop}. This is in contrast to \c{ty} and \c{term} which have type \c{Set}. Types in Prop are intended to represent proof terms, while those in Set are meant to represent data. Types in Prop don't work with Coq's program extraction, but this is no concern for our purposes. The real reason to embrace the distinction is to minimise friction with Coq's standard library, which provides definitions for basic logical connectives operating only on \c{Prop}s.

\section{Integrating with DbLib}

DbLib provides functions for substitution and lifting that abstract over the manipulation of de Bruijn indices. Client libraries wanting to make use of DbLib need only implement a few fundamental operations via Coq's \i{type-classes} (ref Coq and Haskell type classes). For the linear lambda calculus we can use essentially the same definitions as for the simply typed lambda calculus, which are provided as an example with the library.

First, we must inform DbLib which of our term constructors is for variables. DbLib allows the types of \i{values} (\c{V}) and \i{terms} (\c{T}) to differ, but we don't make use of this capability, instead using \c{term}s everywhere and the \c{value} predicate. The type-class has the following definition in DbLib:

\begin{verbatim}
Class Var (V : Type) := {
  var: nat -> V
}.
\end{verbatim}

Our instance is straight-forward:

\begin{minted}{coq}
Instance Var_term : Var term := {
  var := TVar
}.
\end{minted}

% FIXME: Could cut this stuff about Var.

To convey how variables are bound and scoped, we must implement DbLib's \c{Traverse} type-class, which has a single function called \c{traverse}. From the DbLib documentation:

\begin{displayquote}
\c{traverse} can be thought of as a semantic substitution function. The idea is, \c{traverse f l t} traverses the term \c{t}, incrementing the index \c{l} whenever a binder is entered, and, at every variable \c{x}, it invokes \c{f l x}. This produces a value, which is grafted instead of \c{x}.
\end{displayquote}

The only binders in our linear lambda calculus are lambda abstractions, so our implementation of traverse only has to increment \c{l} when recursing below a \c{TAbs} constructor. This is the same as for the simply typed lambda calculus.

\begin{minted}{coq}
Fixpoint traverse_term (f : nat -> nat -> term) l t :=
  match t with
  | TUnit => TUnit
  | TTrue => TTrue
  | TFalse => TFalse
  | TVar x =>
      f l x
  | TAbs t e =>
      TAbs t (traverse_term f (1 + l) e)
  | TApp e1 e2 =>
      TApp (traverse_term f l e1) (traverse_term f l e2)
  end.
\end{minted}

To ensure that the client's implementation of traverse behaves sensibly and can be manipulated accordingly, DbLib requires the implementation of five further type-classes that establish semantic properties of traverse. Also provided are five tactics for proving these properties automatically, which were found to be sufficient for our simple use-case.

Given these type-class definitions, DbLib provides a substitution function that we can make use of in the operational semantics for our language.

\section{Representing Typing Contexts}

Before defining an inductive predicate for our typing relation, a representation for typing environments must be selected. In semi-formal proofs, typing judgements are written $H \vdash e : \tau$, and the environment $H$ is assumed to permit various operations such as looking up the type of a variable $x$ (denoted $H[x]$) and (re)assigning a type to a variable $x$, (denoted $H[x \to \tau]$).

As noted in the background section, control over the actions permitted on typing contexts is at the core of substructural typing. Together with our choice to use de Bruijn indices for variable naming, this narrows down our choice of representation. We have the following options to consider:

\begin{itemize}
\item \textbf{Functions}: Some Coq formalisations of languages with structural typing \cite{pierce15} make use of functions to encode partial maps from variables to types. Assigning a type involves wrapping the existing environment in another conditional statement, as in \c{insert x $\tau$ H = fun y => if x = y then Some $\tau$ else H y}. This approach is unsuitable for substructural type systems because the function is opaque and can't be disassembled into two functions which are equivalent when combined. Given an arbitrary function, it is impossible to know that is always going to consist of a conditional of the form shown, and therefore it is also impossible to extract any of the information about x, $\tau$ or the original $f$.
\item \textbf{Lists of Types}: We could consider using a list of types so that the type for variable $\hat{i}$ is at index $i$. This is preferable to using a function because we can inspect and destructure a list, and can also perform induction. However, splitting an environment becomes problematic because we need to keep the type for variable $\hat{i}$ at index $i$, even if some or all of the types at indexes less than $i$ should no longer be available because they were assigned to the other side of the split. Essentially, if we are to use a list, we need a filler value to occupy the evacuated positions. This leads to our next option...
\item \textbf{Lists of Optional Types}: What if we rather than using a list of types, we use a list of \c{option type}, so that types which are no longer available are represented by \c{None} entries? This fulfils all of our requirements: we can look-up types, alter them, add new entries and split an environment so that variables and their types are divided between the two new environments.
\item \textbf{Dependent Vectors}: Suitable? (TODO/DROP)
\end{itemize}

For these reasons, our formalisation makes use of a list of optional types, as provided by the \c{Env} type from DbLib. However, the lemmas about \c{Env} provided by DbLib proved to be insufficient for reasoning about substructural typing rules. For example, DbLib treats the empty list \c{[] (nil)} as the only empty environment, when it is often useful to treat any number of \c{None}s as an empty environment. Further, context splitting defined on \c{Env} is general enough to be of use in multiple DbLib-based formalisations. The next two sections describe these two aspects of our formalisation, and this thesis's contribution to a general framework for substructural languages.

\subsection{Emptiness}

We define the following predicate for environments which is compatible with any environment from DbLib. We say that an environment is \i{empty} if it contains no typing information. Hence, the empty list environment (\c{nil}) is empty, as is any number of \c{None} values.

\begin{minted}{coq}
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons E (EmptyTail : is_empty E) : is_empty (None :: E).
\end{minted}

The necessity of a predicate for emptiness arises from the forthcoming definition of context splitting. Essentially, we require that sub-expressions be typed under an environment of the same length as their parent expressions. This requirement allows context splitting to split environments whilst preserving their length, which is simpler than accounting for changes in environment length when splitting. FIXME: This isn't right, as the typing rules for abstraction actually *increase* the length of the context unconditionally!

In order to be useful in the verification of languages, lemmas about the properties of the \c{is_empty} predicate and its interaction with other parts of the system are required. Of these lemmas, the ones involving interaction with DbLib were slightly more difficult to prove than those about simple Coq constructs. Here is a full list of the lemmas relating to \c{is_empty} and constructs other than context splitting. Lemmas about emptiness and context splitting are discussed in the next section.

\begin{minted}{coq}
Lemma empty_repeat : forall A (E : env A),
  is_empty E ->
  E = repeat (length E) None.
(* Proof by easy induction on is_empty E *)

Lemma empty_repeat_none : forall A n,
  is_empty (repeat n None : env A).
(* Proof by easy induction on n *)

FIXME: More here.
\end{minted}

\subsection{Context Splitting}

% You've met context splitting before.
% High-level description.
% Coq code.
% Inductive definition vs function definition (nah).
% Discussion of split single.
% Discussion of difficulty of existence lemmas -> worth having in a library.
% GOOD REASONS WHY insert/tl is frustrating!
% Discussion of similarity to other approaches (de Vries mixin approach).
%    Justification that the definition is general enough.


Context splitting is the operation by which a typing environment, implemented as a \c{list (option T)}, is split so that it may be used to type two expressions. We use the notation $E = E_1 \circ E_2$ to represent the splitting of $E$ into two environments $E_1$ and $E_2$ such that each variable from $E$ appears in only one of $E_1$ and $E_2$.

In the linear lambda calculus, the typing rule for application requires that the context used to type $(e_1 e_1)$ can be split into two contexts that type $e_1$ and $e_2$ individually. This splitting causes variables from the typing context to be used at most once in the expression $(e_1 e_2)$, as each element of the context must be assigned to either the left or right sub-expression.

As mentioned in the section above on emptiness, we require that context splitting preserve the length of the environment being split. In Coq, we define an inductive predicate over contexts so that \c{context_split E E1 E1} $\equiv E = E_1 \circ E_2$.

\begin{minted}{coq}
Inductive split_single {A} : option A -> option A -> option A -> Prop :=
  | split_none : split_single None None None
  | split_left (v : A) : split_single (Some v) (Some v) None
  | split_right (v : A) : split_single (Some v) None (Some v).

Inductive context_split {A} : env A -> env A -> env A -> Prop :=
  | split_nil : context_split nil nil nil
  | split_cons E E1 E2 v v1 v2
      (SplitElem : split_single v v1 v2)
      (SplitPre : context_split E E1 E2) :
      context_split (v :: E) (v1 :: E1) (v2 :: E2).
\end{minted}

\subsubsection{Single Element Splits}

One may wonder if the presence of an accompanying \c{split_single} predicate is necessary, as the same meaning can be achieved with the following definition that places the splitting of single elements inline:

\begin{minted}{coq}
Inductive context_split : env A -> env A -> env A -> Prop :=
  | split_nil : context_split nil nil nil
  | split_left E E1 E2 v
  	  (SplitPre : context_split E E1 E2) :
  	  context_split (v :: E) (v :: E1) (None :: E2)
  | split_right E E1 E2 v
  	  (SplitPre : context_split E E1 E2) :
  	  context_split (v :: E) (None :: E1) (v :: E2).
\end{minted}

Although for the purposes of forward reasoning both definitions are equally convenient, when performing inversion on terms of type \c{context_split E E1 E2} it is often useful to be able to abstract over the splitting of individual elements, particularly as this means there are less cases which have to be handled.

For example, in the proof of \c{insert_none_split_backwards}, using context splitting on single elements saves duplicating or creating automation for the main part of the proof. The statement of the lemma is:

\begin{minted}{coq}
Lemma insert_none_split_backwards : forall A (E : env A) E1 E2 x,
  context_split (raw_insert x None E) E1 E2 ->
  exists E1' E2',
    E1 = raw_insert x None E1' /\
    length E1' = length E /\
    E2 = raw_insert x None E2' /\
    length E2' = length E /\
    context_split E E1' E2'.
\end{minted}

Intuitively, this lemma states that if we have inserted \c{None} into the typing context at the position for variable \c{x}, then the two environments resulting from the split of this environment must also contain \c{None} at position \c{x}. The proof is by case analysis on whether or not the variable inserted is past the end of the environment (\c{x >= length E}). In the case where it \i{is not} past the end, and \c{x} is greater than 0, we reach a goal of the form:

\begin{minted}{coq}
exists X1 X2 : env A,
  e1 :: E1' = raw_insert (S x') None X1 /\
  length X1 = S (length E') /\
  e2 :: E2' = raw_insert (S x') None X2 /\
  length X2 = S (length E') /\
  context_split (e :: E') X1 X2
\end{minted}

Here, we have \c{E = e :: E'}, \c{E1 = e1 :: E1'}, \c{E2 = e2 :: E2'}, \c{x = S x'} relative to the statement of the lemma.

Without a \c{split_single} definition, inversion of \c{context_split (e :: raw_insert x' None E') (e1 :: E1') (e2 :: E2')} forces us to consider the cases where \c{e1 = None, e2 = Some t} and \c{e1 = Some t, e2 = None} separately. In this proof, this isn't an important distinction, because we really just need to apply the inductive hypothesis to obtain \c{E1''} and \c{E2''} such that \c{E1' = raw_insert x' None E1''} and \c{E2' = raw_insert x' None E2''}. We can then instantiate \c{X1} and \c{X2} with \c{exists (e1 :: E1''), (e2 :: E2'')} and knock over the remaining goals with facts from the proof context. This all works with a \c{split_single} definition, but without one, the two cases have to be handled with two different calls to the \c{exists} tactic; requiring duplication of proof script, or custom LTac to parametrise these calls.

Hence, \c{split_single} is a more general and useful way for dealing with context splitting, as it allows us to prove goals of the form \c{context_split (e :: E) (e1 :: E1) (e2 :: E2)} without knowing the exact values of \c{e}, \c{e1} and \c{e2}. If the exact values \i{are} required for a proof, they can still be considered by performing inversion on the \c{split_single e e1 e2} fact.

\subsubsection{Properties of Context Splitting}

We prove several basic properties of context splitting, such as commutativity, as well as lemmas motivated by the proof of soundness for the linear lambda calculus.

\textbf{Length} If $E = E_1 \circ E_2$, then the lengths of all three contexts are the same.

\textbf{Commutativity}

If we can split $E = E_1 \circ E_2$, then we can also split $E = E_2 \circ E_1$. In Coq, proof is by a straight-forward induction on the structure of the \c{context_split}, and depends on a similar commutative property of \c{split_single}.

\begin{minted}{coq}
Lemma split_commute : forall A (E : env A) E1 E2,
  context_split E E1 E2 -> context_split E E2 E1.
Proof with boom.
  intros A E E1 E2 Split.
  induction Split...
Qed.
\end{minted}

\textbf{Associativity} If $E = E_0 \circ (E_1 \circ E_2)$ then $E = (E_0 \circ E_1) \circ E_2$.

In Coq we need an existential to express the existence of a context that splits into $E_0$ and $E_1$.

\begin{minted}{coq}
Lemma split_assoc : forall A (E E0 E1 E2 E12 : env A),
  context_split E E0 E12 ->
  context_split E12 E1 E2 ->
  (exists E01, context_split E E01 E2 /\ context_split E01 E0 E1).
\end{minted}

The justification for calling this operation associativity is the view of $(\circ)$ as a \i{partial} binary operator for combining contexts. From this view, context splitting is a partial commutative monoid, with \c{repeat n None} as the identity element. Further work could try to exploit structure like this to improve proof automation.

\textbf{4-way Splits}

In his proofs about uniqueness typing Edsko de Vries [REF] establishes all of these properties, but proves the associativity lemma using a 4-way split lemma like this: $E = (E_{1a} \circ E_{1b}) \circ (E_{2a} \circ E_{2b}) \Rightarrow E = (E_{1a} \circ E_{2a}) \circ (E_{2a} \circ E_{2b})$. In addition to associativity being provable from this lemma, the opposite is also true, and we prove a version of this lemma called \c{split_swap} using a few simple applications of commutativity and associativity.

\textbf{Rotation} The proof of preservation requires a lemma of the form: $E = E_0 \circ (E_1 \circ E_2) \Rightarrow E = E_1 \circ (E_0 \circ E_2)$, which is provable using the other lemmas above.

\textbf{Emptiness} Lemmas about empty contexts as the identity element for context splitting are provable using a few lines of proof script. For details, see \c{Context.v} (UPDATEME).

% TODO: Note rewriting rules for commutativity somewhere, maybe in the automation section?

\subsubsection{Lemmas Required for Soundness}

In order to prove soundness, several lemmas about the interaction between inserts and context splitting were required. Although many of the lemmas are conceptually simple, some required significant effort and represent a large portion of the work involved in the proof of soundness for the linear lambda calculus. Many lemmas also involve the insertion of \c{None} values into contexts, which is a consequence of the strengthened induction required to prove the substitution lemma (see the next section on substitution (TODO)).

The first, and perhaps most difficult to prove, was \c{insert_none_split_backwards}, discussed previously in the section about \c{split_single}. Refer to the statement of the lemma above (FIXME?). We attribute the difficulty in proving the lemma to the fact that we initially lacked ways to express many of the intuitive reasons for the lemma's truth. With the right definitions and lemmas in place, the proof became quite straight-forward.

DbLib's \c{insert} function behaves in two different ways depending on the relationship between the variable \c{x} being inserted and the length of the existing environment, \c{E}. If \c{x < length E}, then the type for \c{x} is inserted \i{between} existing elements, with subsequent elements being shunted along. Conversely, if \c{x >= length E}, then the type of \c{x} is inserted \i{after} existing elements, with the intervening space padded by \c{None} values. These two cases are quite different to reason about, and any attempt at a direct inductive proof on \c{x} or one of the environments inevitably leads to wanting to know which of the two cases one is considering. For this reason, the first step of our proof is to split on the comparison between \c{x} and \c{length E}.

To handle the case where \c{x >= length E} and padding \c{None} values are inserted, we note that the values past the end of the old environment will all be \c{None}. Equipped with a simple definition of a \c{repeat} function on lists, and some lemmas about list append, this case is proved.

\begin{minted}{coq}
Lemma insert_none_def : forall A x (E : env A),
  x >= length E ->
  raw_insert x None E = E ++ repeat (S (x - length E)) None.

Lemma split_app : forall A (E : env A) E1 E2 n,
    context_split (E ++ repeat n None) E1 E2 ->
    exists E1' E2',
      E1 = E1' ++ repeat n None /\
      E2 = E2' ++ repeat n None /\
      context_split E E1' E2'.
\end{minted}

The other case where the new \c{None} value is inserted in between existing elements is handled by an induction on \c{x}, which is made simpler by knowing the bound on \c{x}, i.e. \c{x < length E}. For example, it absolves the prover from having to deal with the case where the environment is empty, which proved to be a nuisance in early versions of the proof. Particularly as \c{length (tl E1) = length (tl E2)} does not imply that \c{length E1 = length E2} as one may expect outside a total programming language.

TODO: More here about the other couple of insertion/split lemmas.

% New plan:
% Talk about insert_none_split_backwards, context_split_insert.
% Talk about substitution lemma.
% Talk about preservation.

\section{Typing Rules}

Typing rules determine which terms are considered well-formed. We define an inductive predicate \c{has_type : (env ty) -> term -> ty -> Prop} so that \c{has_type E e t} is inhabited if the environment $E$ determines $e : t$. In standard mathematical notation this is $E \types e : t$, which we mirror using the Coq notation \c{E |- e ~: t}.

\begin{minted}{coq}
Reserved Notation "E '|-' e '~:' t" (at level 40).

Inductive has_type : (env ty) -> term -> ty -> Prop :=
  | HasTyUnit E
      (UnitEmpty : is_empty E) :
      E |- TUnit ~: TyUnit
  | HasTyTrue E
      (TrueEmpty : is_empty E) :
      E |- TTrue ~: TyBool
  | HasTyFalse E
      (FalseEmpty : is_empty E) :
      E |- TFalse ~: TyBool
  | HasTyVar E x t
      (VarPre : is_empty E) :
      insert x t E |- TVar x ~: t
  | HasTyAbs E e t1 t2
      (AbsPre : (insert 0 t1 E) |- e ~: t2) :
      E |- TAbs t1 e ~: (TyFun t1 t2)
  | HasTyApp E E1 E2 e1 e2 t1 t2
      (AppPreSplit : context_split E E1 E2)
      (AppPreWT1 : E1 |- e1 ~: TyFun t1 t2)
      (AppPreWT2 : E2 |- e2 ~: t1) :
      E  |- TApp e1 e2 ~: t2

where "E '|-' e '~:' t" := (has_type E e t).
\end{minted}

Here we see the definitions for emptiness and context splitting coming into play. In linear lambda calculus variables must be used exactly once, so a single variable \c{x} is well-typed only under an environment containing a type for \c{x} and nothing else, as captured by the \c{HasTyVar} rule. Similarly, primitive values must be typed under environments containing no free variables.

A lambda abstraction $(\lambda \hat{0} : \tau. \; e)$ is well-typed if the body can be typed under an environment extended by the type for its binder, $(0 : \tau)$. This is the only rule that requires the \i{input} context (\c{insert 0 t1 E}) to differ in length to the \i{output} context (\c{E}).

Function applications make use of the \c{context_split} operation to ensure that the variables used to type an application $(e_1 e_2)$ are split between $e_1$ and $e_2$ without duplication. The rule also ensures that the type of the application (\c{t2}) is consistent with the type of the function being applied (\c{t1 -> t2}) and the type of the argument (\c{t1}).

\section{Operational Semantics}

For the semantics of our language we use a call-by-value small step semantics. We represent the semantic rules in Coq as an inductive relation \c{step : term -> term -> Prop} so that if \c{step e e'} is true, \c{e'} is the result of partially evaluating \c{e}.

\begin{minted}{coq}
Inductive step : term -> term -> Prop :=
  | StepAppAbs e e' v t
      (BetaPreVal : value v)
      (BetaPreSubst : subst v 0 e = e') :
      step (TApp (TAbs t e) v) e'
  | StepApp1 e1 e1' e2
      (App1Step : step e1 e1') :
      step (TApp e1 e2) (TApp e1' e2)
  | StepApp2 v1 e2 e2'
      (App2Val : value v1)
      (App2Step : step e2 e2') :
      step (TApp v1 e2) (TApp v1 e2').
\end{minted}

The first rule, \c{StepAppAbs} is the familiar $\beta$-rule for evaluating the application of an abstraction to a value. It states that function application is equivalent to the substitution of the argument value for the bound variable in the body of the function. For substitution, note the use of DbLib's \c{subst} function. In mathematical notation the rule is: $(\lambda 0 : \tau. e) \; v \rightarrow e[v/0]$.

We are using a \i{call-by-value} evaluation strategy, which means that only the outer-most reducible expressions (\i{redexes}) are reduced, and only when their argument is a value \cite{tapl}. Note that as a result there is no rule for stepping a $\lambda$-abstraction when its body is capable of stepping, as in: $e \rightarrow e' \implies (\lambda 0 : \tau. \, e) \rightarrow (\lambda 0 : \tau. \, e')$.

The two rules for stepping an application $(e_1 \; e_2)$ ensure that $e_1$ is fully evaluated before evaluating $e_2$. This is captured by the \c{value v1} argument to \c{StepApp2}. By making this restriction an evaluator never needs to choose which side should be evaluated first, thus removing non-determinism. If we include the $\beta$-rule in our consideration we see that the stepping relation as a whole is deterministic, although this property isn't verified in our Coq proofs.

\section{Progress}

With typing rules and small step semantics established we can state the progress lemma which contributes to the proof of type soundness.

\begin{minted}{coq}
Theorem progress : forall E e t,
  is_empty E ->
  E |- e ~: t ->
  (exists e', step e e') \/ value e.
\end{minted}

This theorem states that any closed term $e$ that is well-typed can either be stepped using the small step semantic relation, or is already a value and cannot be evaluated further.

Proof is by induction on the typing derivation \c{E |- e ~: t}. Unlike the proof of preservation presented in the next section, the proof of progress requires very few supporting lemmas about context splitting or substitution. Most of the cases in the induction require reasoning about the interaction between typing and term structure, which can be handled by simple inversions. For example, one of the supporting lemmas states that any value assigned a function type under an empty environment, must necessarily be a $\lambda$-abstraction:

\begin{minted}{coq}
Lemma fun_value_is_abs : forall E e t1 t2,
  is_empty E ->
  E |- e ~: TyFun t1 t2 ->
  value e ->
  (exists e', e = TAbs t1 e').
\end{minted}

Given that most of the verification effort was expended reasoning about context splitting, substitution and other lemmas required for preservation, the effort required to prove progress represents a relatively small fraction of the total effort. The fact that progress and preservation each form ``half" of the soudness proof does not imply that the difficulty of proving soundness is split evenly between them.

\section{Preservation}

The preservation lemma states that a if well-typed closed term $e$ can take a step to another term $e'$, then $e'$ is also well-typed. In other words, the type of a term is \i{preserved} as it is evaluated in accordance with the small-step semantics.

\begin{minted}{coq}
Theorem preservation : forall E e e' t,
  is_empty E ->
  E |- e ~: t ->
  step e e' ->
  E |- e' ~: t.
\end{minted}

Proof is by induction on the stepping of $e$ to $e'$, \c{step e e'}. In the three cases that result from the three stepping rules, the two for applications are proved directly from the inductive hypothesis. The remaining case for $\beta$-reduction involves interaction between substitution, typing and context splitting, and is proved via a supporting \i{substitution} lemma.

\subsection{Substitution}

The substitution lemma states that the result of a substitution is well-typed if the terms involved are well-typed. With substructural typing, we must also consider the supply of free variables to both terms using context spliting, so the lemma takes the form:

\begin{eqnarray*}
\infer[\text{Substitution}]{
    E_1 \circ E_2 \types e_2 [e_1/x] : \tau_2
}{
    E_1 \types e_1 : \tau_1 \quad E_2, x : \tau_1 \types e_2 : \tau_2
}
\end{eqnarray*}

Substitution lemmas similar to this are common in proofs of similar complexity, as in Software Foundations (REF) and the examples accompanying DbLib. In Software Foundations, a weakening lemma is used in the proof of the substitution lemma, but with substructural typing this technique is unavailable.

In Coq the lemma is:

\begin{minted}{coq}
Lemma substitution: forall E2 e2 t1 t2 x,
  insert x t1 E2 |- e2 ~: t2 ->
  forall E E1 e1, E1 |- e1 ~: t1 ->
  context_split E E1 E2 ->
  E |- (subst e1 x e2) ~: t2.
\end{minted}

Proof is by \i{dependent induction} on the judgement \c{insert x t1 E2 |- e2 ~: t2}. Dependent induction allows us to take into account the fact that the environment is \c{insert x t1 E2} rather than an unadorned variable (e.g. $E$). This is achieved by replacing instantiated variables with general ones, and then adding constraining equalities. In this case, the only instantiated variable is \c{insert x t1 E2}, which dependent induction will replace by a new universally quantified variable $E_\text{new}$ and the equation $E_\text{new} = \c{insert x t1 E2}$. With the goal in the form described the dependent induction tactic then applies the induction principle for \c{has_type} to generate subgoals for each of the cases, whilst preserving the newly added equality constraints. Coq's dependent induction is based on Conor McBride's \c{BasicElim} tactic (REF) which makes use of Conor's humorously named ``John Major" heterogenous equality (\c{JMeq}). Heterogenous equality requires the addition of an extra axiom, the consideration of which is discussed in the Evaluation chapter.

Simple inversion removes the cases that are absurd due to the \c{insert x t1 E2} environment, leaving three cases for variables, $\lambda$-abstractions and applications. The variable case is handled by some straight-forward reasoning about empty contexts, and requires no new supporting lemmas, while the other two cases form the motivation for several of the lemmas about inserts and context splitting.

The $\lambda$-abstraction case requires a proof that the term being substituted is well-typed under the environment for the abstraction sans binder, i.e. \c{(None :: E1) |- shift 0 e1 ~: t1}. This motivates \c{typing_insert_none}, which in turn motivates \c{insert_none_split}. Although \c{typing_insert_none} is more general in that it allows us to prove facts of the form \c{raw_insert x None E |- e ~: t} and not just \c{raw_insert 0 None E |- e ~: t}, generalising for all $x$ makes related lemmas easier to prove by enabling induction on $x$. This technique is sometimes referred to as \i{inductive loading}.

The application case requires that one side of the application be well-typed without referring to the substitution variable $x$. This motivates the following lemma, \c{typing_insert_none_subst}:

\begin{minted}{coq}
Lemma typing_insert_none_subst : forall E e x junk t,
  raw_insert x None E |- e ~: t ->
  E |- subst junk x e ~: t.
\end{minted}

This in turn motivates the rest of the lemmas about inserting none into a typing context, including the difficult to prove \c{insert_none_split_backwards} -- discussed above.

To prove this lemma, DbLib was extended with a \i{lowering} operation that is conceptually inverse of lifting. In the case where variables are lowered by 1 we call the operation \i{unshifting}, by analogy with lifting by 1 (shifting). The lemma is proved by establishing that a similar lemma holds for \c{unshift x e}, and an equivalence of \c{unshift x e} and \c{subst junk x e} when \c{x} does not appear free in \c{e} (see \c{contains_var}).

\chapter{Evaluation}

\section{Generality and Applicability}

How useful is what I've done for the verification of other systems? (Answer: pretty ok)

\section{Quality of Coq Proofs}

Robustness, as measured by evaluation criteria.

\subsection{Automation and Tactic Scripts}

In his book \i{Certified Programming with Dependent Types}, Adam Chlipala advocates:

\i{``The more uninteresting drudge work a proof domain involves, the more important it is to work to prove theorems with single tactics."}
\i{``I like to say that if you find yourself caring about indentation in a proof script, it is a sign that the script is structured poorly."}

\chapter{Conclusion}

%% chapters in the ``backmatter'' section do not have chapter numbering
%% text in the ``backmatter'' is single spaced
\backmatter
\pagebreak
\bibliographystyle{alpha}
\bibliography{pubs}

\chapter{Appendix}

DbLib version (git commit hash): blah

\end{document}
