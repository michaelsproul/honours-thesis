\documentclass[]{unswthesis}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{multicol}
\usepackage{proof}
\usepackage{centernot}
\usepackage{backref}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}
\usepackage{csquotes}
\usepackage{underscore}
\usepackage{tabto}
\usepackage{dsfont}
\usepackage{cleveref}

% Nicely coloured links.
\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={blue!80!black},
    urlcolor={blue!80!black}
}

% Nice reference symbols.
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefformat{section}{\S#2#1#3}

% Put boxes around figures? Maybe.
\usepackage{float}
%\floatstyle{boxed}
\restylefloat{figure}

% Minted font size.
\newminted{coq}{fontsize=\small}

% Abstract env.
\newenvironment{abstract}
 {
  \begin{center}
  \bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  \end{center}
  \list{}{
    \setlength{\leftmargin}{.5cm}%
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}

% Tick symbol.
\def\tick{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\newcommand{\cross}{$\times$}

%%% Class options:

%  undergrad (default)
%  hdr

%  11pt (default)
%  12pt

%  final (default)
%  draft

%  oneside (default for hdr)
%  twoside (default for undergrad)


%% Thesis details

% TODO: Choose a title...
% Re-usable lemmas for the formalisation of linearly-typed languages
% A library-based approach to context splitting
% Towards a framework for the mechanised verification of substructural type systems
% A library based approach to the verification of linear type systems
% Code: library, lemmas, definitions, engineering.
% Effective verification of linear types in Coq.
% Proof engineering for formalisations of languages with linear types
\thesistitle{A Library Based Approach to the Verification of Languages with Linear Types}
\thesisschool{School of Computer Science and Engineering}
\thesisauthor{Michael Alexander Sproul}
\thesisZid{z3484357}
\thesistopic{3680}
\thesisdegree{Bachelor of Science (Honours)}
\thesisdate{May 2016}
\thesissupervisor{Dr. Ben Lippmeier}

%% My own LaTeX macros, definitions, etc.

% Names
\newcommand{\rgnUL}{$\lambda^\text{rgnUL}$\text{ }}
\newcommand{\SSPHS}{\text{SSPHS }}
\newcommand{\Francois}{Fran\frenchc{c}ois }
\newcommand{\Lthree}{\text{$L^3$ }}

% Logic/set theory.
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\newcommand{\Forall}[1]{\forall #1 \ldotp \,}
\newcommand{\Exists}[1]{\exists #1 \ldotp \,}

% Linear lambda calculus.
\newcommand{\lam}[1]{\lambda #1 \ldotp \,}
\newcommand{\Lam}[1]{\Lambda #1 \ldotp \,}
\newcommand{\app}[2]{(#1 \; #2)}
\newcommand{\lolly}{\multimap}
\newcommand{\types}{\vdash}
\newcommand{\steps}{\Rightarrow}

% Let bindings.
\newcommand{\letbe}[3]{\textbf{let $#1$ be $#2$ in $#3$}}

% Syntax or.
\newcommand{\sor}{\; | \;}

% L^3 stuff.
\newcommand{\Ptr}[1]{\textbf{Ptr $#1$}}
\newcommand{\ptr}[1]{\textbf{ptr $#1$}}
\newcommand{\Capa}[2]{\textbf{Cap $#1$ $#2$}}
\newcommand{\capa}{\textbf{cap}}
\newcommand{\lquine}{\left\ulcorner}
\newcommand{\rquine}{\right\urcorner}
\newcommand{\dupl}[1]{\textbf{dupl $#1$}}
\newcommand{\drop}[1]{\textbf{drop $#1$}}
\newcommand{\lnew}[1]{\textbf{new $#1$}}
\newcommand{\lfree}[1]{\textbf{free $#1$}}
\newcommand{\lswap}[3]{\textbf{swap $#1$ $#2$ $#3$}}
\newcommand{\qpair}[2]{\lquine #1, #2 \rquine}

% Text formatting.
\let\frenchc\c
\let\c\texttt
\let\oldi\i
\let\i\textit
\let\oldb\b
\let\b\textbf
\let\oldt\t
\let\t\text

\begin{document}

%% Roman numeral numbering in this section.
\frontmatter
\maketitle

% Mention:
% DbLib, context splitting, DILL, L^3.

\begin{abstract}
We present a library of proofs to aid in the mechanical verification of languages with substructural type systems. We identify \i{context splitting} as a widely applicable technique in the verification of these languages, and argue that our proofs are relevant to any language that uses multiple contexts to separate linear assumptions from non-linear ones, in the style of Dual Intuitionistic Linear Logic. Our work is implemented using the Coq interactive theorem prover and builds upon \Francois Pottier's \c{DbLib} library for variable binding using De Bruijn indices. We provide a proof of type soundness for a linear lambda calculus and highlight how similar techniques might be applied to \i{The Linear Language with Locations}, $L^3$ -- which is representative of a class of more complex languages with destructive updates.
\end{abstract}

\newpage

\section*{Acknowledgements}

I would like to thank my supervisor, Ben Lippmeier, for his sage advice and encouragement. I would also like to thank the awesome Programming Languages and Systems research group at UNSW for stimulating my imagination and supporting my project. In particular, I'm grateful to Gabi and Manuel for their leadership and for making it all possible. I would also like to thank my family, friends and Lily for supporting me throughout and keeping me grounded. Thank you.

\tableofcontents

%% Chapter numbers and normal page numbering.
\mainmatter

\chapter{Introduction}
\label{ch:intro}

Computer systems form an integral part of modern society, both in the form of personal devices and critical infrastructure. Ensuring the correct operation of computer hardware and software is therefore a worthwhile endeavour. One emerging technique for the construction of robust software systems is the use of mathematical formalisations and proofs of correctness. In this paradigm, desirable properties of the software can be proven true using a computer-based \i{proof assistant}, which itself relies on a minimal amount of trusted code. For software formalisation and verification to be truly effective, the objects under consideration must have precise mathematical models associated with them. Typically these models are created based on the \i{semantics} (meaning) of the programming language that the software is written in. Unfortunately for the would-be software verifier, most popular programming languages lack formal semantics and are therefore not amenable to verification techniques.

The C Programming Language, in which large amounts of low-level \i{systems software} is written, is an example of a language that is difficult to reason about because of its murky semantics. According to the language specification the results of some operations are classified as \i{undefined behaviour}, meaning that the compiler has no obligation to produce a specific result \cite{isoc}. Ideally, we would like to rule out undefined behaviour at compile time through the use of a type system. In this thesis we focus on the formal semantics of languages with strong type systems that are more well-suited to low-level software verification than C.

In particular, we are interested in \i{resource-aware} languages which exploit \i{substructural} type systems based on linear logic \cite{girard87} to track how values are consumed and destroyed. Intuitively, a \i{linear} value is one that is guaranteed to be used exactly once. In the context of systems software, we can use variants of linearity to enable destructive updates of uniquely reference values, or automatic memory management without garbage collection.

We begin by discussing the Linear Lambda Calculus (LLC), which is a simple extension of $\lambda$-calculus with a linear type system. We then go on to describe more expressive languages which include features like shared pointers and destructive updates.

The \b{main contribution} of this thesis is a software library for the interactive theorem prover Coq. The library includes definitions and lemmas primarily about \i{context splitting}, which is a technique used in many substructural type systems. Our code is an extension of \Francois Pottier's \c{DbLib} library which aims to de-duplicate the effort required to reason about variable binding, just as we hope to de-duplicate the effort required to reason about linear typing.

As a proof-of-concept for our approach we have undertaken a proof of \i{type soundness} for a Linear Lambda Calculus (LLC). Type soundness is a key result in establishing the sensibility of a language and we argue that our approach is also applicable to soundness proofs for more complex languages with the same foundations. Specifically, we argue that a proof of soundness for \i{The Linear Language with Locations} ($L^3$) might make effective use of our library, and that $L^3$ is representative of languages that build upon the Linear Lambda Calculus by adding destructive updates and shared pointers.

% Background.
\chapter{Background}
\label{ch:background}

\section{Coq and the Curry-Howard Correspondence}
\label{sec:curry_howard}

Recall that the Curry-Howard correspondence establishes an equivalence between logical systems and type systems for programming languages. By considering logical propositions as types, the proof of a proposition $P$ can be given by constructing a value of type $P$ in the equivalent programming language. The Coq proof assistant provides a dependently typed programming language with inductive data-types that allows complex propositions to be expressed and proved in this manner. Coq proofs make use of \i{tactics} which abstract over repetitive reasoning.

\section{Beyond C}

Historically, low-level \textit{systems software} has been written primarily in the C programming language, which was originally designed without a formal semantics. Despite this, a large body of work has developed around semantics for C and the verification of low-level software that such work enables. Michael Norrish's contribution of a structural operational semantics for C \cite{norrish98} is notable in that it laid the foundations for C verification projects like the seL4 micro-kernel \cite{klein14}. The seL4 kernel is written in C and is accompanied by proofs of correctness about its security, including crash-freedom and the absence of memory safety violations. The kernel makes use of the AutoCorres tool for translating C code into a higher-level monadic language embedded in the Isabelle/HOL theorem prover \cite{greenaway14}. AutoCorres generates proofs that the translation is a sound with respect to the semantics of the source C program.
Another notable C verification project is the CompCert verified compiler by Xavier Leroy \cite{leroy09}, which is capable of compiling and optimising C code to assembly whilst preserving its semantics. However, CompCert does not guarantee the absence of undefined behaviour in compiled programs, and can only detect undefined behaviour in code that doesn't perform I/O, by dynamically interpreting it.

In this thesis we are concerned with the formal foundations for a possible successor to C. Whether or not this approach will result in simpler and less labour-intensive software verification is an open question. However, we hope that by starting afresh, and using types to enforce useful invariants, a new language for low-level software verification can emerge.

\section{Overview of Linear and Affine Typing}

Linear, affine and uniqueness typing are closely-related features of type systems that enforce rules about the number of times values may be used and referenced. These restrictions are motivated by several desirable properties that can be obtained by enforcing them. The Clean programming language (introduced in \cite{clean87}) uses uniqueness typing to ensure that values in memory have at most one reference to them, thus enabling \i{destructive updates} whilst preserving referential transparency. The Rust programming language \cite{rustWeb} uses uniqueness typing to track and free heap-allocated memory, thus allowing it to achieve memory safety without garbage collection. This makes it suitable for writing systems software where a garbage collector isn't available, like a garbage collector itself, or an operating system.

At their core, all of these systems enforce their constraints using typing rules derived from linear logic \cite{girard87}. To get a feel for linear logic, we first turn our attention to the Linear Lambda Calculus. We then continue by discussing more complex languages and the techniques used in their mechanical formalisation.

\section{The Linear Lambda Calculus}

For our presentation of the linear lambda calculus we follow the work of Plotkin and Barber \cite{barber96} on Dual Intuitionistic Linear Logic (DILL). First, the syntax for variables, types and terms:
\begin{eqnarray*}
x,y & \in & \b{Vars} \\
% \tau & \in & \b{PrimTypes} \\
A, B & ::= & \mathds{I} \sor A \otimes B \sor A \lolly B \sor {!A} \\
t, u & ::= & x \sor * \sor \letbe{*}{t}{u} \sor t \otimes u \sor \letbe{x \otimes y : A \otimes B}{t}{u} \sor \\
  &     & \lam{x : A} t \sor \app{t}{u} \sor {!t} \sor \letbe{!x : A}{t}{u}
\end{eqnarray*}

The meta-variables $x$ and $y$ range over a countably infinite set of variables, \b{Vars}. Meta-variables $A$ and $B$ range over types, and $t$ and $u$ range over terms. The core constructs of the language are those of the lambda calculus: variables ($x$), abstractions ($\lam{x : A} t$) and applications $\app{t}{u}$. We introduce the other constructs through the typing rules of the language.

We write the judgement $\Gamma; \Delta \types t : A$ to denote that a term $t$ has type $A$ relative to two typing environments $\Gamma$ and $\Delta$ which record the types of free variables in $t$. The two typing environments are the origin of the name \b{Dual} Intuitionistic Linear Logic, and are part of DILL's mechanism for differentiating linear and non-linear values.

Typing environments can have many representations, and for the purposes of precise mechanical formalisation this will become very important (see \cref{sec:repr-ty-contexts}). In ``semi-formal" contexts, including Barber's presentation of DILL, the environments are defined to be sequences of variable-type pairs $x_0 : A_0, x_1 : A_1, \dots$ such that no variable appears more than once. Comma-separated environments like $(\Gamma, x : A)$ and $(\Delta_1, \Delta_2)$ are understood to be the concatenation of two environments with disjoint sets of variables.

The first environment, $\Gamma$, records the types of non-linear intuitionistic variables. In contrast, the second environment $\Delta$ records the types of linear variables which must be used exactly once. Although a language with \i{just} linear values is easily conceivable, it is both more practical and flexible to allow non-linear values to exist alongside linear ones. Trivially copyable values like integers are well-suited to being non-linear in a linear language.

To define which typing judgements are well-formed, we use inference rules in the style of natural deduction:

% In the simply-typed (non-linear) lambda calculus and many of the languages described later, a single typing environment suffices.

% For the simply-typed (non-linear) lambda calculus, typing judgements are typically written $H \types t : \tau$, where $H$ is an environment. In a linear language however, we need to make a distinction between variables which can be duplicated freely, and variables which must be used once. The approach take by DILL achieves this by using two typing.

% DILL achieves this by using two environments rather than one, so that where $\Gamma$ records the types of all duplicable (intuitionistic) variables, and $\Delta$

% In Dual Intuitionistic Linear Logic however, we make use of \i{two} environments that record the types of free intuitionistic variables and linear variables respectively respectively, $\Gamma; \Delta \types t : \tau$.

% Types are assigned to terms relative to \i{two} typing environments, rather than just one as is typical for the simply typed lambda calculus. The use of two contexts is the origin of the name \b{Dual} Intuitionistic Linear Logic, and the intent

\begin{figure}[h]
\caption{Typing Rules for DILL Linear Lambda Calculus}
\begin{displaymath}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{cc}
\infer[\text{(Int-Var)}]{\Gamma, x : A; \emptyset \types x : A}{} &
\infer[\text{(Lin-Var)}]{\Gamma; x : A \types x : A}{} \\
\infer[\text{(Unit-I)}]{\Gamma; \emptyset \types * : \mathds{I}}{} &
\infer[\text{(Unit-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \letbe{*}{t}{u} : A}
  {\Gamma; \Delta_1 \types t : \mathds{I}  &  \Gamma; \Delta_2 \types u : A} \\
\infer[\text{($\otimes$-I)}]
  {\Gamma; \Delta_1, \Delta_2 \types t \otimes u : A \otimes B}
  {\Gamma; \Delta_1 \types t : A  &  \Gamma; \Delta_2 \types u : B} &
\infer[\text{($\otimes$-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \letbe{x \otimes y : A \otimes B}{u}{t} : C}
  {\Gamma; \Delta_1 \types u : A \otimes B  &  \Gamma; \Delta_2, x : A, y : B \types t : C} \\
\infer[\text{($\lolly$-I)}]
  {\Gamma; \Delta \types (\lambda x : A. t) : A \lolly B}
  {\Gamma; \Delta, x : A \types t : B} &
\infer[\text{($\lolly$-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types (u \; t) : B}
  {\Gamma; \Delta_1 \types u : A \lolly B  &  \Gamma; \Delta_2 \types t : A} \\
\infer[\text{(!-I)}]
  {\Gamma; \emptyset \types {!t} : {!A}}
  {\Gamma; \emptyset \types t : A} &
\infer[\text{(!-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \letbe{!x : A}{u}{t} : B}
  {\Gamma; \Delta_1 \types u : {!A}  &  \Gamma, x : A; \Delta_2 \types t : B} \\
\end{array}
\end{displaymath}
\end{figure}

There are two rules for typing variables, corresponding to intuitionistic (Int-Var) and linear variables (Lin-Var) respectively. In both cases we allow extra intuitionistic variables to be present in the environment, as we are free to ignore them. Conversely, linear variables must be used exactly once, so there can't be any present when typing a non-linear variable, and there can't be any extra ones present when typing a linear variable. Hence the linear contexts for the two rules are $\emptyset$ and $x : A$ respectively.

The unit type, $\mathds{I}$, is inhabited by a single term $*$. The term \b{let $*$ be $t$ in $u$} allows for the \i{elimination} of any term $t$ of type $\mathds{I}$, as shown by the rule (Unit-E). Every type constructor has an introduction rule and an elimination rule, suffixed \b{-I} and \b{-E} respectively.

The product type $A \otimes B$ represents a pair of values and is inhabited by terms of the form $t \otimes u$ if $t : A$ and $u : B$, as demonstrated by its introduction rule ($\otimes$-I). In order to guarantee that linear variables are used exactly once in $t \otimes u$, it is required that the linear context $(\Delta_1, \Delta_2)$ is the result of joining the two variable-disjoint linear contexts of $t$ and $u$. Alternately, we can view this as the environment for $t \otimes u$ being \i{split} into the two environments for $t$ and $u$. This is the \b{context splitting} operation that is at the core of many substructural type systems, and is the focus of our Coq library. Note that we use both the words \i{environment} and \i{context} to refer to typing environments.

The elimination rule for products ($\otimes$-E) also makes use of context splitting to ensure linearity. The rule states that if we have a term $u : A \otimes B$ and a term $t : C$ that is well-typed with free variables $x : A$ and $y : B$, then the expression that makes the components of $u$ available as $x$ and $y$ in $t$ also has type $C$. DILL uses the notation \b{let $x \otimes y : A \otimes B$ be $u$ in $t$} for this expression, which is equivalent to \c{let (x, y) = u in t} in Haskell syntax. The intuitionistic context $\Gamma$ from the \b{let} expression is available to both premises, whilst the linear context $(\Delta_1, \Delta_2)$ is split so that some variables are used to type $u$ ($\Delta_1$), and others are used to type $t$ in combination with the components of $u$ ($\Delta_2, x : A, y : B$).

The \i{lollipop} type, $A \lolly B$, is the type of functions that consume a linear value of type $A$ and produce a value of type $B$. As we shall see shortly, regular functions $A \to B$ that don't consume their input can be encoded as $!A \lolly B$. Introducing a new value of lollipop type is done by writing a $\lambda$-abstraction, $(\lambda x : A. \; t)$ which is well-typed only if its body $t$ is well-typed under a linear environment extended with the type of the binder: $\Delta, x : A$. This rule, ($\lolly$-I) is identical to the rule for typing $\lambda$-abstractions in the Simply-Typed Lambda Calculus (STLC) except for the fact that it places the binder in the linear context.

The elimination rule for lollipop types ($\lolly$-E) uses a function application $(u \; t)$, where $u : A \lolly B$ and $t : A$. The context splitting is identical to the context splitting in the ($\otimes$-I) rule, with the linear context for $(u \; t)$ splitting into two sub-contexts for each of the sub-expressions $u$ and $t$.

Finally we come to the \i{bang} type, $!A$, which allows the embedding of intuitionistic terms within the language. The introduction rule for bang types, (!-I), states that if a term $t$ can be assigned the type $A$ without reference to any linear variables, then we can construct a term ${!t} : {!A}$ which represents a duplicable version of $t$. Intuitively, this makes sense, as we can refer to the variables in the intuitionistic context for $t$ as many times as we like whilst creating copies via $!t$.

To understand how terms of bang type become usable as duplicates requires consideration of the associated elimination rule, (!-E). This rule allows a term $u : {!A}$ to be destructured so that its ``inner" term becomes available in the \i{non-linear} context as re-usable assumption $x : A$. If $u : {!A}$ was determined using the introduction rule for bang types such that $u = {!v}$ for some $v$, then the let binding \b{let $!x : A$ be $u$ in $t$} binds a new name $x$ to $v$ and makes it available in the intuitionistic context for $t$.

Alternatively, it's possible for a term $u$ to have type $!A$ without an application of the bang introduction rule. This occurs, for example, when writing a function to duplicate its input as a pair. Terms of linear type can't be duplicated, so the argument to this function must have type $!A$ for some type $A$. We must then use a bang let-binding, and (!-E) to bring a duplicable version of the binder variable into the intuitionistic context. The term we would like to type is therefore: $(\lambda x : {!A}. \; \t{let } {!y} : A \t{ be } x \t{ in } ({!y} \otimes {!y}))$. The typing derivation is:

\begin{eqnarray*}
\infer[\t{($\lolly$-I)}]{\emptyset; \emptyset \types (\lambda x : {!A}. \; \t{let } {!y} : A \t{ be } x \t{ in } ({!y} \otimes {!y})) : {!A} \lolly ({!A} \otimes {!A})}{
  \infer[\t{(!-E)}]{\emptyset; x : {!A} \types \t{let } {!y} : A \t{ be } x \t{ in } ({!y} \otimes {!y}) : ({!A} \otimes {!A})}{
    \infer[\t{(Lin-Var)}]{
      \emptyset; x : {!A} \types x : {!A}
    }{} &
    \infer[\t{($\otimes$-I)}]{
      y : A; \emptyset \types ({!y} \otimes {!y}) : ({!A} \otimes {!A})
    }{
      \infer[\t{(!-I)}]{y : A; \emptyset \types {!y} : {!A}}{
        \infer[\t{(Int-Var)}]{y : A; \emptyset \types y : A}{}
      } &
      \infer[\t{(!-I)}]{y : A; \emptyset \types {!y} : {!A}}{
        \infer[\t{(Int-Var)}]{y : A; \emptyset \types y : A}{}
      }
    }
  }
}
\end{eqnarray*}

Note how we rely on (Lin-Var), rather than the bang introduction rule (!-I), to form the judgement $\emptyset; x : {!A} \types x : {!A}$. We also see the bang elimination rule in action here, adding the new name $y$ for $x$ into the intuitionistic context for ${!y} \otimes {!y}$.

In the Simply-Typed Lambda Calculus, an equivalent duplication function would have type $A \to (A \otimes A)$. In fact, it is impossible to embed all intuitionistic (STLC) terms and types in linear logic using reasoning similar to the above \cite{barber96}. Intuitionistic types $A$ can be translated to duplicable bang types ${!A}$, and functions $A \to B$ can similarly be translated to ${!A} \lolly B$.

\subsection{Structural Rules}

Philip Wadler gives a similar typing derivation for a duplication function in his 1993 presentation of linear logic \cite{wadler93}, although his system differs from DILL in a few key ways.

In Wadler's system, a single typing environment is used, containing both linear assumptions of the form $\langle x : A \rangle$ and intuitionistic ones of the form $[x : A]$. These are conceptually equivalent to assumptions in the respective linear and intuitionistic contexts of DILL. For rules that require the context to consist entirely of intuitionistic assumptions, Wadler uses the notation $[\Gamma]$, which is approximately equivalent to a $\Gamma; \emptyset$ pair of contexts in DILL. When formalising linear logic with an interactive theorem prover such as Coq, the dual-context approach taken by DILL is preferable to Wadler's approach because of the clear and forced separation of the two worlds -- intuitionistic and linear. For example, Wadler's system would require a Coq data-type to differentiate the two types of assumptions, and an additional predicate to state if a context contains only intuitionistic assumptions. A further difference between the two systems is that Wadler's renders the structural rules that lend their name to \i{substructural} type systems explicit.

To save writing the entire set of typing rules, which are very similar to the ones for DILL already presented, we cherry-pick a few of the structural rules to demonstrate our point, and rely on the approximate translation described above and the original paper \cite{wadler93} to provide the full details.

Unlike DILL, both kinds of variables in Wadler's LLC require their typing contexts to be empty except for the variable of interest:

\begin{eqnarray*}
\infer[\t{(Int-Var')}]{[x : A] \types x : A}{} \qquad
\infer[\t{(Lin-Var')}]{\langle x : A \rangle \types x : A}{}
\end{eqnarray*}

Duplication and discarding of intuitionistic variables is then enabled by two \i{structural} rules, Contraction and Weakening:

\begin{eqnarray*}
\infer[\t{(Contraction)}]{\Gamma, [x : A] \types u[x/y, x/z] : B}{
  \Gamma, [y : A], [z : A] \types u : B
}
\qquad
\infer[\text{(Weakening)}]{\Gamma, [x : A] \types t : B}{
  \Gamma \types t : B
}
\end{eqnarray*}

Contraction makes use of substitutions ($u[x/y, x/z]$) to form terms with two occurrences of the intuitionistic variable $x$. Weakening allows a non-vital variable and type to be introduced from nowhere, or, when reasoning backwards, it allows an unused variable to be discarded in order to type a sub-term.

In DILL the contraction and weakening rules for intuitionistic variables are implicit in how the intuitionistic context is managed -- there can be additional intuitionistic assumptions present when typing variables (weakening), and intuitionistic assumptions can be duplicated freely (contraction). In contrast, in Wadler's system all duplication and discarding happens via (Contraction) and (Weakening). With the exception of the structural rules, Wadler's typing contexts behave entirely linearly, and are \b{split} in the same way linear contexts in DILL are split.

If the use of structural rules is restricted, we get a substructural type system. Without Contraction, \i{variables can only appear once in a term}. Without Weakening, \i{all variables in the context must be used in the term}. Banning both contraction and weakening results in a \b{linear} type system where variables must be used exactly once. The systems we've seen so far, DILL and Wadler's LLC, are both linear in that contraction and weakening are banned for the non-intuitionistic terms of the language. Under the Curry-Howard correspondence, linear type systems correspond to linear logic, which was first introduced by Girard \cite{girard87}.

If contraction is restricted but weakening is allowed, the result is an \b{affine} type system, corresponding to an affine logic (REF). Variables in affine logic can be thrown away but not duplicated, so every variable is used \i{at most once}.

% With both (Contraction) and (Weakening), intuitionistic variables can be used just as freely as in DILL or the STLC.
% The typing derivation for a duplication function in Wadler's LLC requires the use of (Contraction) to type both sides of the pair.

\subsection{Summary of the Linear Lambda Calculus}

In this section we've seen how typing rules can be used to enforce constraints on the use of variables, by examining Dual Intuitionistic Linear Logic. We have seen that intuitionistic terms can be embedded in substructural languages via (!) types and the careful management of typing contexts. The importance of \b{context splitting} to divide linear assumptions between sub-terms has also been highlighted. In the next sections we examine more complex calculi that build on the linear lambda calculus to model useful programming language constructs like mutation and destructive updates.

\section{Operational Semantics and Type Soundness}

Up until this point, we have described only the syntax and static semantics (typing rules) of linearly-typed programming languages, with only vague notions of how terms behave \i{dynamically} at ``run-time". To now describe the dynamic behaviour of languages we turn to \i{structural operational semantics}, a mathematical formalism for the step-wise evaluation of terms, also known as small-step semantics.

To define the small-step semantics of a language, we inductively define a relation on pairs of terms, denoted $t \steps t'$. The relation needn't be strictly on pairs of terms, and many small-step semantics also thread through a global state or \i{store}, $\sigma$, so that the stepping relation ends up being: $(\sigma, t) \steps (\sigma', t')$. In small-step semantics each step $t \steps t'$ is intended to represent a single step of the computation, in contrast to big-step \i{natural} semantics where the values that terms evaluate to are stated directly, as in $t \Downarrow v$ (REF cow book).

The $\beta$-rule of the $\lambda$-calculus can be stated for DILL as:
\begin{eqnarray*}
\infer[\beta]{(\lambda x : A. \; t) \; v \steps t[v/x]}{}
\end{eqnarray*}

Here we use the variable $v$ to indicate a \i{value} of the language. Values are terms that are fully evaluated according to the small-step semantics of the language \cite{tapl} -- formally, they are terms that are in normal form with respect to the stepping relation $(\steps)$. By specifying that $\beta$-reduction can only occur if the argument is a value we have fixed the reduction strategy to call-by-value. Other reduction strategies can be similarly encoded, but we consider only call-by-value, as discussed in (section FIXME).

% FIXME: Maybe include something about rules with premises.
% Semantic rules can also have premises, as in the case of the two rules for

Type systems for programming languages are said to be \i{sound} if well-typed programs are guaranteed not to get stuck when evaluated according to the language's operational semantics. Stated formally, the soundness lemma is:
\begin{eqnarray*}
\infer[\t{(Type Soundness)}]
{(\exists e''. \; e' \steps e'') \lor \t{$e'$ is a value}}
{\emptyset \types e : \tau  &  e \steps^* e'}
\end{eqnarray*}

The notation $(\steps^*)$ represents zero or more applications of $(\steps)$. Our definition of stuckness states that a term $e'$ is not stuck if either it can step to some other term $e''$, or it is a value of the language.

Soundness can be proved via two supporting lemmas called \i{progress} and \i{preservation}, using an approach introduced by Wright and Felleisen \cite{wright94}.

The progress property holds if all well-typed terms are either values, or can take a step.
\begin{eqnarray*}
\infer[\t{(Progress)}]
{(\exists e'. \; e \steps e') \lor \t{$e$ is a value}}
{\emptyset \types e : \tau}
\end{eqnarray*}

The preservation, or \i{subject reduction}, property holds if well-typed terms retain their type during evaluation.
\begin{eqnarray*}
\infer[\t{(Preservation)}]
{\emptyset \types e' : \tau}
{\emptyset \types e : \tau  &  e \steps e'}
\end{eqnarray*}

Our proof-of-concept Coq formalisation of a linear lambda calculus makes use of progress and preservation lemmas to establish type soundness.

\section{The Linear Language with Locations, $L^3$}

We now turn our attention to \i{The Linear Language with Locations}, $L^3$ \cite{ahmed05}, which extends the linear lambda calculus with \b{strong destructive updates} and explicit memory management. In a language with references or pointers, a destructive update is a re-assignment of a value pointed to by a pointer. A \i{strong} destructive update is one that may also change the \i{type} of the value pointed to by the pointer. Destructive updates are a common feature of imperative languages, and their addition to the linear lambda calculus brings us a step closer to a sound and usable programming language for low-level programming. $L^3$ itself is not such a language, but could serve as a foundation for a language with more features (e.g. polymorphism). Its designers created it as a foundational calculus for strong updates, which can be used to model CPU registers.

% due to registers almost always containing values of different types throughout a program's execution.

As for Dual Intuitionistic Linear Logic, we begin with a description of the syntax of the language, see Figure \ref{l3_syntax}.

\begin{figure}[h]
\caption{Syntax for The Linear Language with Locations}
\label{l3_syntax}
\begin{eqnarray*}
x,y & \in & \b{Vars} \\
l & \in & \b{LocConsts} \\
\rho & \in & \b{LocVars} \\
\eta & ::= & l \sor \rho \\
A & ::= & \mathds{I} \sor A \otimes B \sor A \lolly B \sor !A \sor \Ptr{\eta} \sor \Capa{\eta}{A} \sor
\Forall{\rho} A \sor \Exists{\rho} A \\
t,u & ::= & * \sor \letbe{*}{t}{u} \sor \\
  && t \otimes u \sor \letbe{x \otimes y}{t}{u} \sor \\
  && x \sor \lam{x} t \sor \app{t}{u} \sor \\
  && {!v} \sor \letbe{!x}{t}{u} \sor \dupl{t} \sor \drop{t} \sor \\
  && \ptr{l} \sor \capa \sor \lnew{t} \sor \lfree{t} \sor \lswap{t_1}{t_2}{t_3} \\
  && \Lam{\rho} t \sor t [\eta] \sor \qpair{\eta}{t} \sor \letbe{\qpair{\rho}{x}}{t}{u} \\
v & ::= & * \sor v_1 \otimes v_2 \sor x \sor \lam{x} t \sor {!v} \sor \ptr{l} \sor \capa
  \sor \Lam{\rho} t \sor \qpair{\eta}{v}
\end{eqnarray*}
\end{figure}

\begin{figure}[h]
\caption{Typing Rules for The Linear Language with Locations}
\label{l3_typing}
\begin{displaymath}
\arraycolsep=1.2pt\def\arraystretch{2.2}
\begin{array}{cc}
% Unit
\infer{\Delta; \emptyset \types * : \mathbb{I}}{} &
\infer{\Delta; \Gamma_1, \Gamma_2 \types \letbe{*}{t}{u} : A}{
  \Delta; \Gamma_1 \types t : \mathbb{I}  &  \Delta; \Gamma_2 \types u : A
} \\
% Pairs
\infer{\Delta; \Gamma_1, \Gamma_2 \types t \otimes u : A \otimes B}{
  \Delta; \Gamma_1 \types t : A  &  \Delta; \Gamma_2 \types u : A
}\\
\infer
{\Delta; \Gamma_1, \Gamma_2 \types \letbe{x_1 \otimes x_2}{t}{u} : C}
{\Delta; \Gamma_1 \types t : A \otimes B  &
 \Delta; \Gamma_2, x_1 : A, x_2 : A \types u : C}\\
% Vars
\infer[\t{(Var)}]{\Delta; x : A \types x : A}{FLV(A) \subseteq \Delta}\\
% Functions
\infer[\t{($\lolly$-I)}]{\Delta; \Gamma \types \lam{x} t : A \lolly B}{
  \Delta; \Gamma, x : A \types t : B
} &
\infer[\t{($\lolly$-E)}]{\Delta; \Gamma_1, \Gamma_2 \types \app{t}{u} : B}{
  \Delta; \Gamma_1 \types t : A \lolly B  &  \Delta; \Gamma_2 \types u : A
} \\
% Bang
\infer[\t{(!-I)}]{\Delta; !\Gamma \types {!v} : {!A}}{\Delta; !\Gamma \types v : A} &
\infer[\t{(!-E)}]{\Delta; \Gamma_1, \Gamma_2 \types \letbe{!x}{t}{u} : B}{
  \Delta; \Gamma_1 \types t : A  &  \Delta; \Gamma_2, x : A \types u : B
}\\
\infer[\t{(Dupl)}]{\Delta; \Gamma \types \dupl{t} : {!A} \otimes {!A}}{
  \Delta; \Gamma \types t : {!A}
} &
\infer[\t{(Drop)}]{\Delta; \Gamma \types \drop{t} : \mathbb{I}}{
  \Delta; \Gamma \types t : {!A}
}
\end{array}
\end{displaymath}
\end{figure}

Many of the terms and types have the same meaning as in DILL. Functions, variables, products and the unit value are all the same. $L^3$'s description also includes small-step operational semantics, for which the set of values denoted by meta-variable $v$ are normal forms.

% Bang types are handled in a slightly different way, with two new primitives added: $\dupl{t}$ and $\drop{t}$.

The main addition is the set of primitives for allocating and managing memory: $\lnew{t}$, $\lfree{t}$, $\lswap{t_1}{t_2}{t_3}$. Each piece of memory allocated is at a constant location $l$, drawn from a set of location constants \b{LocConsts} which can be considered memory addresses. The precise locations are hidden from the programmer by existential types $\Exists{\rho} t$, for location variables $\rho \in \b{LocVars}$.

$L^3$ is typical of other modern calculi in that it separates resources like pointers from \i{capabilities} to use those resources. A pointer value $\ptr{l} : \Ptr{l}$ is unusable without a capability to read from and write to it: $\capa : \Capa{l}{A}$. Capabilities are linear values, whilst pointers are duplicable. To see how this is enforced, and the destructive updates that it enables, we now considering the typing rules for \Lthree, as in Figure \ref{l3_typing}.

\Lthree's typing judgements include two contexts, one for locations and another for types: $\Delta; \Gamma \types t : A$. All variables and their types are stored in one typing context, which makes \Lthree more similar to Wadler's LLC than DILL -- despite the syntactic similarity. The location context $\Delta$ is a sequence of locations that are currently in scope, like variables in a typing context without the type information: $\Delta ::= \emptyset \sor \Delta, \rho$.

As in DILL, \b{context splitting} is used extensively to ensure that each variable appears exactly once in a term. The typing rules for unit types, product types ($\otimes$) and functions ($\lolly$) are almost identical to the rules for their counterparts in Wadler's LLC, and DILL, modulo some context shuffling.

The rule for variables, (Var), is similar to Wadler's, except that the \i{free location variables} of the variable's type, $FLV(A)$, must be present in the location context. Another interesting difference is that \Lthree has only one rule for variables, rather than two like DILL and Wadler's LLC. The reason for this is that \Lthree doesn't differentiate between linear and intuitionistic assumptions. Wadler motivates the two types of assumptions by giving an example of how the proof reduction rule equivalent to $\beta$-reduction becomes unsound if contraction and weakening are allowed for assumptions of the form $\langle x : {!A} \rangle$ (or just $x : {!A}$ in \Lthree syntax). This doesn't pose a problem to \Lthree because there are no explicit contraction and weakening rules -- the same functionality is instead provided by $\dupl{t}$ and $\drop{t}$ primitives.

These primitives form part of \Lthree's handling of intuitionistic values using the bang (!) type. The introduction rule for bang types is almost the same as in DILL, except that an intuitionistic context $!\Gamma$ is now just one where all assumptions are of the form $x : {!A}$, and only values $v$ are permitted. The elimination rule is also similar, except that the context that the newly bound variable is introduced into is linear, which means the variable can only be used once after the rule is applied. The duplication primitive mitigates this restriction by explicitly transforming a value of type $!A$ into a pair of values with the same type, ${!A} \otimes {!A}$. The components of the duplicate pair can be given names and introduced into the typing context by the elimination rule for products.

As well as duplicating values, which is equivalent to contraction, intuitionistic terms $t$ can also be discarded using $\drop{t}$, equivalent to weakening.

\begin{eqnarray*}
(\sigma, \text{new } v) \Rightarrow (\sigma \uplus \{l \mapsto v\},
  \lquine l, \langle \capa, \ptr l \rangle \rquine)
\\
(\sigma, \text{let } \lquine \rho, x \rquine = \lquine l, v \rquine \text{in } e)
  \Rightarrow
  (\sigma, e[l/\rho][v/x])
\end{eqnarray*}

\pagebreak

%the mechanical verification of a small language -- \i{The Linear Language with Locations}, $L^3$. Although formal semantics and semi-formal proofs of correctness exist for $L^3$, no computer-based proofs exist. We verify $L^3$ because it is typical of modern resource-aware calculi in its use of capabilities -- whilst remaining simple. $L^3$'s concepts of ownership make its mechanisation relevant to the complete verification of low-level systems like garbage collectors and operating systems, which are at the cutting edge of verification research.

The Linear Language with Locations $L^3$ \cite{ahmed05}, makes use of concepts from linear typing to manage \i{capabilities} -- 

$L^3$ is a \i{low-level} language by design and features primitive operations for allocating and deallocating memory. By virtue of linear capabilities, \i{strong updates} are supported, whereby the type of a value in a memory location may change upon writing. Strong updates enable staged value initialisation, and simulation of register type-changes throughout program execution. For example, a pointer initially pointing to an \c{Int} can be overwritten with a \c{Bool} and the type system can statically track this change.

$L^3$'s definition includes a description of syntax, operational semantics and typing rules. Notably, the operational semantics include a store type which maps locations to values. Similarly, the typing rules include a location context that tracks which locations are in scope.



These two rules from the operational semantics show the behaviour of the \c{new} keyword for allocating memory, and the \c{let} construct for unpacking pointers and capabilities. Note how the store $\sigma$ is extended with a mapping from a new location $l$ to the value $v$ in the case of the rule for \c{new}. The $\lquine l, \langle \capa, \ptr l \rangle \rquine$ notation represents a value of \i{existential type} that witnesses the existence of a pair containing a capability for, and pointer to, some location $l$ which is hidden from the programmer.

$L^3$'s rules for managing capabilities are closer to standard linear typing than de Vries' rules for modelling Clean's uniqueness typing. Typing assumptions are treated linearly, there are context-splitting operations and contraction and weakening are permitted for values of bang (!) type. This is in contrast to the use of kinds and attribute type constructors in de Vries' model.

Conceptually, because the rules that introduce capabilities are baked into the type system, capabilities are guaranteed \i{not to have been shared} (uniqueness) and \i{not to be shared in the future} (linearity). The use of capabilities also yields all of the same properties yielded by substructural and uniqueness typing -- notably destructive updates and the potential to eliminate garbage collection. It is typical of many of the later systems which we cover in the next section.

\section{Uniqueness Typing}

Although linear and affine type theory capture the essence of uniquely referenced values, they are insufficient to describe the concept of \i{uniqueness} as it appears in languages like Clean. In his 2007 paper, de Vries \cite{deVries07} notes that terms of a unique type should be \i{guaranteed to never have been shared}, which is sufficient to guarantee a unique pointer at runtime. In contrast, terms of linear (or affine) type are \i{guaranteed not to be shared in the future}, which is insufficient to guarantee a unique pointer.

The distinctness of linearity and uniqueness is further highlighted by the \i{dereliction} rule present in some versions of linear type theory, and the rule that we'll refer to as \i{uniqueness removal} present in Clean's type system. Let $\alpha^\odot$ and $\alpha^\otimes$ stand for linear and non-linear versions of a base type $\alpha$. Similarly, let $\alpha^\bullet$ and $\alpha^\times$ stand for unique and shared versions of a base type $\alpha$. We have:

\begin{eqnarray*}
(\lambda x. \; x) \; : & \alpha^\otimes \rightarrow \alpha^\odot & \text{(linear dereliction)}\\
(\lambda x. \; x) \; : & \alpha^\bullet \rightarrow \alpha^\times & \text{(uniqueness removal)}
\end{eqnarray*}

The dereliction rule allows a non-linear value to be transformed into a linear one. As noted by de Vries \cite{deVriesPhD08}, an analogous rule for unique types that converts shared values to unique ones cannot possibly be sound. The ``unique" value resulting from such a rule would not necessarily be unique because other shared references may still exist.

Conversely, uniqueness removal only makes sense in the context of uniqueness types. A unique value may sacrifice its uniqueness to become shared, but a linear value which models the existence of a single resource should not be transformed into an unlimited supply of that resource.

Note that dereliction need not form a part of type systems based on linear logic, and that it is absent from Wadler's presentations \cite{wadler90, wadler93} and all systems considered in the next sections. Further note that if uniqueness is to be exploited to make garbage collection unnecessary -- as in the case of Rust -- then the uniqueness removal rule is undesirable as it prevents values from having a unique owner.

Due to the non-equivalence of linearity and uniqueness, de Vries constructed a distinct set of semantics and typing rules to model Clean's type system \cite{deVries07}.

One key component of his approach is the use of the \i{kind} (type of types) system to track uniqueness and non-uniqueness. As in Haskell, de Vries' uniqueness system includes a kind for data (\c{*}) which is the kind of all inhabited types (and \c{Void}). In addition, there is a uniqueness kind $\mathcal{U}$ inhabited by two types $\bullet$ and $\times$ representing uniqueness and non-uniqueness respectively. A third kind, $\mathcal{T}$ is the kind of base types (like \c{Int}). These kinds are brought together by a type constructor $\c{Attr} ::_k \mathcal{T} \rightarrow \mathcal{U} \rightarrow *$ which applies a uniqueness attribute to a base type to form a type that is inhabited. For example, \c{Attr} $\bullet$ \c{Int} or $\c{Int}^\bullet$ is the type of uniquely referenced integers.

The other main technique employed by de Vries' model is the use of arbitrary boolean expressions as uniqueness attributes, with $\bullet$ as true and $\times$ as false. Clean's type system allows uniqueness polymorphism, which results in constraint relationships between uniqueness variables, which are represented in de Vries' system as simple boolean expressions that can be handled by a standard unification algorithm.

Importantly, de Vries' work includes the only known mechanical verification of a type system similar to Clean's. The formalisation uses the Coq proof assistant, and the \i{locally nameless} approach to variable naming that is discussed in Section \ref{sec:de_bruijn}.

% Linear + affine logic and control over contraction and weakening. DONE.

% Uniqueness typing as an alternative (mention dereliction, non guarantees). DONE.

% de Vries formalisation of Clean's uniqueness typing using attributes.

\section{Systems of Capabilities}

\subsection{Cyclone}

Several systems extend and generalise the capability-based approach employed in $L^3$. Fluet, Morrisett and Ahmed followed up their paper on $L^3$ with a region-based system that borrows many ideas from $L^3$, called \rgnUL \cite{fluet06}. Notably, it makes use of linear capabilities to provide safe access to \i{dynamic regions}, which are first-class abstractions for the allocation of memory. Dynamic regions extend simpler lexical regions by allowing regions to exist independent of lexical scopes. Accompanying the \rgnUL paper is a mechanised proof of type soundness using the Twelf proof assistant \cite{pfenning99}.

The same authors are also responsible for the Cyclone project \cite{grossman05}, which extends the C programming language with regions and uniqueness typing in order to achieve safe memory management without garbage collection or manual intervention. The \rgnUL calculus models Cyclone's core features, and there exists a translation from Cyclone to \rgnUL via an intermediate language $F^\text{RGN}$ which makes use of a generalised ST monad \cite{fluet06, fluet04}. No mechanised proofs of correctness for this work exist, although an earlier semi-formal proof of type soundness for Cyclone \cite{jim01} is structured in a way that looks amenable to mechanised verification. On their webpage \cite{cycloneWeb}, the creators of Cyclone note that work on the project has stopped, with many of the ideas living on in Rust. Future formalisations of Rust can hopefully make use of this work.

\subsection{Pottier's type-and-capability system with hidden state}

A mechanical formalisation for a system even more similar to $L^3$ than \rgnUL is given in a 2013 article by \Francois Pottier \cite{pottier13}. Pottier's system, \SSPHS, uses affine capabilities in the style of $L^3$, but adds polymorphism and support for \i{hidden state}. Hidden state allows an object to completely conceal mutable internal state from its clients. Pottier gives a memory manager as an example where such a feature is useful -- clients care only about the memory allocated or de-allocated, and not about internal data-structures modified in the process. Hidden state is realised via a typing rule called the \i{anti-frame rule}, which makes terms with hidden state subtypes of the type sans hidden state.

The concept of hidden state is distinct from, yet related to, the existential types that $L^3$ uses to conceal exact locations. \SSPHS also employs hidden state for the purpose of general resource management, rather than just memory management. The ability to express memory management in the language obsoletes $L^3$ and similar systems' explicit rules for memory management, which Pottier describes as ``magic" \cite{pottier13}.

All of $L^3$'s features, including strong updates, are covered by Pottier's system. It also subsumes \rgnUL, with support for polymorphism and regions. Unlike previous systems it also guarantees the runtime-irrelevance of capabilities, which are proved to be erasable.

Pottier's formalisation is done within the Coq proof assistant and makes use of de Bruijn indices for variable binding (a pre-cursor to his \c{dblib} library, discussed in Section \ref{sec:de_bruijn}). The formalisation consists of 20,000 lines of Coq source and follows the syntactic approach to proving type soundness via progress and preservation. Pottier notes that the formalisation took around 6 months to complete.

\subsection{Mezzo}

Together with Thibaut Balabonski and Jonathan Protzenko, Pottier is also responsible for the Mezzo programming language and its associated Coq formalisation \cite{mezzo14}. Mezzo differs from \SSPHS and \rgnUL in that it is designed to be high-level and expressive. Like the other systems examined, its system of ownership is based around linear \i{permissions}, which allow programmers to design diverse usage \i{protocols} for functions and data. Mezzo's model of concurrency leverages ownership to guarantee that well-typed programs do not contain data-races, a property that is also formalised in Coq.

Mezzo includes mechanisms for deferring permissions checks to runtime in order to gain more expressive power, at the cost of some synchronisation overhead. Its surface syntax is also designed to be more minimal than languages like $L^3$ which favour explicit annotations. Both of these aspects reflect Mezzo's ambition to be a user-facing programming language that provides control over resources.

The prototypical compiler for Mezzo uses untyped OCaml as its target language and as such requires garbage collection at runtime. Further, due to OCaml's lack of parallelism, concurrent and race-free Mezzo programs are currently unable to take advantage of multiple cores. One can imagine further work to compile Mezzo to a low-level language with similar semantics, in order to take advantage of its full feature set.

Mezzo's Coq formalisation consists of 14,000 lines of code and makes use of a 2000 line library called \c{dblib} for handling de Bruijn indices. Like the proof for \SSPHS, it uses progress and preservation to prove type soundness.

\section{Typed assembly languages and trustworthy compilers}

Strong updates can be used to model the storage of type-distinct values in a single register through-out program execution. As such, low-level calculi like $L^3$ and \rgnUL are conceptually linked to \i{typed assembly languages} (TALs), which extend regular assembly languages with type annotations.

Well-typed TAL programs typically guarantee memory safety given an axiomatisation of a machine architecture. In the TALx86 \cite{morrisett99, crary99} system, blocks are annotated with pre-conditions that place requirements on the types of registers. This approach to typing is substantially different from the operational semantics and inductive typing judgements used to describe the semantics of the other languages we've surveyed ($L^3$, \rgnUL, \SSPHS). However, recent work by Amal Ahmed \i{et al.}  has successfully resulted in a more traditional model for typed assembly languages \cite{ahmed10}. This model still differs from the others considered in this thesis in that it uses denotational semantics, Hoare logic and several interconnecting layers in order to minimise the number of axioms required. Ahmed's paper includes a Twelf formalisation of soundness for the TAL semantic framework and an example language.

Another take on the typed assembly language concept, is Bedrock from Adam Chlipala's research group \cite{chlipala11}. Bedrock uses a domain-specific assembly language embedded within Coq to express low-level programs. Aided by user-provided annotations, Bedrock can prove properties about these assembly programs in an automated way using custom Coq tactics. The block annotations resemble the block pre-conditions of TALx86.

More broadly it is worth noting the contribution of the CompCert \cite{leroy09} project to program verification. Through a series of semantics-preserving translations through intermediate languages, CompCert compiles a variant of C to multiple assembly languages. CompCert is programmed and verified in Coq. Verification of programs written in a low-level linearly-typed language could use parts of CompCert, perhaps with a language like $L^3$ or \SSPHS as an intermediate language.

\section{Variable Naming and Binding}
\label{sec:var_naming}

One problem that arises frequently in the formalisation of language semantics is that of \i{capture-avoiding substitution}. Substitution operations, whereby a value is substituted for a variable in a term, form the core computational component of the operational semantics in many languages. In the simply-typed (and untyped) lambda calculus, the $\beta$-rule uses substitution (denoted $[v/x]e$) to describe the semantics of function application:

\begin{eqnarray*}
(\lambda x : \tau. \; e) \; v \Longrightarrow_\beta [v/x]e
\end{eqnarray*}

The problem of \i{variable capture}, which we wish to avoid, is demonstrated by the following example:

\begin{eqnarray*}
(\lambda x. \; \lambda y. \; x + y) \; y \; {\centernot\Longrightarrow}_\beta \; (\lambda y. \; y + y)
\end{eqnarray*}

Here the parameter $y$ is a free variable acting as a place-holder for a value in the environment. After substitution however, the $y$ replacing $x$ in the abstraction body $x + y$ becomes bound due to the name collision between the free $y$ and the binder $y$. This altering of the meaning of terms during substitution is something we would like to avoid.

One way to avoid variable capture is to forbid the substitution of any terms containing free variables. In such a system, free variables like $y$ are never considered values and as such cannot be used in variable capturing substitutions. This is the approach taken by \i{Software Foundations} \cite{pierce15} in formalisations of the simply-typed lambda calculus and its variants. A further consequence of this approach is that globally-shared integers or strings for variable names are sufficient to guarantee soundness. Although it's tempting to embrace this approach for its simplifying properties, the $L^3$ specification requires \i{``standard capture-avoiding substitution"} \cite{ahmed05}.

% Extra: It doesn't accurately capture the behaviour of common functional languages like Haskell and ML, which perform substitutions whilst avoiding variable capture.

In our Coq formalisation of $L^3$ we would therefore like to include capture-avoiding substitution as part of the definitions of variable names and substitution operations. For this we consider three main approaches from the literature which all exploit the observation that the exact names of bound variables are insignificant at the level of language formalisation. In other words, although the names of variables may hold meaning for the authors of programs, they do not impact the meanings of programs themselves.

\subsection{Higher-order Abstract Syntax}

When using Higher-order Abstract Syntax (HOAS) to handle variable binding, the binders of the host language (in our case Coq) are used to represent binding constructs in the object language. Twelf encourages use of HOAS through its light-weight syntax (this example adapted from \cite{twelf08}):

\begin{verbatim}
exp : type.
let : exp -> (exp -> exp) -> exp.
\end{verbatim}

The full definition for \c{exp} is omitted, but this example demonstrates that a let-binding in the \i{object language}, can be considered in the \i{meta-language} as a value representing the expression being bound, and a function that accepts that bound expression as input. For example, the object language expression \c{let x = 1 + 2 in x + 3} would be encoded as \c{let (plus 1 2) ([x] plus x 3)}, where \c{plus : nat -> nat -> expr} and \c{([x] e)} is syntax for $(\lambda x. \; e)$.

This sort of encoding becomes problematic in Coq due to the difficulty of encoding types involving \i{negative occurrences} inductively. A type appears as a negative occurrence if it would appear below an odd number of negations in a translation to classical logic \cite{tapl}. In our example, the argument to \c{let}'s higher-order function is a negative occurrence: \c{exp -> (\underline{exp} -> exp) -> exp}. An (invalid) inductive Coq definition for the above Twelf example would be:

\begin{verbatim}
Inductive exp : Set :=
  | exp_plus : nat -> nat -> exp
  | exp_let : exp -> (exp -> exp) -> exp.
\end{verbatim}

Coq rejects this definition with the error: \c{Non strictly positive occurrence of "exp" in
 "exp -> (exp -> exp) -> exp"}, as expected.

There are ways to simulate HOAS-like systems in Coq by either limiting the expressiveness and defining filters on the inductive types obtained \cite{despeyroux95} or by mixing de Bruijn indices and HOAS \cite{capretta07}. As HOAS is entirely absent from the Coq formalisations surveyed we choose to look past it in favour of plain de Bruijn indices.

\subsection{De Bruijn Indices and the Locally Nameless Approach}
\label{sec:de_bruijn}

Building on the idea that the exact names of bound variables are irrelevant, de Bruijn indices represent variables as \i{distances from their binding occurrence} \cite{deBruijn72}. For example, the identity function $(\lambda x. \; x)$ is encoded as $(\lambda . \; \hat{0})$, where a natural number $i$ annotated with a hat as in $\hat{i}$ represents a de Bruijn index.

For terms that contain free variables, a fixed naming context is used to map free variables to indices \cite{tapl}. For example, with the naming context $\Gamma = x, y, z$ which maps $\{x \mapsto \hat{2}, y \mapsto \hat{1}, z \mapsto \hat{0}\}$, the term $(\lambda x. \; (x \; y) \; z)$ would be encoded as $(\lambda. \; (\hat{0} \; \hat{2}) \; \hat{1})$. We can imagine the context prepended to the term as an ordered list of binders, so that the use of $z$ ends up being separated from its binding occurrence by \i{1} -- the lambda.

Capture-avoiding substitution with de Bruijn indices can be defined as a recursive function that makes use of a \i{shifting} operation. Shifting a term by $d$ conceptually renumbers free variables for the introduction of $d$ elements at the end of the naming context. To avoid renumbering bound variables, a cut-off parameter $c$ is threaded through the computation. We denote shifting a term $t$ by $d$ with cut-off $c$ as $\uparrow^d_c t$.

\begin{eqnarray*}
\uparrow^d_c k & = &
  \begin{cases}
  k \quad & \text{if} \quad k < c \\
  k + d \quad & \text{if} \quad k \geq c
  \end{cases}\\
\uparrow^d_c (\lambda. \; t_1) & = & \lambda. \; \uparrow^d_{c + 1} t_1\\
\uparrow^d_c (t_1 \; t_2) & = & (\uparrow^d_c t_1) \; (\uparrow^d_c t_2)
\end{eqnarray*}

With shifting defined, the definition of substitution is straight-forward -- we simply shift the free variables of the substituted term each time we move under a lambda.

\begin{eqnarray*}
[s/j]k & = &
  \begin{cases}
  s \quad & \text{if} \quad j = k \\
  k \quad & \text{otherwise}
  \end{cases}\\
\left[s/j\right](\lambda. \; t_1) & = & \lambda. \; [(\uparrow^1_0 s)/(j + 1)] \; t_1\\
\left[s/j\right](t_1 \; t_2) & = & ([s/j] \; t_1) \; ([s/j] \; t_2)
\end{eqnarray*}

These equations for shifting and substitution are due to \cite{tapl}.

Unlike HOAS, the recursive functions for de Bruijn indices are well-suited for use with the Coq proof assistant. Of the Coq formalisations surveyed in our literature review, two of the largest and most similar to our planned formalisation use de Bruijn indices. The first, \SSPHS \cite{pottier13} defines a module with several lemmas about substitution, while the Mezzo formalisation \cite{mezzo14} makes use of a stand-alone library called \c{dblib} \cite{dblib13}. This library uses Coq's type-classes to provide useful substitution lemmas, given the definitions of a few basic operations on terms of the object language. This is an appealing prospect, and we hope that by using \c{dblib} for our formalisation we will be able to assess its suitability as a generic library for de Bruijn indices.

The alternative to \c{dblib} would have been to use Arthur Chargu\'{e}raud's \i{Engineering Formal Metatheory} library \cite{aydemir08} for binding using a \i{locally nameless} (LN) representation. The locally nameless representation uses de Bruijn indices for bound variables and traditional names for free variables. In his formalisation of uniqueness typing Edsko de Vries notes that use of the LN library \i{``meant that little of our subject reduction proof needs to be concerned with alpha-equivalence or freshness"} \cite{deVries07}.

However, LN does depend on Chargu\'{e}raud's TLC library for \i{non-constructive} logic within Coq \cite{tlc15}. We elect not to use this library, in order to keep the set of axioms minimal and to aid compatibility with other proofs. Further, the formalisations of \SSPHS and Mezzo make successful use of de Bruijn indices and they are considerably closer to $L^3$ than de Vries' uniqueness typing system.

\section{Summary of Mechanisation Techniques}

The following table contains a summary of languages and type systems and their mechanisations. A tick (\tick) indicates that a property is true for a given language, a cross (\cross) indicates that it is false and a dash (-) indicates that the property is not applicable. The * indicates work to be completed as part of this thesis. Note that we also write ``Clean" here to mean Edsko de Vries' uniqueness typing system \cite{deVries07}.

\vspace{10mm}

\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{System} & \textbf{DU} & \textbf{SU} & \textbf{Cp} & \textbf{Poly} & \textbf{Other} & \textbf{Mechanised?} & \textbf{Naming}\\
\hline
Clean \cite{deVries07} & \tick & \cross & \cross & \tick & - & \tick (Coq) & LN\\
\hline
Rust \cite{rustWeb} & \tick & \cross & - & \tick 	& No GC & \cross & - \\
\hline
$L^3$ \cite{ahmed05} & \tick & \tick & \tick & $\times$ & - & Yes* (Coq) & DB*\\
\hline
\rgnUL \cite{fluet06} & \tick & - & \tick & \tick & Cyclone base & \tick (Twelf) & HOAS\\
\hline
\SSPHS \cite{pottier13} & \tick & \tick & \tick & \tick & Hidden state & \tick (Coq) & DB\\
\hline
Mezzo \cite{mezzo14} &  \tick & \tick & \tick & \tick & Data-race free & \tick (Coq) & DB\\
\hline
\end{tabular}

Key: DU=Destructive Updates, SU=Strong Updates, Cp=Capabilities, Poly=Polymorphism, LN=Locally Nameless, DB=De Bruijn Indices, HOAS=Higher-order Abstract Syntax.

\section{Summary of Previous Work}

In summary, previous work on the formalisation of resource-aware type systems has culminated in the wide-spread use of capabilities. The basic ideas of linear and affine logic have been adapted to form the core of these systems, with some extra features and approaches mixed in (e.g. hidden state and de Vries' use of kinds). The use of mechanical verification in proofs of type soundness has gained popularity, with most recent works including a formalisation in Coq or Twelf. Other mainstream proof assistants like Isabelle seem to be less used in this space, but we suspect this is primarily due to the limited number of research groups performing this kind of research, and their personal preferences. The notable exception to the verification trend is the $L^3$ language, which remains unverified but has had all of its salient features mechanically verified as part of other projects. Several capability systems mention Alias Types \cite{smith00} and separation logic \cite{reynolds02} as foundational concepts, but we defer discussion of these to future work.

\section{Evaluation Framework}

In this section we describe some criteria for assessing the quality of work completed as part of this thesis. Given our goal of creating a general Coq library for the verification of linearly-typed languages, what properties should our library ideally possess? We break the criteria into two subsections: \b{Conceptual Goals}, relating to the theoretical content of the library, and its generality and applicability to other languages; and \b{Implementation Goals} regarding the quality of the Coq library itself. Dividing the criteria in this way provides broad coverage of the quality of the work whilst allowing us to separate concerns.

\subsection{Conceptual Goals}

Our main conceptual goal is that the library be applicable and useful in the mechanical formalisation of numerous languages with substructural typing. We say \i{numerous}, rather than \i{all}, because new type systems with substructural influences are still being developed (cite Conor's Plenty o' Nuttin' paper). The library should be:

\begin{enumerate}
\item Applicable to the formalisation of $L^3$ and related systems.
\item Usable without modifications to the core definitions.
\item Usable with the addition of a minimal number of lemmas about the library's content.
\end{enumerate}

Point (1) is a restatement of our main overarching goal specialised to $L^3$, which we argue is representative of a class of similar languages.

Point (2) expresses the ideal that the verifier of a new linearly-typed language should be able to use the library without making bespoke modifications to the core definitions and lemmas. Such modifications would indicate a lack of generality in the library's content. \i{Usable} here also means that the potential user of the library should not have to work around the library's inadequacies in convoluted ways.

Point (3) relates to the coverage provided by the library's lemmas. Ideally, all of the interesting relationships between its parts should be chronicled as lemmas within the library. We allow some flexibility, to acknowledgement that the ideal is typically very time-consuming to achieve. We believe it is reasonable for users of the library to discover a small number of missing relationships between the library's components which they can then contribute \i{upstream} for general use.

\subsection{Implementation Goals}

The construction of a large Coq proof, as in the development of any complex piece of software, demands attention to quality of design and implementation. Due to the lack of well established engineering techniques for interactive proofs we describe and justify some of our own criteria here. Discussion of Adam Chlipala's automated style of theorem proving is given as part of a larger discussion about proof automation in the evaluation section.

Firstly, our Coq proofs should be \b{easy to read and understand}. By this we mean that additional complexity or obfuscation that detracts from the \i{intent} of the proof should be kept to a minimum. Coq proofs differ from most other bits of code in that they are often difficult to read without knowing the goal and hypotheses at each step, so our criteria for readability must take this into account.

Some heuristics we can use to assess \b{readability} are:

\begin{itemize}
\item Different levels of indentation for goals and sub-goals.
\item Minimal nesting of cases and sub-cases; prefer lemmas.
\item Meaningful variable names at every step; avoid auto-generated names.
\item Short proofs of lemmas.
\end{itemize}

These heuristics are adapted from common software engineering practice, and should be familiar to anyone with a programming background. Short proofs of lemmas are like short functions, and preferring a small lemma to more code in the same lemma is like preferring a utility function to more code in the same function body. The meaningful naming of variables \i{at every step} links back to our altered definition of readability where we imagine that the reader is stepping through the proof examining the goals and hypotheses at each step.

One aim of readable proofs is to convey key insights, but readability also aids \b{maintainability}. Proofs, particularly libraries of proofs, are not static entities and must be constructed so that they can be updated as easily as possible as features are added and new versions of related software are released. Between different versions of the theorem prover the behaviour of tactics can change in ways that are not backwards compatible. References to automatically named variables are considered fragile as a change in the automatic name generation can invalidate the reference and all subsequent proof steps. Some additional criteria to aid \b{maintainability} are:

\begin{itemize}
\item No repetition of proof script.
\item Markers to enforce different cases and sub-cases.
\end{itemize}

Avoiding copied sections of proof script aids maintainability in an obvious way -- changing one section of code doesn't require changing all of its copies.

Enforced case markers improve the clarity of error messages and the general debugging experience. By \i{enforced} we mean that the case markers only allow a new case to begin if the existing case has already been solved. Without case markers a tactic failure early in a proof script can cause tactics on subsequent lines to be applied to the wrong goals, leading to confusing error messages. With case markers, the cases that fail are isolated from their surroundings.

An assessment of the work according to this evaluation framework is given in the Evaluation (REF) section. We turn our attention now to a detailed description of the work.

\chapter{Own Work}

TODO: Some sort of intro paragraph.

\section{Research Questions}

The primary research question that this thesis attempts to answer relates to the efficient development of verified proofs about low-level substructural languages. Specifically,

\begin{itemize}
\item What are the common properties of all proofs about substructural languages, and can these properties be exploited to avoid duplicated work in the context of interactive theorem proving?
\end{itemize}

In the context of Coq, this is can concretely be phrased:

\begin{itemize}
\item Is it possible to write Coq lemmas and definitions which are useful for the verification of numerous substructural type systems?
\end{itemize}

%TODO: Maybe a note about DbLib here.

%TODO: If I do stuff about $L^3$ I might have to nest some stuff under a specific LLC section.

\section{Outline of Own Work}

\begin{itemize}
\item Development of general definitions and lemmas about context splitting and other actions on typing environments.
\item Proof of type soundness for a linear lambda calculus.
\end{itemize}

The proof of type soundness for the linear lambda calculus consists of a collection of lemmas and definitions, culminating in proof of \textbf{preservation} and \textbf{progress} which together establish soundness.

We say that a lemma or definition is \i{universal} if it is applicable to the verification of other linearly typed languages (using the DbLib library). Conversely, we say that a lemma is \i{language-specific} if it is not universal, and is concerned with some specific part of the concrete language being verified.

As is somewhat standard in Coq proofs about programming languages [REF], we use inductive definitions for the terms, types and typing judgements of the language. These definitions are language-specific, and all subsequent lemmas that reference them are also necessarily language-specific.

%These definitions are language-specific, as attempting to make them universal results in numerous other problems.

\section{Syntax and Types}

To define the syntax of the linear lambda calculus (LLC) we must first define the set of types. The reason for this is that lambda abstractions are explicitly annotated with the type of their parameter in order to avoid type-inference when writing proofs. A lambda abstraction $(\lambda x : \tau. e)$ is represented by the Coq expression \c{TAbs $\tau$ e}, which hides the name of the binding variable $x$ through the use of de Bruijn indices. The syntax for types is:

\begin{minted}{coq}
Inductive ty : Set :=
    | TyUnit
    | TyFun : ty -> ty -> ty
    | TyBool.
\end{minted}

Now, with LLC's types defined, we can define the syntax of terms:

\begin{minted}{coq}
Inductive term : Set :=
    | TUnit | TTrue | TFalse
    | TVar : nat -> term
    | TAbs : ty -> term -> term
    | TApp : term -> term -> term.
\end{minted}

FIXME: Should I drop bools? They're not particularly useful.

Variables are represented by the \c{TVar} constructor which takes a de Bruijn index representing the variable and constructs a \c{term}. Later when defining the typing judgement we will see that a variable \c{TVar i} is well-typed only if the typing context contains a type at index $i$.

In order to state the progress and preservation lemmas, Coq also needs an idea of which terms are considered \i{values}, i.e. those terms in a form that can not be simplified further. For this, we use an inductive predicate \c{value t} with the following definition:

\begin{minted}{coq}
Inductive value : term -> Prop :=
    | VUnit : value TUnit
    | VVar : forall x, value (TVar x)
    | VAbs : forall t e, value (TAbs t e)
    | VTrue : value TTrue
    | VFalse : value TFalse
\end{minted}

Note that the type of \c{value} is \c{term -> Prop}. Given a \c{term}, \c{t}, a Coq term with type \c{value t} \i{witnesses} the truth of the proposition \c{value t}, which itself has type \c{Prop}. This is in contrast to \c{ty} and \c{term} which have type \c{Set}. Types in Prop are intended to represent proof terms, while those in Set are meant to represent data. Types in Prop don't work with Coq's program extraction, but this is no concern for our purposes. The real reason to embrace the distinction is to minimise friction with Coq's standard library, which provides definitions for basic logical connectives operating only on \c{Prop}s.

\section{Integrating with DbLib}

DbLib provides functions for substitution and lifting that abstract over the manipulation of de Bruijn indices. Client libraries wanting to make use of DbLib need only implement a few fundamental operations via Coq's \i{type-classes} (ref Coq and Haskell type classes). For the linear lambda calculus we can use essentially the same definitions as for the simply typed lambda calculus, which are provided as an example with the library.

First, we must inform DbLib which of our term constructors is for variables. DbLib allows the types of \i{values} (\c{V}) and \i{terms} (\c{T}) to differ, but we don't make use of this capability, instead using \c{term}s everywhere and the \c{value} predicate. The type-class has the following definition in DbLib:

\begin{verbatim}
Class Var (V : Type) := {
  var: nat -> V
}.
\end{verbatim}

Our instance is straight-forward:

\begin{minted}{coq}
Instance Var_term : Var term := {
  var := TVar
}.
\end{minted}

% FIXME: Could cut this stuff about Var.

To convey how variables are bound and scoped, we must implement DbLib's \c{Traverse} type-class, which has a single function called \c{traverse}. From the DbLib documentation:

\begin{displayquote}
\c{traverse} can be thought of as a semantic substitution function. The idea is, \c{traverse f l t} traverses the term \c{t}, incrementing the index \c{l} whenever a binder is entered, and, at every variable \c{x}, it invokes \c{f l x}. This produces a value, which is grafted instead of \c{x}.
\end{displayquote}

The only binders in our linear lambda calculus are lambda abstractions, so our implementation of traverse only has to increment \c{l} when recursing below a \c{TAbs} constructor. This is the same as for the simply typed lambda calculus.

\begin{minted}{coq}
Fixpoint traverse_term (f : nat -> nat -> term) l t :=
  match t with
  | TUnit => TUnit
  | TTrue => TTrue
  | TFalse => TFalse
  | TVar x =>
      f l x
  | TAbs t e =>
      TAbs t (traverse_term f (1 + l) e)
  | TApp e1 e2 =>
      TApp (traverse_term f l e1) (traverse_term f l e2)
  end.
\end{minted}

To ensure that the client's implementation of traverse behaves sensibly and can be manipulated accordingly, DbLib requires the implementation of five further type-classes that establish semantic properties of traverse. Also provided are five tactics for proving these properties automatically, which were found to be sufficient for our simple use-case.

Given these type-class definitions, DbLib provides a substitution function that we can make use of in the operational semantics for our language.

\section{Representing Typing Contexts}
\label{sec:repr-ty-contexts}

Before defining an inductive predicate for our typing relation, a representation for typing environments must be selected. In semi-formal proofs, typing judgements are written $H \vdash e : \tau$, and the environment $H$ is assumed to permit various operations such as looking up the type of a variable $x$ (denoted $H[x]$) and (re)assigning a type to a variable $x$, (denoted $H[x \to \tau]$).

As noted in the background section, control over the actions permitted on typing contexts is at the core of substructural typing. Together with our choice to use de Bruijn indices for variable naming, this narrows down our choice of representation. We have the following options to consider:

\begin{itemize}
\item \textbf{Functions}: Some Coq formalisations of languages with structural typing \cite{pierce15} make use of functions to encode partial maps from variables to types. Assigning a type involves wrapping the existing environment in another conditional statement, as in \c{insert x $\tau$ H = fun y => if x = y then Some $\tau$ else H y}. This approach is unsuitable for substructural type systems because the function is opaque and can't be disassembled into two functions which are equivalent when combined. Given an arbitrary function, it is impossible to know that is always going to consist of a conditional of the form shown, and therefore it is also impossible to extract any of the information about x, $\tau$ or the original $f$.
\item \textbf{Lists of Types}: We could consider using a list of types so that the type for variable $\hat{i}$ is at index $i$. This is preferable to using a function because we can inspect and destructure a list, and can also perform induction. However, splitting an environment becomes problematic because we need to keep the type for variable $\hat{i}$ at index $i$, even if some or all of the types at indexes less than $i$ should no longer be available because they were assigned to the other side of the split. Essentially, if we are to use a list, we need a filler value to occupy the evacuated positions. This leads to our next option...
\item \textbf{Lists of Optional Types}: What if we rather than using a list of types, we use a list of \c{option type}, so that types which are no longer available are represented by \c{None} entries? This fulfils all of our requirements: we can look-up types, alter them, add new entries and split an environment so that variables and their types are divided between the two new environments.
\end{itemize}

For these reasons, our formalisation makes use of a list of optional types, as provided by the \c{Env} type from DbLib. However, the lemmas about \c{Env} provided by DbLib proved to be insufficient for reasoning about substructural typing rules. For example, DbLib treats the empty list \c{[] (nil)} as the only empty environment, when it is often useful to treat any number of \c{None}s as an empty environment. Further, context splitting defined on \c{Env} is general enough to be of use in multiple DbLib-based formalisations. The next two sections describe these two aspects of our formalisation, and this thesis's contribution to a general framework for substructural languages.

\subsection{Emptiness}

We define the following predicate for environments which is compatible with any environment from DbLib. We say that an environment is \i{empty} if it contains no typing information. Hence, the empty list environment (\c{nil}) is empty, as is any number of \c{None} values.

\begin{minted}{coq}
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons E (EmptyTail : is_empty E) : is_empty (None :: E).
\end{minted}

The necessity of a predicate for emptiness arises from several uses of \i{inductive loading} in proofs of lemmas related to substitution and type preservation. Using \c{nil} environments proved to be too limiting, and there are several cases in the proof where an induction generates an empty environment like \c{raw_insert x None nil}, and has to apply an inductive hypothesis about empty contexts to it.

In order to be useful in the verification of languages, lemmas about the properties of the \c{is_empty} predicate and its interaction with other parts of the system are required. Of these lemmas, the ones involving interaction with DbLib were slightly more difficult to prove than those about simple Coq constructs. Here is a full list of the lemmas relating to \c{is_empty} and constructs other than context splitting. Lemmas about emptiness and context splitting are discussed in the next section.

\begin{minted}{coq}
Lemma empty_repeat : forall A (E : env A),
  is_empty E ->
  E = repeat (length E) None.
(* Proof by easy induction on is_empty E *)

Lemma empty_repeat_none : forall A n,
  is_empty (repeat n None : env A).
(* Proof by easy induction on n *)

FIXME: More here.
\end{minted}

\subsection{Context Splitting}

% You've met context splitting before.
% High-level description.
% Coq code.
% Inductive definition vs function definition (nah).
% Discussion of split single.
% Discussion of difficulty of existence lemmas -> worth having in a library.
% GOOD REASONS WHY insert/tl is frustrating!
% Discussion of similarity to other approaches (de Vries mixin approach).
%    Justification that the definition is general enough.


Context splitting is the operation by which a typing environment, implemented as a \c{list (option T)}, is split so that it may be used to type two expressions. We use the notation $E = E_1 \circ E_2$ to represent the splitting of $E$ into two environments $E_1$ and $E_2$ such that each variable from $E$ appears in only one of $E_1$ and $E_2$.

In the linear lambda calculus, the typing rule for application requires that the context used to type $(e_1 e_1)$ can be split into two contexts that type $e_1$ and $e_2$ individually. This splitting causes variables from the typing context to be used at most once in the expression $(e_1 e_2)$, as each element of the context must be assigned to either the left or right sub-expression.

As mentioned in the section above on emptiness, we require that context splitting preserve the length of the environment being split. In Coq, we define an inductive predicate over contexts so that \c{context_split E E1 E1} $\equiv E = E_1 \circ E_2$.

\begin{minted}{coq}
Inductive split_single {A} : option A -> option A -> option A -> Prop :=
  | split_none : split_single None None None
  | split_left (v : A) : split_single (Some v) (Some v) None
  | split_right (v : A) : split_single (Some v) None (Some v).

Inductive context_split {A} : env A -> env A -> env A -> Prop :=
  | split_nil : context_split nil nil nil
  | split_cons E E1 E2 v v1 v2
      (SplitElem : split_single v v1 v2)
      (SplitPre : context_split E E1 E2) :
      context_split (v :: E) (v1 :: E1) (v2 :: E2).
\end{minted}

\subsubsection{Single Element Splits}

One may wonder if the presence of an accompanying \c{split_single} predicate is necessary, as the same meaning can be achieved with the following definition that places the splitting of single elements inline:

\begin{minted}{coq}
Inductive context_split : env A -> env A -> env A -> Prop :=
  | split_nil : context_split nil nil nil
  | split_left E E1 E2 v
  	  (SplitPre : context_split E E1 E2) :
  	  context_split (v :: E) (v :: E1) (None :: E2)
  | split_right E E1 E2 v
  	  (SplitPre : context_split E E1 E2) :
  	  context_split (v :: E) (None :: E1) (v :: E2).
\end{minted}

Although for the purposes of forward reasoning both definitions are equally convenient, when performing inversion on terms of type \c{context_split E E1 E2} it is often useful to be able to abstract over the splitting of individual elements, particularly as this means there are less cases which have to be handled.

For example, in the proof of \c{insert_none_split_backwards}, using context splitting on single elements saves duplicating or creating automation for the main part of the proof. The statement of the lemma is:

\begin{minted}{coq}
Lemma insert_none_split_backwards : forall A (E : env A) E1 E2 x,
  context_split (raw_insert x None E) E1 E2 ->
  exists E1' E2',
    E1 = raw_insert x None E1' /\
    length E1' = length E /\
    E2 = raw_insert x None E2' /\
    length E2' = length E /\
    context_split E E1' E2'.
\end{minted}

Intuitively, this lemma states that if we have inserted \c{None} into the typing context at the position for variable \c{x}, then the two environments resulting from the split of this environment must also contain \c{None} at position \c{x}. The proof is by case analysis on whether or not the variable inserted is past the end of the environment (\c{x >= length E}). In the case where it \i{is not} past the end, and \c{x} is greater than 0, we reach a goal of the form:

\begin{minted}{coq}
exists X1 X2 : env A,
  e1 :: E1' = raw_insert (S x') None X1 /\
  length X1 = S (length E') /\
  e2 :: E2' = raw_insert (S x') None X2 /\
  length X2 = S (length E') /\
  context_split (e :: E') X1 X2
\end{minted}

Here, we have \c{E = e :: E'}, \c{E1 = e1 :: E1'}, \c{E2 = e2 :: E2'}, \c{x = S x'} relative to the statement of the lemma.

Without a \c{split_single} definition, inversion of \c{context_split (e :: raw_insert x' None E') (e1 :: E1') (e2 :: E2')} forces us to consider the cases where \c{e1 = None, e2 = Some t} and \c{e1 = Some t, e2 = None} separately. In this proof, this isn't an important distinction, because we really just need to apply the inductive hypothesis to obtain \c{E1''} and \c{E2''} such that \c{E1' = raw_insert x' None E1''} and \c{E2' = raw_insert x' None E2''}. We can then instantiate \c{X1} and \c{X2} with \c{exists (e1 :: E1''), (e2 :: E2'')} and knock over the remaining goals with facts from the proof context. This all works with a \c{split_single} definition, but without one, the two cases have to be handled with two different calls to the \c{exists} tactic; requiring duplication of proof script, or custom LTac to parametrise these calls.

Hence, \c{split_single} is a more general and useful way for dealing with context splitting, as it allows us to prove goals of the form \c{context_split (e :: E) (e1 :: E1) (e2 :: E2)} without knowing the exact values of \c{e}, \c{e1} and \c{e2}. If the exact values \i{are} required for a proof, they can still be considered by performing inversion on the \c{split_single e e1 e2} fact.

\subsubsection{Properties of Context Splitting}

We prove several basic properties of context splitting, such as commutativity, as well as lemmas motivated by the proof of soundness for the linear lambda calculus.

\textbf{Length} If $E = E_1 \circ E_2$, then the lengths of all three contexts are the same.

\textbf{Commutativity}

If we can split $E = E_1 \circ E_2$, then we can also split $E = E_2 \circ E_1$. In Coq, proof is by a straight-forward induction on the structure of the \c{context_split}, and depends on a similar commutative property of \c{split_single}.

\begin{minted}{coq}
Lemma split_commute : forall A (E : env A) E1 E2,
  context_split E E1 E2 -> context_split E E2 E1.
Proof with boom.
  intros A E E1 E2 Split.
  induction Split...
Qed.
\end{minted}

\textbf{Associativity} If $E = E_0 \circ (E_1 \circ E_2)$ then $E = (E_0 \circ E_1) \circ E_2$.

In Coq we need an existential to express the existence of a context that splits into $E_0$ and $E_1$.

\begin{minted}{coq}
Lemma split_assoc : forall A (E E0 E1 E2 E12 : env A),
  context_split E E0 E12 ->
  context_split E12 E1 E2 ->
  (exists E01, context_split E E01 E2 /\ context_split E01 E0 E1).
\end{minted}

The justification for calling this operation associativity is the view of $(\circ)$ as a \i{partial} binary operator for combining contexts. From this view, context splitting is a partial commutative monoid, with \c{repeat n None} as the identity element. Further work could try to exploit structure like this to improve proof automation.

\textbf{4-way Splits}

In his proofs about uniqueness typing Edsko de Vries [REF] establishes all of these properties, but proves the associativity lemma using a 4-way split lemma like this: $E = (E_{1a} \circ E_{1b}) \circ (E_{2a} \circ E_{2b}) \Rightarrow E = (E_{1a} \circ E_{2a}) \circ (E_{2a} \circ E_{2b})$. In addition to associativity being provable from this lemma, the opposite is also true, and we prove a version of this lemma called \c{split_swap} using a few simple applications of commutativity and associativity.

\textbf{Rotation} The proof of preservation requires a lemma of the form: $E = E_0 \circ (E_1 \circ E_2) \Rightarrow E = E_1 \circ (E_0 \circ E_2)$, which is provable using the other lemmas above.

\textbf{Emptiness} Lemmas about empty contexts as the identity element for context splitting are provable using a few lines of proof script. For details, see \c{Context.v} (UPDATEME).

% TODO: Note rewriting rules for commutativity somewhere, maybe in the automation section?

\subsubsection{Lemmas Required for Soundness}

In order to prove soundness, several lemmas about the interaction between inserts and context splitting were required. Although many of the lemmas are conceptually simple, some required significant effort and represent a large portion of the work involved in the proof of soundness for the linear lambda calculus. Many lemmas also involve the insertion of \c{None} values into contexts, which is a consequence of the strengthened induction required to prove the substitution lemma (see the next section on substitution (TODO)).

The first, and perhaps most difficult to prove, was \c{insert_none_split_backwards}, discussed previously in the section about \c{split_single}. Refer to the statement of the lemma above (FIXME?). We attribute the difficulty in proving the lemma to the fact that we initially lacked ways to express many of the intuitive reasons for the lemma's truth. With the right definitions and lemmas in place, the proof became quite straight-forward.

DbLib's \c{insert} function behaves in two different ways depending on the relationship between the variable \c{x} being inserted and the length of the existing environment, \c{E}. If \c{x < length E}, then the type for \c{x} is inserted \i{between} existing elements, with subsequent elements being shunted along. Conversely, if \c{x >= length E}, then the type of \c{x} is inserted \i{after} existing elements, with the intervening space padded by \c{None} values. These two cases are quite different to reason about, and any attempt at a direct inductive proof on \c{x} or one of the environments inevitably leads to wanting to know which of the two cases one is considering. For this reason, the first step of our proof is to split on the comparison between \c{x} and \c{length E}.

To handle the case where \c{x >= length E} and padding \c{None} values are inserted, we note that the values past the end of the old environment will all be \c{None}. Equipped with a simple definition of a \c{repeat} function on lists, and some lemmas about list append, this case is proved.

\begin{minted}{coq}
Lemma insert_none_def : forall A x (E : env A),
  x >= length E ->
  raw_insert x None E = E ++ repeat (S (x - length E)) None.

Lemma split_app : forall A (E : env A) E1 E2 n,
    context_split (E ++ repeat n None) E1 E2 ->
    exists E1' E2',
      E1 = E1' ++ repeat n None /\
      E2 = E2' ++ repeat n None /\
      context_split E E1' E2'.
\end{minted}

The other case where the new \c{None} value is inserted in between existing elements is handled by an induction on \c{x}, which is made simpler by knowing the bound on \c{x}, i.e. \c{x < length E}. For example, it absolves the prover from having to deal with the case where the environment is empty, which proved to be a nuisance in early versions of the proof. Particularly as \c{length (tl E1) = length (tl E2)} does not imply that \c{length E1 = length E2} as one may expect outside a total programming language.

TODO: More here about the other couple of insertion/split lemmas.

% New plan:
% Talk about insert_none_split_backwards, context_split_insert.
% Talk about substitution lemma.
% Talk about preservation.

\section{Typing Rules}

Typing rules determine which terms are considered well-formed. We define an inductive predicate \c{has_type : (env ty) -> term -> ty -> Prop} so that \c{has_type E e t} is inhabited if the environment $E$ determines $e : t$. In standard mathematical notation this is $E \types e : t$, which we mirror using the Coq notation \c{E |- e ~: t}.

\begin{minted}{coq}
Reserved Notation "E '|-' e '~:' t" (at level 40).

Inductive has_type : (env ty) -> term -> ty -> Prop :=
  | HasTyUnit E
      (UnitEmpty : is_empty E) :
      E |- TUnit ~: TyUnit
  | HasTyTrue E
      (TrueEmpty : is_empty E) :
      E |- TTrue ~: TyBool
  | HasTyFalse E
      (FalseEmpty : is_empty E) :
      E |- TFalse ~: TyBool
  | HasTyVar E x t
      (VarPre : is_empty E) :
      insert x t E |- TVar x ~: t
  | HasTyAbs E e t1 t2
      (AbsPre : (insert 0 t1 E) |- e ~: t2) :
      E |- TAbs t1 e ~: (TyFun t1 t2)
  | HasTyApp E E1 E2 e1 e2 t1 t2
      (AppPreSplit : context_split E E1 E2)
      (AppPreWT1 : E1 |- e1 ~: TyFun t1 t2)
      (AppPreWT2 : E2 |- e2 ~: t1) :
      E  |- TApp e1 e2 ~: t2

where "E '|-' e '~:' t" := (has_type E e t).
\end{minted}

Here we see the definitions for emptiness and context splitting coming into play. In linear lambda calculus variables must be used exactly once, so a single variable \c{x} is well-typed only under an environment containing a type for \c{x} and nothing else, as captured by the \c{HasTyVar} rule. Similarly, primitive values must be typed under environments containing no free variables.

A lambda abstraction $(\lambda \hat{0} : \tau. \; e)$ is well-typed if the body can be typed under an environment extended by the type for its binder, $(0 : \tau)$. This is the only rule that requires the \i{input} context (\c{insert 0 t1 E}) to differ in length to the \i{output} context (\c{E}).

Function applications make use of the \c{context_split} operation to ensure that the variables used to type an application $(e_1 e_2)$ are split between $e_1$ and $e_2$ without duplication. The rule also ensures that the type of the application (\c{t2}) is consistent with the type of the function being applied (\c{t1 -> t2}) and the type of the argument (\c{t1}).

\section{Operational Semantics}

For the semantics of our language we use a call-by-value small step semantics. We represent the semantic rules in Coq as an inductive relation \c{step : term -> term -> Prop} so that if \c{step e e'} is true, \c{e'} is the result of partially evaluating \c{e}.

\begin{minted}{coq}
Inductive step : term -> term -> Prop :=
  | StepAppAbs e e' v t
      (BetaPreVal : value v)
      (BetaPreSubst : subst v 0 e = e') :
      step (TApp (TAbs t e) v) e'
  | StepApp1 e1 e1' e2
      (App1Step : step e1 e1') :
      step (TApp e1 e2) (TApp e1' e2)
  | StepApp2 v1 e2 e2'
      (App2Val : value v1)
      (App2Step : step e2 e2') :
      step (TApp v1 e2) (TApp v1 e2').
\end{minted}

The first rule, \c{StepAppAbs} is the familiar $\beta$-rule for evaluating the application of an abstraction to a value. It states that function application is equivalent to the substitution of the argument value for the bound variable in the body of the function. For substitution, note the use of DbLib's \c{subst} function. In mathematical notation the rule is: $(\lambda 0 : \tau. e) \; v \rightarrow e[v/0]$.

We are using a \i{call-by-value} evaluation strategy, which means that only the outer-most reducible expressions (\i{redexes}) are reduced, and only when their argument is a value \cite{tapl}. Note that as a result there is no rule for stepping a $\lambda$-abstraction when its body is capable of stepping, as in: $e \rightarrow e' \implies (\lambda 0 : \tau. \, e) \rightarrow (\lambda 0 : \tau. \, e')$.

The two rules for stepping an application $(e_1 \; e_2)$ ensure that $e_1$ is fully evaluated before evaluating $e_2$. This is captured by the \c{value v1} argument to \c{StepApp2}. By making this restriction an evaluator never needs to choose which side should be evaluated first, thus removing non-determinism. If we include the $\beta$-rule in our consideration we see that the stepping relation as a whole is deterministic, although this property isn't verified in our Coq proofs.

\section{Progress}

With typing rules and small step semantics established we can state the progress lemma which contributes to the proof of type soundness.

\begin{minted}{coq}
Theorem progress : forall E e t,
  is_empty E ->
  E |- e ~: t ->
  (exists e', step e e') \/ value e.
\end{minted}

This theorem states that any closed term $e$ that is well-typed can either be stepped using the small step semantic relation, or is already a value and cannot be evaluated further.

Proof is by induction on the typing derivation \c{E |- e ~: t}. Unlike the proof of preservation presented in the next section, the proof of progress requires very few supporting lemmas about context splitting or substitution. Most of the cases in the induction require reasoning about the interaction between typing and term structure, which can be handled by simple inversions. For example, one of the supporting lemmas states that any value assigned a function type under an empty environment, must necessarily be a $\lambda$-abstraction:

\begin{minted}{coq}
Lemma fun_value_is_abs : forall E e t1 t2,
  is_empty E ->
  E |- e ~: TyFun t1 t2 ->
  value e ->
  (exists e', e = TAbs t1 e').
\end{minted}

Given that most of the verification effort was expended reasoning about context splitting, substitution and other lemmas required for preservation, the effort required to prove progress represents a relatively small fraction of the total effort. The fact that progress and preservation each form ``half" of the soudness proof does not imply that the difficulty of proving soundness is split evenly between them.

\section{Preservation}

The preservation lemma states that a if well-typed closed term $e$ can take a step to another term $e'$, then $e'$ is also well-typed. In other words, the type of a term is \i{preserved} as it is evaluated in accordance with the small-step semantics.

\begin{minted}{coq}
Theorem preservation : forall E e e' t,
  is_empty E ->
  E |- e ~: t ->
  step e e' ->
  E |- e' ~: t.
\end{minted}

Proof is by induction on the stepping of $e$ to $e'$, \c{step e e'}. In the three cases that result from the three stepping rules, the two for applications are proved directly from the inductive hypothesis. The remaining case for $\beta$-reduction involves interaction between substitution, typing and context splitting, and is proved via a supporting \i{substitution} lemma.

\subsection{Substitution}

The substitution lemma states that the result of a substitution is well-typed if the terms involved are well-typed. With substructural typing, we must also consider the supply of free variables to both terms using context splitting, so the lemma takes the form:

\begin{eqnarray*}
\infer[\text{Substitution}]{
    E_1 \circ E_2 \types e_2 [e_1/x] : \tau_2
}{
    E_1 \types e_1 : \tau_1 \quad E_2, x : \tau_1 \types e_2 : \tau_2
}
\end{eqnarray*}

Substitution lemmas similar to this are common in proofs of similar complexity, as in Software Foundations (REF) and the examples accompanying DbLib. In Software Foundations, a weakening lemma is used in the proof of the substitution lemma, but with substructural typing this technique is unavailable.

In Coq the lemma is:

\begin{minted}{coq}
Lemma substitution: forall E2 e2 t1 t2 x,
  insert x t1 E2 |- e2 ~: t2 ->
  forall E E1 e1, E1 |- e1 ~: t1 ->
  context_split E E1 E2 ->
  E |- (subst e1 x e2) ~: t2.
\end{minted}

Proof is by \i{dependent induction} on the judgement \c{insert x t1 E2 |- e2 ~: t2}. Dependent induction allows us to take into account the fact that the environment is \c{insert x t1 E2} rather than an unadorned variable (e.g. $E$). This is achieved by replacing instantiated variables with general ones, and then adding constraining equalities. In this case, the only instantiated variable is \c{insert x t1 E2}, which dependent induction will replace by a new universally quantified variable $E_\text{new}$ and the equation $E_\text{new} = \c{insert x t1 E2}$. With the goal in the form described the dependent induction tactic then applies the induction principle for \c{has_type} to generate subgoals for each of the cases, whilst preserving the newly added equality constraints. Coq's dependent induction is based on Conor McBride's \c{BasicElim} tactic (REF) which makes use of Conor's humorously named ``John Major" heterogenous equality (\c{JMeq}). Heterogenous equality requires the addition of an extra axiom, the consideration of which is discussed in the Evaluation chapter.

Simple inversion removes the cases that are absurd due to the \c{insert x t1 E2} environment, leaving three cases for variables, $\lambda$-abstractions and applications. The variable case is handled by some straight-forward reasoning about empty contexts, and requires no new supporting lemmas, while the other two cases form the motivation for several of the lemmas about inserts and context splitting.

The $\lambda$-abstraction case requires a proof that the term being substituted is well-typed under the environment for the abstraction sans binder, i.e. \c{(None :: E1) |- shift 0 e1 ~: t1}. This motivates \c{typing_insert_none}, which in turn motivates \c{insert_none_split}. Although \c{typing_insert_none} is more general in that it allows us to prove facts of the form \c{raw_insert x None E |- e ~: t} and not just \c{raw_insert 0 None E |- e ~: t}, generalising for all $x$ makes related lemmas easier to prove by enabling induction on $x$. This technique is sometimes referred to as \i{inductive loading}.

The application case requires that one side of the application be well-typed without referring to the substitution variable $x$. This motivates the following lemma, \c{typing_insert_none_subst}:

\begin{minted}{coq}
Lemma typing_insert_none_subst : forall E e x junk t,
  raw_insert x None E |- e ~: t ->
  E |- subst junk x e ~: t.
\end{minted}

This in turn motivates the rest of the lemmas about inserting none into a typing context, including the difficult to prove \c{insert_none_split_backwards} -- discussed above.

To prove this lemma, DbLib was extended with a \i{lowering} operation that is conceptually inverse of lifting. In the case where variables are lowered by 1 we call the operation \i{unshifting}, by analogy with lifting by 1 (shifting). The lemma is proved by establishing that a similar lemma holds for \c{unshift x e}, and an equivalence of \c{unshift x e} and \c{subst junk x e} when \c{x} does not appear free in \c{e} (see \c{contains_var}).

\chapter{Evaluation}

In this section we assess the quality of the work according to the Evaluation Framework of section (REF). We discuss the success of the library according to its conceptual goals, primarily by outlining how a formalisation of $L^3$ could make use of the library. Further, we evaluate the library's implementation according to the implementation goals and discuss possible improvements.

\section{Generality and Applicability}

% Further, we require that splitting a linear context $E = E_1 \circ E_2$ results in every assumption from $E$ appearing in $E_1$ or $E_2$ but not both.

According to the conceptual goals of our evaluation framework, we would like the library to be applicable to the formalisation of languages more complex than the linear lambda calculus. In concrete terms, this means that the context splitting operation provided by the library should be useful in constructing syntactic proofs of soundness for some class of languages with substructural typing. We argue that this class of languages includes those based on DILL -- which could act as a foundation for further formalisations.

Extending the proof-of-concept LLC proof to a complete formalisation of DILL would require the addition of an intuitionistic context, product types and bang types. The paper for DILL (REF) includes proofs of substitution lemmas similar to the one used in the proof of preservation for our LLC, which suggests that it would admit a complete proof of soundness in a syntactic style. This is in contrast to several earlier systems based on linear logic, which Philip Wadler showed \i{do not} have substitution lemmas (REF No substitute for linear logic). Wadler's result is our primary motivation for using DILL as a foundation, motivated further by Pottier's success with DILL-based syntactic soundness proofs in SSHPS (REF).

The proofs of progress and preservation for DILL could re-use much of the proof effort for the LLC. For example, the ($\otimes$-I) rule is similar to the LLC's existing ($\lolly$-E) rule, implying that inductive cases related to ($\otimes$-I) could be handled using the same library lemmas used for ($\lolly$-E). Further, the lemmas provided by the library about \c{insert} and context splitting would aid in reasoning about ($\otimes$-E) and (!-E), which both include augmented contexts in their premises.

Our approach may also be applicable to a modified version of Edsko de Vries' uniqueness type system (REF, REF). Edsko's soundness proof is syntactic and makes use of an inductive context splitting relation that is identical to ours except that it allows non-unique (intuitionistic) types to be split to both sides. To make his system compatible with purely linear context splitting a stand-alone contraction rule could be added, in the style of Wadler's linear lambda calculus of 1993 (REF). The contraction rule would provide the ability to duplicate non-unique values, which would previously have been provided by context splitting.

% TODO: Move this to background section/further work/conclusions.
% Implicit in the criteria for applicability is the idea that the language uses a traditional typing environment where variables either possess a type or are entirely absent. This is not true of Conor McBride's recent work on combining linear and dependent typing (REF), which annotates variables in the context by the number of occurrences available at run-time. Linear variables are annotated with a $\b{1}$ if available, or a $\b{0}$ if they have already been used in a neighbouring part of the term. This allows types to depend upon linear values, by referring to the $\b{0}$ available copies if necessary -- a process Conor calls \i{contemplation}. In this system, context splitting is the division of a context such that the sum of the resources of the resulting sub-contexts equals the resources of the original. This is incompatible with context splitting as we've defined it because assumptions involving variables unavailable at run-time are replaced by information-free \c{None} values. Further work could develop a software library for typing contexts in Conor's style, which would be general enough to handle all of the languages handled by our library.

\section{Towards a Coq Formalisation of $L^3$}

\i{The Linear Language with Locations}, $L^3$, is a good candidate for assessing the applicability of our approach to languages beyond the linear lambda calculus. As argued in the Background chapter, its use of linear capabilities and shared pointers is typical of other modern calculi supporting destructive updates.

\Lthree seems like it may be amenable to mechanical verification with our library because like DILL and Wadler's linear lambda calculus, contexts are split in a purely linear way. Intuitionistic terms are handled by the $\dupl{t}$ and $\drop{t}$ primitives, rather than implicit or explicit contraction and weakening, which means the \c{context_split} predicate could be employed unaltered. However, as noted in the $L^3$ paper (REF), the operational semantics and typing rules are not set-up for a syntactic proof of soundness. As an example, consider the preservation lemma for the memory allocation primitive $\lnew{v}$. Under the operational semantics the term steps unconditionally: $(\sigma, \lnew{v}) \steps (\sigma \uplus \{l \to v\}, \qpair{l}{\capa \otimes !(\ptr{l})})$, hence we should have:
\begin{eqnarray*}
\infer{\Delta; \Gamma \types \qpair{l}{\capa \otimes !(\ptr{l})} : \Exists{\rho} A}{
  \Delta; \Gamma \types \lnew{v} : \Exists{\rho} A  & (\sigma, \lnew{v}) \steps (\sigma \uplus \{l \to v\}, \qpair{l}{\capa \otimes !(\ptr{l})})
}
\end{eqnarray*}

The premises are both true, by the rules (New) and (new), but the conclusion is unprovable. It might seem that we could apply the (LPack) rule, but this would require $l \in \Delta$, which is nonsensical because $l$ is a location constant, not a location variable. Further, we can't type the pair $\capa \otimes !(\ptr{l})$ because there are no typing rules for lone capabilities or pointers. To do a syntactic proof of soundness would therefore require altering $L^3$'s typing rules. The authors of the $L^3$ paper suggest using \i{store typing}, as in Alias Types (REF), whereby constraints on the runtime store $\sigma$ are expressed in the typing rules. In this case, reasoning about context splitting would likely only comprise a small fraction of the proof effort. Pottier's proof for \SSPHS and the proof for \rgnUL are around 20,000 lines of Coq and Twelf code respectively, implying that the complexity of reasoning about these language features increases proof complexity significantly. For comparison, our linear lambda calculus formalisation is around 1,300 lines of Coq code including the library for context splitting.

% TODO: Could list things that *would* work, things that wouldn't here.
%\begin{itemize}
%\item Modify the typing rules to use dual intuitionistic and linear contexts.
%\item Augment the typing rules with a location environment, $\Delta$, implemented as another DbLib environment with nullary type information, i.e. \c{env () = list (option ())}. This representation would allow checking if the de Bruijn index $\rho$ for a location variable is in scope by checking if the location environment contains \c{Some ()} at index $\rho$. Further, the \c{insert} function and other utilities provided by DbLib could be used to manipulate the location environment, as is required by the (Let-LPack) rule for example.
%\item Implement capture-avoiding location substitution by defining another instance of the \c{Traverse} type-class for terms. Coq's named instances would prevent a conflict with the instance for term substitution. Location, err...
%\end{itemize}

\section{Quality of Coq Proofs}

As identified in the Implementation Goals (REF) section of our evaluation criteria, we would like the library to be both \b{readable} and \b{maintainable}.

\subsection{Case Analysis and Indentation}

To separate different cases when performing induction and destructuring, the proofs make use of Benjamin Pierce's \c{Case} markers from Software Foundations (REF). Combined with indentation, these markers fulfil our desire to create readable proofs that are also resistant to corruption upon refactoring. The \c{Case} tactic ensures that the proof remains structured by failing if an existing case at the same level remains unproven, as can happen if an earlier tactic fails. The following example taken from the proof of \c{insert_none_def} demonstrates our usage of the markers and our indentation scheme:

\begin{minted}{coq}
induction x as [|x']; intros.
Case "x = 0".
  destruct E as [|e E'].
  SCase "E = []".
    rewrite raw_insert_zero...
  SCase "E = e :: E'".
    solve by inversion.
\end{minted}

If, for example, the tactic \c{rewrite raw_insert_zero...} fails to prove the goal for the case where \c{E = []}, then the \c{SCase "E = e :: E'"} tactic will fail and prevent the proof from proceeding, clearly signalling that the error lies in the previous case. The string arguments are uninterpreted but provide useful documentation.

The style of proof shown above was followed meticulously throughout the entire development, aiding both readability and maintainability. Excessive nesting of cases was also successfully avoided, with the deepest level of nesting being 3 sub-cases deep (an \c{SSCase}), occurring only once in the proof-of-concept formalisation.

An alternative to using Pierce's \c{Case} markers would have been to use Coq's \i{bullets}, which are available as part of Coq's core since version 8.4 (REF). They function identically to the case markers except that documentation strings can't be embedded as case descriptions.

\subsection{Avoiding Auto-Generated Variable Names}

Avoiding references to automatically generated names is an important part of creating a readable and maintainable proof. Generated names are subject to change between Coq versions, which renders all pieces of proof script reliant on them in need of upgrading. Furthermore, updating the proof can be difficult if the exact values that the names were referring to have been forgotten, effectively requiring old goals to be solved anew.

Although there are several straight-forward techniques that can be used to avoid generated names, in practice we didn't manage to avoid them entirely, notably when using dependent induction.

\subsubsection{Introduction Patterns}

When performing induction it is often necessary to leave some hypotheses as premises of the inductive hypothesis. In order to avoid generating names for these hypotheses we followed a pattern whereby the \c{intros} tactic would be used to name every hypothesis, \c{generalize dependent} would be used to re-quantify the necessary variables, and then induction performed. This leads to hypotheses being re-introduced with the names used by \c{intros}, rather than generated ones. For example:

\begin{minted}{coq}
Lemma empty_lookup : forall A x (E : env A), is_empty E ->
  lookup x E = None.
Proof.
  intros A x E Empty.
  generalize dependent E.
  induction x as [|x'].
  Case "x = 0".
    intros. inversion Empty; auto.
  (* Proof continues... *)
\end{minted}

Here the fact \c{is_empty E} is named \c{Empty} by \c{intros}, abstracted over, and then automatically re-introduced with the name \c{Empty}. This relies partly on Coq's name generator to remember the name from the first application of \c{intros}, but this is less fragile than relying on entirely automatic naming.

\subsubsection{Named Constructor Arguments}

A cosmetic variation of the above pattern for introductions could use named function arguments instead of an explicit \c{forall}. This is the approach taken for all inductive constructors, with the aim of generating unique names during inversion. For example, we can declare the \c{is_empty} predicate in two semantically equivalent ways:

\begin{coqcode}
(* With implicit argument names *)
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons : forall E, is_empty E -> is_empty (None :: E).

(* With explicit argument names (preferred) *)
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons E (EmptyTail : is_empty E) : is_empty (None :: E).
\end{coqcode}

With the second variant, when inverting a fact of the form \c{is_empty E} the fact about the environment's tail will be named \c{EmptyTail} if that name isn't already taken. In practice this was found to be quite effective, as most proofs required only a single inversion per data-type. In cases where more control over naming is needed, names can be provided to the tactic, as discussed in the next section.

\subsubsection{Named Destructuring}

Named destructuring allows names to be provided to tactics like \c{induction}, \c{destruct} and \c{inversion} which would otherwise generate names automatically. For simple cases like induction on natural numbers and lists, we found it to be highly effective, e.g. \c{induction n as [|n']}. However, destructuring large chains of existentials, conjunctions and disjunctions in this manner quickly becomes unwieldy.

For example, our proof of soundness for the linear lambda calculus contains a line: \c{destruct AppPreSplit as [E1' [E2' [? [? [? [? ?]]]]]]}. This is to the detriment of both readability and maintainability, in particular because changes to the lemma that forms \c{AppPreSplit} would require digging into several layers of brackets to ensure the names are correct and that the chain of conjunctions is fully decomposed. This is an area where the library falls short of the evaluation criteria. Further work could seek to eliminate maintenance burdens such as these, possibly using Arthur Charguraud's improved tactics library, \c{LibTactics} (REF).

\subsubsection{Dependent Induction}

Dependent induction is used at one point in the proof of soundness for the LLC, as discussed in section (REF). Unlike the regular induction tactic, Coq's dependent induction tactic doesn't provide a way to explicitly name the variables introduced by the inversion -- there is no \c{dependent induction x as [..]}. In our proof, we fall back on using the auto-generated names, which is less than ideal. We experienced the fragility of this approach when refactoring, which demonstrates the utility of our other work to avoid generated names.

\subsubsection{Overview of Efforts to Avoid Generated Names}

% TODO.
% Auto-generated names suck.

\subsection{Automation and Repetition}

We found that repeated sections of proof script could be de-duplicated in one of two ways: either by writing a lemma to encapsulate the common truth, or by using Coq's tactic language (Ltac) to repeat the same steps of reasoning. Of these, we found writing a lemma to be preferable as we found Ltac code more difficult to debug. In this section we discuss the efficacy of the automation employed by our proofs, and suggest possible improvements.

In his book \i{Certified Programming with Dependent Types} (REF), Adam Chlipala advocates:

\i{``The more uninteresting drudge work a proof domain involves, the more important it is to work to prove theorems with single tactics."}
\i{``I like to say that if you find yourself caring about indentation in a proof script, it is a sign that the script is structured poorly."}

This is contrary to many of the steps taken by our proof to remain readable, including the use of case markers and indentation discussed above. This is primarily because constructing a proof in Chlipala's style is easiest when following his approach from the outset. Our proof began in a primarily manual style, with modest uses of the \c{auto} tactic with explicit hint lemmas. Mid-way through the development a hint database was created with an accompanying specialised version of \c{auto} called \c{boom}.

Adapting the proof to use \c{boom} did remove repetition in some places, but was quite time-consuming for the savings achieved. The auto tactic works by repeatedly calling the \c{apply} tactic with lemmas from the hint databases and the current set of hypotheses. Unfortunately, many manual proofs are not structured as a straight-forward series of lemma applications, which means significant effort is often required to adapt proofs for use with auto. In our proofs, rewrites and inversion were the most significant barriers to automation with auto because they don't map cleanly onto stand-alone lemmas. To simulate a rewrite with a lemma requires restating the goal before and after the rewrite, which leads to a lemma for each goal and rewrite-rule pair. Similarly, the outcome of an inversion is usually a set of equalities about the components of the term being inverted, which can be used in a myriad of ways.

That said, the number of rewrites in our proof-of-concept is exacerbated by our choice to keep DbLib's definitions \i{opaque}. By default, DbLib exports the functions for lookups and inserts as opaque to the Coq simplifier. This means that an explicit rewrite is required every time a lookup or insert needs simplification -- the \c{simpl} tactic has no effect. DbLib makes the definitions opaque by default to prevent \i{``fragile simplifications"}, and it is possible to selectively make definitions transparent again. If starting the proof again from scratch or reworking the proof to be more automated, we believe it would be best to make the definitions transparent for the whole project, with opt-in opacity where it is required to deal with odd simplification behaviour. This would remove rewrites and make \c{auto} applicable in more places.

As well as \c{auto}, Coq also includes an \c{autorewrite} tactic for repeatedly rewriting using a collection of rewrite rules. Use of this tactic could be investigated as an alternative to making definitions transparent, although it wouldn't solve the problem of automatically simplifying hypotheses, which we get for free with transparent definitions and the simplify-everywhere tactic, \c{simpl in *}.

Although automating proofs extensively reduces proof-effort and therefore makes larger developments feasible (REF Chlipala), it is not without downsides. We found debugging Ltac code quite difficult and labour-intensive. For example, Ltac's semicolon operator which makes it possible to ``pipe" the results of tactics into each other, simultaneously makes debugging the middle of a pipeline difficult. The debugging mode for Ltac (\c{Set Ltac Debug}) doesn't show intermediate results in semicolon pipelines and also isn't available in CoqIDE (or Vim). This leaves one with no option but to break the pipeline apart into individual tactic applications. If a tactic is failing in only a few branches this also requires the temporary use of the \c{admit} tactic to navigate to the failing cases. Once the error is found and fixed, one then has to glue the tactics back together. This debugging experience is unarguably less than ideal, and is an inevitable consequence of automating heavily using current tools. Our proof walks the line between difficult-to-debug Ltac code and repetitive manual proof, erring slightly on the side of manual proof. Further work could develop improved debugging facilities for Coq tactics, possibly using a graphical interface to convey the branching into different cases.

% Auto-rewrite, pattern-matching Ltac, etc.

% Debugging semi-colons.

%\section{Time Efficiency}

%The library and proof-of-concept were both very time consuming to create. We attribute this both to inexperience with Coq's best practices and the fact that DbLib's environment type is ill-suited for proofs about context splitting. The first point, of inexperience, can only be mitigated by practice and study, which have both been provided by this project. The second deserves a more thorough discussion.

%In our previous discussion about representing typing contexts (see REF), we saw the myriad.

% Inexperience, not the best abstraction, DbLib's environment ill-suited to proofs about linear languages due to odd insert behaviour, etc.

% Very time consuming to create. Partly due to the DbLib list env not being well-suited to linearity.
% Definitions are not trivial to write proofs about,

\section{Summary of Evaluation}

\chapter{Conclusion}

%% chapters in the ``backmatter'' section do not have chapter numbering
%% text in the ``backmatter'' is single spaced
\backmatter
\pagebreak
\bibliographystyle{alpha}
\bibliography{pubs}

\chapter{Appendix}

DbLib version (git commit hash): blah

\end{document}
