\documentclass[]{unswthesis}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{multicol}
\usepackage{proof}
\usepackage{centernot}
\usepackage{backref}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{minted}
\usepackage{csquotes}
\usepackage{underscore}
\usepackage{tabto}
\usepackage{dsfont}
\usepackage{cleveref}

% Nicely coloured links.
\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={blue!80!black},
    urlcolor={blue!80!black}
}

% Nice reference symbols.
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefformat{section}{\S#2#1#3}

% Put boxes around figures? Maybe.
\usepackage{float}
%\floatstyle{boxed}
\restylefloat{figure}

% Minted font size.
\newminted{coq}{fontsize=\small}

% Abstract env.
\newenvironment{abstract}
 {
  \begin{center}
  \bfseries \abstractname\vspace{-.5em}\vspace{0pt}
  \end{center}
  \list{}{
    \setlength{\leftmargin}{.5cm}%
    \setlength{\rightmargin}{\leftmargin}%
  }%
  \item\relax}
 {\endlist}

% Tick symbol.
\def\tick{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\newcommand{\cross}{$\times$}

%%% Class options:

%  undergrad (default)
%  hdr

%  11pt (default)
%  12pt

%  final (default)
%  draft

%  oneside (default for hdr)
%  twoside (default for undergrad)


%% Thesis details

\thesistitle{A Library Based Approach to the Verification of Languages with Linear Types}
\thesisschool{School of Computer Science and Engineering}
\thesisauthor{Michael Alexander Sproul}
\thesisZid{z3484357}
\thesistopic{3680}
\thesisdegree{Bachelor of Science (Honours)}
\thesisdate{May 2016}
\thesissupervisor{Dr. Ben Lippmeier}

%% My own LaTeX macros, definitions, etc.

% Names
\newcommand{\rgnUL}{$\lambda^\text{rgnUL}$\text{ }}
\newcommand{\SSPHS}{\text{SSPHS }}
\newcommand{\Francois}{Fran\frenchc{c}ois }

% Logic/set theory.
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\newcommand{\Forall}[1]{\forall #1 \ldotp \,}
\newcommand{\Exists}[1]{\exists #1 \ldotp \,}

% Linear lambda calculus.
\newcommand{\lam}[1]{\lambda #1 \ldotp \,}
\newcommand{\Lam}[1]{\Lambda #1 \ldotp \,}
\newcommand{\app}[2]{(#1 \; #2)}
\newcommand{\lolly}{\multimap}
\newcommand{\types}{\vdash}
\newcommand{\steps}{\Rightarrow}

% Let bindings.
\newcommand{\letbe}[3]{\textbf{let $#1$ be $#2$ in $#3$}}

% Syntax or.
\newcommand{\sor}{\; | \;}

% L^3 stuff.
\newcommand{\Ptr}[1]{\textbf{Ptr $#1$}}
\newcommand{\ptr}[1]{\textbf{ptr $#1$}}
\newcommand{\Capa}[2]{\textbf{Cap $#1$ $#2$}}
\newcommand{\capa}[1]{\textbf{cap $#1$}}
\newcommand{\lquine}{\left\ulcorner}
\newcommand{\rquine}{\right\urcorner}
\newcommand{\dupl}[1]{\textbf{dupl $#1$}}
\newcommand{\drop}[1]{\textbf{drop $#1$}}
\newcommand{\lnew}[1]{\textbf{new $#1$}}
\newcommand{\lfree}[1]{\textbf{free $#1$}}
\newcommand{\lswap}[2]{\textbf{swap $#1$ $#2$}}
\newcommand{\qpair}[2]{\lquine #1, #2 \rquine}
\newcommand{\semrule}[1]{\text{(#1)}}

% Pure LLC stuff.
\newcommand{\TyPrim}[1]{\b{TyPrim } #1}
\newcommand{\TPrim}[1]{\b{TPrim } #1}

% Text formatting.
\let\frenchc\c
\let\c\texttt
\let\oldi\i
\let\i\textit
\let\oldb\b
\let\b\textbf
\let\oldt\t
\let\t\text

\begin{document}

%% Roman numeral numbering in this section.
\frontmatter
\maketitle

% Mention:
% DbLib, context splitting, DILL, L^3.

\begin{abstract}
We present a library of proofs to aid in the mechanical verification of languages with substructural type systems. We identify \i{context splitting} as a widely applicable technique in the verification of these languages, and argue that our proofs are relevant to any language that uses multiple contexts to separate linear assumptions from non-linear ones, in the style of Dual Intuitionistic Linear Logic. Our work is implemented using the Coq interactive theorem prover and builds upon \Francois Pottier's DbLib library for variable binding using De Bruijn indices. We provide a proof of type soundness for a linear lambda calculus and highlight how similar techniques might be applied to \i{The Linear Language with Locations}, $L^3$ -- which is representative of a class of more complex languages with destructive updates.
\end{abstract}

\newpage

\section*{Acknowledgements}

I would like to thank my supervisor, Ben Lippmeier, for his sage advice and encouragement. I would also like to thank the awesome Programming Languages and Systems research group at UNSW for stimulating my imagination and supporting my project. In particular, I'm grateful to Gabi and Manuel for their leadership and for making it all possible. I would also like to thank my family, friends and Lily for supporting me throughout and keeping me grounded. Thank you.

\tableofcontents

%% Chapter numbers and normal page numbering.
\mainmatter

\chapter{Introduction}
\label{ch:intro}

Computer systems form an integral part of modern society, both in the form of personal devices and critical infrastructure. Ensuring the correct operation of computer hardware and software is therefore a worthwhile endeavour. One emerging technique for the construction of robust software systems is the use of mathematical formalisations and proofs of correctness. In this paradigm, desirable properties of the software can be proven true using a computer-based \i{proof assistant}, which itself relies on a minimal amount of trusted code. For software formalisation and verification to be truly effective, the objects under consideration must have precise mathematical models associated with them. Typically these models are created based on the \i{semantics} (meaning) of the programming language that the software is written in. Unfortunately for the would-be software verifier, most popular programming languages lack formal semantics and are therefore not amenable to verification techniques.

The C Programming Language, in which large amounts of low-level \i{systems software} is written, is an example of a language that is difficult to reason about because of its murky semantics. According to the language specification the results of some operations are classified as \i{undefined behaviour}, meaning that the compiler has no obligation to produce a specific result \cite{isoc}. Ideally, we would like to rule out undefined behaviour at compile time through the use of a type system. In this thesis we focus on the formal semantics of languages with strong type systems that are more well-suited to low-level software verification than C.

In particular, we are interested in \i{resource-aware} languages which exploit \i{substructural} type systems based on linear logic \cite{girard87} to track how values are consumed and destroyed. Intuitively, a \i{linear} value is one that is guaranteed to be used exactly once. In the context of systems software, we can use variants of linearity to enable destructive updates for uniquely reference values, or automatic memory management without garbage collection.

We begin by discussing the linear lambda calculus (LLC), which is a simple extension of $\lambda$-calculus with a linear type system. We then go on to describe more expressive languages which include features like shared pointers and destructive updates.

The \b{main contribution} of this thesis is a software library for the interactive theorem prover Coq. The library includes definitions and lemmas primarily about \i{context splitting}, which is a technique used in many substructural type systems. Our code is an extension of \Francois Pottier's DbLib library which aims to de-duplicate the effort required to reason about variable binding, just as we hope to de-duplicate the effort required to reason about linear typing.

As a proof-of-concept for our approach we have undertaken a proof of \i{type soundness} for a linear lambda calculus. Type soundness is a key result in establishing the sensibility of a language and we argue that our approach is also applicable to soundness proofs for more complex languages with the same foundations. Specifically, we argue that a proof of soundness for \i{The Linear Language with Locations} ($L^3$) might make effective use of our library, and that $L^3$ is representative of languages that build upon the linear lambda calculus by adding destructive updates and shared pointers.

% Background.
\chapter{Background}
\label{ch:background}

\section{Coq and the Curry-Howard Correspondence}
\label{sec:curry_howard}

Recall that the Curry-Howard correspondence establishes an equivalence between logical systems and type systems for programming languages. By considering logical propositions as types, the proof of a proposition $P$ can be given by constructing a value of type $P$ in the equivalent programming language. The Coq proof assistant provides a dependently typed programming language with inductive data-types that allows complex propositions to be expressed and proved in this manner. Coq proofs make use of \i{tactics} which abstract over repetitive reasoning.

\section{Beyond C}

Historically, low-level \textit{systems software} has been written primarily in the C programming language, which was originally designed without a formal semantics. Despite this, a large body of work has developed around semantics for C and the verification of low-level software that such work enables. Michael Norrish's contribution of a structural operational semantics for C \cite{norrish98} is notable in that it laid the foundations for C verification projects like the seL4 micro-kernel \cite{klein14}. The seL4 kernel is written in C and is accompanied by proofs of correctness about its security, including crash-freedom and the absence of memory safety violations. The kernel makes use of the AutoCorres tool for translating C code into a higher-level monadic language embedded in the Isabelle/HOL theorem prover \cite{greenaway14}. AutoCorres generates proofs that the translation is sound with respect to the semantics of the source C program.
Another notable C verification project is the CompCert verified compiler by Xavier Leroy \cite{leroy09}, which is capable of compiling and optimising C code to assembly whilst preserving its semantics. However, CompCert does not guarantee the absence of undefined behaviour in compiled programs, and can only detect undefined behaviour in code that doesn't perform I/O, by dynamically interpreting it.

In this thesis we are concerned with the formal foundations for a possible successor to C. Whether or not this approach will result in simpler and less labour-intensive software verification is an open question. However, we hope that by starting afresh, and using types to enforce useful invariants, a new language for low-level software verification can emerge.

\section{Overview of Linear and Affine Typing}

Linear, affine and uniqueness typing are closely-related features of type systems that enforce rules about the number of times values may be used and referenced. These restrictions are motivated by several desirable properties that can be obtained by enforcing them. The Clean programming language (introduced in \cite{clean87}) uses uniqueness typing to ensure that values in memory have at most one reference to them, thus enabling \i{destructive updates} whilst preserving referential transparency. The Rust programming language \cite{rustWeb} uses uniqueness typing to track and free heap-allocated memory, thus allowing it to achieve memory safety without garbage collection. This makes it suitable for writing systems software where a garbage collector isn't available, like a garbage collector itself, or an operating system.

At their core, all of these systems enforce their constraints using typing rules derived from linear logic \cite{girard87}. To get a feel for linear logic, we first turn our attention to the linear lambda calculus. We then continue by discussing more complex languages and the techniques used in their mechanical formalisation.

\section{The Linear Lambda Calculus}

For our presentation of the linear lambda calculus we follow the work of Plotkin and Barber \cite{barber96} on Dual Intuitionistic Linear Logic (DILL). First, the syntax for variables, types and terms:
\begin{eqnarray*}
x,y & \in & \b{Vars} \\
% \tau & \in & \b{PrimTypes} \\
A, B & ::= & \mathds{I} \sor A \otimes B \sor A \lolly B \sor {!A} \\
t, u & ::= & x \sor * \sor \letbe{*}{t}{u} \sor t \otimes u \sor \letbe{x \otimes y : A \otimes B}{t}{u} \sor \\
  &     & \lam{x : A} t \sor \app{t}{u} \sor {!t} \sor \letbe{!x : A}{t}{u}
\end{eqnarray*}

The meta-variables $x$ and $y$ range over a countably infinite set of variables, \b{Vars}. Meta-variables $A$ and $B$ range over types, and $t$ and $u$ range over terms. The core constructs of the language are those of the lambda calculus: variables ($x$), abstractions ($\lam{x : A} t$) and applications $\app{t}{u}$. We introduce the other constructs through the typing rules of the language.

We write the judgement $\Gamma; \Delta \types t : A$ to denote that a term $t$ has type $A$ relative to two typing environments $\Gamma$ and $\Delta$ which record the types of free variables in $t$. The two typing environments are the origin of the name \b{Dual} Intuitionistic Linear Logic, and are part of DILL's mechanism for differentiating linear and non-linear values.

Typing environments can have many representations, and for the purposes of precise mechanical formalisation this will become very important (see \cref{sec:repr-ty-contexts}). In ``semi-formal" contexts, including Barber's presentation of DILL, the environments are defined to be sequences of variable-type pairs $x_0 : A_0, x_1 : A_1, \dots$ such that no variable appears more than once. Comma-separated environments like $(\Gamma, x : A)$ and $(\Delta_1, \Delta_2)$ are understood to be the concatenation of two environments with disjoint sets of variables.

The first environment, $\Gamma$, records the types of non-linear intuitionistic variables. In contrast, the second environment $\Delta$ records the types of linear variables which must be used exactly once. Although a language with \i{just} linear values is easily conceivable, it is both more practical and flexible to allow non-linear values to exist alongside linear ones. Trivially copyable values like integers are well-suited to being non-linear in a linear language.

To define which typing judgements are well-formed, we use inference rules in the style of natural deduction:

% In the simply-typed (non-linear) lambda calculus and many of the languages described later, a single typing environment suffices.

% For the simply-typed (non-linear) lambda calculus, typing judgements are typically written $H \types t : \tau$, where $H$ is an environment. In a linear language however, we need to make a distinction between variables which can be duplicated freely, and variables which must be used once. The approach take by DILL achieves this by using two typing.

% DILL achieves this by using two environments rather than one, so that where $\Gamma$ records the types of all duplicable (intuitionistic) variables, and $\Delta$

% In Dual Intuitionistic Linear Logic however, we make use of \i{two} environments that record the types of free intuitionistic variables and linear variables respectively respectively, $\Gamma; \Delta \types t : \tau$.

% Types are assigned to terms relative to \i{two} typing environments, rather than just one as is typical for the simply typed lambda calculus. The use of two contexts is the origin of the name \b{Dual} Intuitionistic Linear Logic, and the intent

\begin{figure}[h]
\caption{Typing Rules for DILL linear lambda calculus}
\begin{displaymath}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{cc}
\infer[\text{(Int-Var)}]{\Gamma, x : A; \emptyset \types x : A}{} &
\infer[\text{(Lin-Var)}]{\Gamma; x : A \types x : A}{} \\
\infer[\text{(Unit-I)}]{\Gamma; \emptyset \types * : \mathds{I}}{} &
\infer[\text{(Unit-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \letbe{*}{t}{u} : A}
  {\Gamma; \Delta_1 \types t : \mathds{I}  &  \Gamma; \Delta_2 \types u : A} \\
\infer[\text{($\otimes$-I)}]
  {\Gamma; \Delta_1, \Delta_2 \types t \otimes u : A \otimes B}
  {\Gamma; \Delta_1 \types t : A  &  \Gamma; \Delta_2 \types u : B} &
\infer[\text{($\otimes$-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \letbe{x \otimes y : A \otimes B}{u}{t} : C}
  {\Gamma; \Delta_1 \types u : A \otimes B  &  \Gamma; \Delta_2, x : A, y : B \types t : C} \\
\infer[\text{($\lolly$-I)}]
  {\Gamma; \Delta \types (\lambda x : A. t) : A \lolly B}
  {\Gamma; \Delta, x : A \types t : B} &
\infer[\text{($\lolly$-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types (u \; t) : B}
  {\Gamma; \Delta_1 \types u : A \lolly B  &  \Gamma; \Delta_2 \types t : A} \\
\infer[\text{(!-I)}]
  {\Gamma; \emptyset \types {!t} : {!A}}
  {\Gamma; \emptyset \types t : A} &
\infer[\text{(!-E)}]
  {\Gamma; \Delta_1, \Delta_2 \types \letbe{!x : A}{u}{t} : B}
  {\Gamma; \Delta_1 \types u : {!A}  &  \Gamma, x : A; \Delta_2 \types t : B} \\
\end{array}
\end{displaymath}
\end{figure}

There are two rules for typing variables, corresponding to intuitionistic (Int-Var) and linear variables (Lin-Var) respectively. In both cases we allow extra intuitionistic variables to be present in the environment, as we are free to ignore them. Conversely, linear variables must be used exactly once, so there can't be any present when typing a non-linear variable, and there can't be any extra ones present when typing a linear variable. Hence the linear contexts for the two rules are $\emptyset$ and $x : A$ respectively.

The unit type, $\mathds{I}$, is inhabited by a single term $*$. The term \b{let $*$ be $t$ in $u$} allows for the \i{elimination} of any term $t$ of type $\mathds{I}$, as shown by the rule (Unit-E). Every type constructor has an introduction rule and an elimination rule, suffixed \b{-I} and \b{-E} respectively.

The product type $A \otimes B$ represents a pair of values and is inhabited by terms of the form $t \otimes u$ if $t : A$ and $u : B$, as demonstrated by its introduction rule ($\otimes$-I). In order to guarantee that linear variables are used exactly once in $t \otimes u$, it is required that the linear context $(\Delta_1, \Delta_2)$ is the result of joining the two variable-disjoint linear contexts of $t$ and $u$. Alternately, we can view this as the environment for $t \otimes u$ being \i{split} into the two environments for $t$ and $u$. This is the \b{context splitting} operation that is at the core of many substructural type systems, and is the focus of our Coq library. Note that we use both the words \i{environment} and \i{context} to refer to typing environments.

The elimination rule for products ($\otimes$-E) also makes use of context splitting to ensure linearity. The rule states that if we have a term $u : A \otimes B$ and a term $t : C$ that is well-typed with free variables $x : A$ and $y : B$, then the expression that makes the components of $u$ available as $x$ and $y$ in $t$ also has type $C$. DILL uses the notation \b{let $x \otimes y : A \otimes B$ be $u$ in $t$} for this expression, which is equivalent to \c{let (x, y) = u in t} in Haskell syntax. The intuitionistic context $\Gamma$ from the \b{let} expression is available to both premises, whilst the linear context $(\Delta_1, \Delta_2)$ is split so that some variables are used to type $u$ ($\Delta_1$), and others are used to type $t$ in combination with the components of $u$ ($\Delta_2, x : A, y : B$).

The \i{lollipop} type, $A \lolly B$, is the type of functions that consume a linear value of type $A$ and produce a value of type $B$. As we shall see shortly, regular functions $A \to B$ that don't destructively consume their input can be encoded as $!A \lolly B$. Introducing a new value of lollipop type is done by writing a $\lambda$-abstraction, $(\lambda x : A. \; t)$, which is well-typed only if its body $t$ is well-typed under a linear environment extended with the type of the binder: $\Delta, x : A$. This rule, ($\lolly$-I) is identical to the rule for typing $\lambda$-abstractions in the Simply-Typed Lambda Calculus (STLC) except for the fact that it places the binder in a linear context.

The elimination rule for lollipop types ($\lolly$-E) uses a function application $(u \; t)$, where $u : A \lolly B$ and $t : A$. The context splitting is identical to the context splitting in the ($\otimes$-I) rule, with the linear context for $(u \; t)$ splitting into two sub-contexts for each of the sub-expressions $u$ and $t$.

Finally we come to the \i{bang} type, $!A$, which allows the embedding of intuitionistic terms within the language. The introduction rule for bang types, (!-I), states that if a term $t$ can be assigned the type $A$ without reference to any linear variables, then we can construct a term ${!t} : {!A}$ which represents a duplicable version of $t$. Intuitively, this makes sense, as we can refer to the variables in the intuitionistic context for $t$ as many times as we like whilst creating copies via $!t$.

To understand how terms of bang type become usable as duplicates requires consideration of the associated elimination rule, (!-E). This rule allows a term $u : {!A}$ to be destructured so that its ``inner" term becomes available in the \i{non-linear} context as re-usable assumption $x : A$. If $u : {!A}$ was determined using the introduction rule for bang types such that $u = {!v}$ for some $v$, then the let binding \b{let $!x : A$ be $u$ in $t$} binds a new name $x$ to $v$ and makes it available in the intuitionistic context for $t$.

Alternatively, it's possible for a term $u$ to have type $!A$ without an application of the bang introduction rule. This occurs, for example, when writing a function to duplicate its input as a pair. Terms of linear type can't be duplicated, so the argument to this function must have type $!A$ for some type $A$. We must then use a bang let-binding, and (!-E) to bring a duplicable version of the binder variable into the intuitionistic context. The term we would like to type is therefore: $(\lambda x : {!A}. \; \t{let } {!y} : A \t{ be } x \t{ in } ({!y} \otimes {!y}))$. The typing derivation is:

\begin{eqnarray*}
\infer[\t{($\lolly$-I)}]{\emptyset; \emptyset \types (\lambda x : {!A}. \; \t{let } {!y} : A \t{ be } x \t{ in } ({!y} \otimes {!y})) : {!A} \lolly ({!A} \otimes {!A})}{
  \infer[\t{(!-E)}]{\emptyset; x : {!A} \types \t{let } {!y} : A \t{ be } x \t{ in } ({!y} \otimes {!y}) : ({!A} \otimes {!A})}{
    \infer[\t{(Lin-Var)}]{
      \emptyset; x : {!A} \types x : {!A}
    }{} &
    \infer[\t{($\otimes$-I)}]{
      y : A; \emptyset \types ({!y} \otimes {!y}) : ({!A} \otimes {!A})
    }{
      \infer[\t{(!-I)}]{y : A; \emptyset \types {!y} : {!A}}{
        \infer[\t{(Int-Var)}]{y : A; \emptyset \types y : A}{}
      } &
      \infer[\t{(!-I)}]{y : A; \emptyset \types {!y} : {!A}}{
        \infer[\t{(Int-Var)}]{y : A; \emptyset \types y : A}{}
      }
    }
  }
}
\end{eqnarray*}

Note how we rely on (Lin-Var), rather than the bang introduction rule (!-I), to form the judgement $\emptyset; x : {!A} \types x : {!A}$. We also see the bang elimination rule in action here, adding the new name $y$ for $x$ into the intuitionistic context for ${!y} \otimes {!y}$.

In the Simply-Typed Lambda Calculus, an equivalent duplication function would have type $A \to (A \otimes A)$. In fact, it is possible to embed all intuitionistic (STLC) terms and types in linear logic using reasoning similar to the above \cite{barber96}. Intuitionistic types $A$ can be translated to duplicable bang types ${!A}$, and functions $A \to B$ can be translated to ${!A} \lolly B$.

\subsection{Structural Rules}

Philip Wadler gives a similar typing derivation for a duplication function in his 1993 presentation of linear logic \cite{wadler93}, although his system differs from DILL in a few key ways.

In Wadler's system, a single typing environment is used, containing both linear assumptions of the form $\langle x : A \rangle$ and intuitionistic ones of the form $[x : A]$. These are conceptually equivalent to assumptions in the respective linear and intuitionistic contexts of DILL. For rules that require the context to consist entirely of intuitionistic assumptions, Wadler uses the notation $[\Gamma]$, which is approximately equivalent to a $\Gamma; \emptyset$ pair of contexts in DILL. When formalising linear logic with an interactive theorem prover such as Coq, the dual-context approach taken by DILL is preferable to Wadler's approach because of the clear and forced separation of the two worlds -- intuitionistic and linear. For example, Wadler's system would require a Coq data-type to differentiate the two types of assumptions, and an additional predicate to state if a context contains only intuitionistic assumptions. A further difference between the two systems is that Wadler's renders the structural rules that lend their name to \i{substructural} type systems explicit.

To save writing the entire set of typing rules, which are very similar to the ones for DILL already presented and the upcoming rules for $L^3$, we cherry-pick a few of the structural rules to demonstrate our point, and rely on the approximate translation described above and the original paper \cite{wadler93} to provide the full details.

Unlike DILL, both kinds of variables in Wadler's LLC require their typing contexts to be empty except for the variable of interest:

\begin{eqnarray*}
\infer[\t{(Int-Var')}]{[x : A] \types x : A}{} \qquad
\infer[\t{(Lin-Var')}]{\langle x : A \rangle \types x : A}{}
\end{eqnarray*}

Duplication and discarding of intuitionistic variables is then enabled by two \i{structural} rules, Contraction and Weakening:

\begin{eqnarray*}
\infer[\t{(Contraction)}]{\Gamma, [x : A] \types u[x/y, x/z] : B}{
  \Gamma, [y : A], [z : A] \types u : B
}
\qquad
\infer[\text{(Weakening)}]{\Gamma, [x : A] \types t : B}{
  \Gamma \types t : B
}
\end{eqnarray*}

Contraction makes use of substitutions ($u[x/y, x/z]$) to form terms with two occurrences of the intuitionistic variable $x$. Weakening allows a non-vital variable and type to be introduced from nowhere, or, when reasoning backwards, it allows an unused variable to be discarded in order to type a sub-term.

In DILL the contraction and weakening rules for intuitionistic variables are implicit in how the intuitionistic context is managed -- there can be additional intuitionistic assumptions present when typing variables (weakening), and intuitionistic assumptions can be duplicated freely (contraction). In contrast, in Wadler's system all duplication and discarding happens via (Contraction) and (Weakening). With the exception of the structural rules, Wadler's typing contexts behave entirely linearly, and are \b{split} in the same way linear contexts in DILL are split.

If the use of structural rules is restricted, we get a substructural type system. Without contraction, \i{variables can only appear once in a term}. Without weakening, \i{all variables in the context must be used in the term}. Banning both contraction and weakening results in a \b{linear} type system where variables must be used exactly once. The systems we've seen so far, DILL and Wadler's LLC, are both linear in that contraction and weakening are banned for the non-intuitionistic terms of the language. Under the Curry-Howard correspondence, linear type systems correspond to linear logic, which was first introduced by Girard \cite{girard87}.

If contraction is restricted but weakening is allowed, the result is an \b{affine} type system, corresponding to an affine logic (Grishin, 1974, Russian; recently \cite{tov11}). Variables in affine type systems can be thrown away but not duplicated, so every variable is used \i{at most once}.

\subsection{Summary of the Linear Lambda Calculus}

In this section we've seen how typing rules can be used to enforce constraints on the use of variables, by examining Dual Intuitionistic Linear Logic. We have seen that intuitionistic terms can be embedded in substructural languages via bang types and the careful management of typing contexts. The importance of \b{context splitting} to divide linear assumptions between sub-terms has also been highlighted. In the next sections we examine more complex calculi that build on the linear lambda calculus to model useful programming constructs like mutation and memory allocation.

\section{Operational Semantics and Type Soundness}
\label{sec:op-sems-soundness}

Up until this point, we have described only the syntax and static semantics (typing rules) of linearly-typed programming languages, with only vague notions of how terms behave dynamically at run-time. To now describe the dynamic behaviour of languages we turn to \i{structural operational semantics}, a mathematical formalism for the step-wise evaluation of terms, also known as small-step semantics.

To define the small-step semantics of a language, we inductively define a relation on pairs of terms, denoted $t \steps t'$. The relation needn't be strictly on pairs of terms, and many small-step semantics also thread through a global state or \i{store}, $\sigma$, so that the stepping relation ends up being: $(\sigma, t) \steps (\sigma', t')$. In small-step semantics each step $t \steps t'$ is intended to represent a single step of the computation, in contrast to big-step \i{natural} semantics where the values that terms evaluate to are stated directly, as in $t \Downarrow v$ \cite{gunterSemantics}.

The $\beta$-rule of the $\lambda$-calculus can be stated for DILL as:
\begin{eqnarray*}
\infer[(\beta)]{(\lambda x : A. \; t) \; v \steps t[v/x]}{}
\end{eqnarray*}

Here we use the variable $v$ to indicate a \i{value} of the language. Values are terms that are fully evaluated according to the small-step semantics \cite{tapl} -- formally, they are terms that are in normal form with respect to the stepping relation $(\steps)$. By specifying that $\beta$-reduction can only occur if the argument is a value we have fixed the reduction strategy to call-by-value. Other reduction strategies can be similarly encoded, but we consider only call-by-value here, as it's simple and well-suited to low-level programming, as evidenced by the literate surveyed.

Type systems for programming languages are said to be \i{sound} if well-typed programs are guaranteed not to get stuck when evaluated according to the language's operational semantics. Stated formally, the soundness lemma is:

\begin{eqnarray*}
\infer[\t{(Type Soundness)}]
{(\exists e''. \; e' \steps e'') \lor \t{$e'$ is a value}}
{\emptyset \types e : \tau  &  e \steps^* e'}
\end{eqnarray*}

The notation $(\steps^*)$ represents zero or more applications of $(\steps)$. Our definition of stuckness states that a term $e'$ is not stuck if either it can step to some other term $e''$, or it is a value of the language.

Soundness can be proved via two supporting lemmas called \i{progress} and \i{preservation}, using an approach introduced by Wright and Felleisen \cite{wright94}.

The progress property holds if all well-typed closed terms are either values, or can take a step.
\begin{eqnarray*}
\infer[\t{(Progress)}]
{(\exists e'. \; e \steps e') \lor \t{$e$ is a value}}
{\emptyset \types e : \tau}
\end{eqnarray*}

The preservation, or \i{subject reduction}, property holds if well-typed terms retain their type during evaluation.
\begin{eqnarray*}
\infer[\t{(Preservation)}]
{\emptyset \types e' : \tau}
{\emptyset \types e : \tau  &  e \steps e'}
\end{eqnarray*}

Our proof-of-concept Coq formalisation of a linear lambda calculus makes use of progress and preservation lemmas to establish type soundness. There are other formalisms and techniques that can be used to demonstrate soundness and similar properties, but we restrict our attention here to syntactic proofs by progress and preservation.

\section{The Linear Language with Locations, $L^3$}

We now turn our attention to \i{The Linear Language with Locations}, $L^3$ \cite{ahmed05}, which extends the linear lambda calculus with strong destructive updates and explicit memory management. In a language with references or pointers, a destructive update is a re-assignment of a value pointed to by a pointer. A \i{strong} destructive update is one that may also change the \i{type} of the value pointed to by the pointer. Destructive updates are a common feature of imperative languages, and their addition to the linear lambda calculus brings us a step closer to a sound and usable programming language for low-level programming. $L^3$ itself is not such a language, but could serve as a foundation for a language with more features (e.g. polymorphism). Its designers created it as a foundational calculus for strong updates, which can be used to model type-varying CPU registers.

% due to registers almost always containing values of different types throughout a program's execution.

As for Dual Intuitionistic Linear Logic, we begin with a description of the syntax of the language, in Figure \ref{l3_syntax}.

\begin{figure}[h]
\caption{Syntax for The Linear Language with Locations}
\label{l3_syntax}
\begin{eqnarray*}
x,y & \in & \b{Vars} \\
l & \in & \b{LocConsts} \\
\rho & \in & \b{LocVars} \\
\eta & ::= & l \sor \rho \\
A & ::= & \mathds{I} \sor A \otimes B \sor A \lolly B \sor !A \sor \Ptr{\eta} \sor \Capa{\eta}{A} \sor
\Forall{\rho} A \sor \Exists{\rho} A \\
t,u & ::= & * \sor \letbe{*}{t}{u} \sor \\
  && t \otimes u \sor \letbe{x \otimes y}{t}{u} \sor \\
  && x \sor \lam{x} t \sor \app{t}{u} \sor \\
  && {!v} \sor \letbe{!x}{t}{u} \sor \dupl{t} \sor \drop{t} \sor \\
  && \ptr{l} \sor \capa{l} \sor \lnew{t} \sor \lfree{t} \sor \lswap{t}{u} \\
  && \Lam{\rho} t \sor t [\eta] \sor \qpair{\eta}{t} \sor \letbe{\qpair{\rho}{x}}{t}{u} \\
v & ::= & * \sor v_1 \otimes v_2 \sor x \sor \lam{x} t \sor {!v} \sor \ptr{l} \sor \capa{l}
  \sor \Lam{\rho} t \sor \qpair{\eta}{v}
\end{eqnarray*}
\end{figure}

Many of the terms and types have the same meaning as in DILL. Functions, variables, products and the unit value are all the same. $L^3$'s complete specification also includes small-step operational semantics, for which the set of values denoted by meta-variable $v$ are normal forms.

The main addition is the set of primitives for allocating and managing memory: $\lnew{t}$, $\lfree{t}$ and $\lswap{t}{u}$. Each piece of memory allocated is at a constant location $l$, drawn from a set of location constants \b{LocConsts} which can be considered memory addresses. The precise locations are hidden from the programmer by existential types $\Exists{\rho} A$, for location variables $\rho \in \b{LocVars}$.

$L^3$ is typical of other modern calculi in that it separates resources like pointers from \i{capabilities} to use resources. A pointer value $\ptr{l} : \Ptr{l}$ is unusable without a capability to read from and write to it: $\capa{l} : \Capa{l}{A}$. Capabilities are linear values, whilst pointers are duplicable. To see how this is enforced, and the destructive updates that it enables, we now considering the typing rules for $L^3$, as in Figures \ref{l3_typing_part1} and \ref{l3_typing_part2}.

\clearpage

\begin{figure}[H]
\caption{Typing Rules for The Linear Language with Locations (Part I)}
\label{l3_typing_part1}
\begin{displaymath}
\arraycolsep=0.5pt\def\arraystretch{2.2}
\begin{array}{c}
% Unit
\infer[\t{(Unit-I)}]{\Delta; \emptyset \types * : \mathbb{I}}{} \\
\infer[\t{(Unit-E)}]{\Delta; \Gamma_1, \Gamma_2 \types \letbe{*}{t}{u} : A}{
  \Delta; \Gamma_1 \types t : \mathbb{I}  &  \Delta; \Gamma_2 \types u : A
} \\
% Vars
\infer[\t{(Var)}]{\Delta; x : A \types x : A}{FLV(A) \subseteq \Delta} \\
% Pairs
\infer[\t{($\otimes$-I)}]{\Delta; \Gamma_1, \Gamma_2 \types t \otimes u : A \otimes B}{
  \Delta; \Gamma_1 \types t : A  &  \Delta; \Gamma_2 \types u : B
} \\
\infer[\t{($\otimes$-E)}]
{\Delta; \Gamma_1, \Gamma_2 \types \letbe{x_1 \otimes x_2}{t}{u} : C}
{\Delta; \Gamma_1 \types t : A \otimes B  &
 \Delta; \Gamma_2, x_1 : A, x_2 : B \types u : C} \\
% Functions
\infer[\t{($\lolly$-I)}]{\Delta; \Gamma \types \lam{x} t : A \lolly B}{
  \Delta; \Gamma, x : A \types t : B
} \\
\infer[\t{($\lolly$-E)}]{\Delta; \Gamma_1, \Gamma_2 \types \app{t}{u} : B}{
  \Delta; \Gamma_1 \types t : A \lolly B  &  \Delta; \Gamma_2 \types u : A
} \\
% Bang
\infer[\t{(!-I)}]{\Delta; !\Gamma \types {!v} : {!A}}{\Delta; !\Gamma \types v : A} \\
\infer[\t{(!-E)}]{\Delta; \Gamma_1, \Gamma_2 \types \letbe{!x}{t}{u} : B}{
  \Delta; \Gamma_1 \types t : {!A}  &  \Delta; \Gamma_2, x : A \types u : B
}\\
% Dupl and drop.
\infer[\t{(Dupl)}]{\Delta; \Gamma \types \dupl{t} : {!A} \otimes {!A}}{
  \Delta; \Gamma \types t : {!A}
} \\
\infer[\t{(Drop)}]{\Delta; \Gamma \types \drop{t} : \mathbb{I}}{
  \Delta; \Gamma \types t : {!A}
} \\

\end{array}
\end{displaymath}
\end{figure}

% \clearpage

\begin{figure}[H]
\caption{Typing Rules for The Linear Language with Locations (Part II)}
\label{l3_typing_part2}
\begin{displaymath}
\arraycolsep=0.5pt\def\arraystretch{2.2}
\begin{array}{c}
% New and free.
\infer[\t{(New)}]{\Delta; \Gamma \types \lnew{t} : \Exists{\rho} \Capa{\rho}{A} \otimes {!(\Ptr{\rho})}}{
  \Delta; \Gamma \types t : A
} \\
\infer[\t{(Free)}]{\Delta; \Gamma \types \lfree{t} : \Exists{\rho} A}{
  \Delta; \Gamma \types t : \Exists{\rho} \Capa{\rho}{A} \otimes {!(\Ptr{\rho})}
} \\
% Swap.
\infer[\t{(Swap)}]{\Delta; \Gamma_1, \Gamma_2 \types \lswap{t}{u} : \; (\Capa{\rho}{B}) \otimes A}{
  \Delta; \Gamma_1 \types t : \Ptr{\rho}  &  \Delta; \Gamma_2 \types u : (\Capa{\rho}{A}) \otimes B
} \\
% LFun
\infer[\t{(LFun)}]{\Delta; \Gamma \types \Lam{\rho} t : \Forall{\rho} A}{
  \Delta, \rho; \Gamma \types t : A
} \\
% LApp
\infer[\t{(LApp)}]{\Delta; \Gamma \types t [\rho'] : A[\rho' / \rho]}{
  \Delta; \Gamma \types t : \Forall{\rho} A  &  \rho' \in \Delta
} \\
% LPack
\infer[\t{(LPack)}]{\Delta; \Gamma \types \qpair{\rho'}{t} : \Exists{\rho} A} {
  \rho' \in \Delta  &  \Delta; \Gamma \types t : A[\rho' / \rho]
} \\
% Let-LPack
\infer[\t{(Let-LPack)}]{\Delta; \Gamma_1, \Gamma_2 \types \letbe{\qpair{\rho}{x}}{t}{u} : B}{
  \Delta; \Gamma_1 \types t : \Exists{\rho} A  &  FLV(B) \subseteq \Delta  &
  \Delta, \rho; \Gamma_2, x : A \types u : B
}
\end{array}
\end{displaymath}
\end{figure}

$L^3$'s typing judgements include two contexts, one for locations ($\Delta$) and another for types ($\Gamma$), as in $\Delta; \Gamma \types t : A$. All variables and their types are stored in one typing context, which makes $L^3$ more similar to Wadler's LLC than DILL, despite the syntactic similarity. The location context $\Delta$ is a sequence of location variables that are currently in scope, like variables in a typing context without the type information: $\Delta ::= \emptyset \sor \Delta, \rho$.

As in DILL, \b{context splitting} is used extensively to ensure that each variable appears exactly once in a term. The typing rules for unit types, product types ($\otimes$) and functions ($\lolly$) are almost identical to the rules for their counterparts in Wadler's LLC, and DILL, modulo the different number of contexts.

The rule for variables, (Var), is similar to Wadler's, except that the \i{free location variables} of the variable's type, $FLV(A)$, must be present in the location context. Another interesting difference is that $L^3$ has only one rule for variables, rather than two like DILL and Wadler's LLC. The reason for this is that $L^3$ doesn't differentiate between linear and intuitionistic assumptions. Wadler motivates the two types of assumptions by giving an example of how the proof reduction rule equivalent to $\beta$-reduction becomes unsound if contraction and weakening are allowed for assumptions of the form $\langle x : {!A} \rangle$ (or just $x : {!A}$ in $L^3$ syntax). This doesn't pose a problem to $L^3$ because there are no explicit contraction and weakening rules -- the same functionality is instead provided by $\dupl{t}$ and $\drop{t}$ primitives. As a result, a variable is considered intuitionistic (and duplicable) if it has type ${!A}$ for some $A$.

The \b{drop} and \b{dupl} primitives form part of $L^3$'s handling of intuitionistic values using the bang type. The introduction rule for bang types is almost the same as in DILL, except that only values $v$ are permitted, and an intuitionistic context $!\Gamma$ is now just one where all assumptions are of the form $x : {!A}$. The elimination rule is also similar, except that the context which the newly bound variable is introduced into is linear, which means the variable can only be used once after the rule is applied. The duplication primitive mitigates this restriction by explicitly transforming a value of type $!A$ into a pair of values with the same type, ${!A} \otimes {!A}$. The components of the duplicate pair can then be given names and introduced into the typing context by the elimination rule for products, ($\otimes$-E). This duplicating of intuitionistic values is equivalent to a contraction typing rule.

Further, there is an equivalent to a weakening rule for intuitionistic terms, in the form of the \b{drop} primitive, which allows a term to be discarded. Given a term $t$ with type ${!A}$, dropping it results in a term $\drop{t}$ with type unit $\mathds{I}$.

The remaining typing rules relate to $L^3$'s primitives for memory allocation. By the (New) rule, we see that a term $t$ can be assigned a reference and accompanying capability by the $\lnew{t}$ term. The type of $\lnew{t}$ is an existential type $\Exists{\rho} (\Capa{\rho}{A} \otimes {!(\Ptr{\rho})})$, which hides the precise location from the programmer. As can be seen from the (Let-LPack) rule, this existential package can be unpacked by a $\letbe{\qpair{\rho}{x}}{t}{u}$ term, causing the location $\rho$ to become available in the location context, and a variable for the pointer and capability to become available in the typing context.

Note that the capabilities produced by the \b{new} construct have linear type $\Capa{\rho}{A}$, while the pointers themselves are non-linear and duplicable $!(\Ptr{l})$. Further, the type of the pointer doesn't mention the type of the value it points to. These two properties enable strong destructive updates, as the unique capability can be passed to the \b{swap} construct, which replaces the value at a location and evaluates to a pair containing the old value and a capability with the new value's type. The (Swap) rule demonstrates this.

Finally, the \b{free} primitive, which is responsible for de-allocating a piece of memory allocated with \b{new}. A term $\lfree{t}$ is well-typed if the term $t$ is a capability-and-pointer package as produced by \b{new}. The value of $\lfree{t}$ after de-allocation is an existential package containing the value that was stored at the location, with type $\Exists{\rho} A$.

The run-time interpretations of the above operations are provided by a small-step operational semantics that makes use of a store $\sigma$ mapping locations $l$ to closed values $v$. We display only a selection of the reduction rules, which are lifted to small-step operational semantics by evaluation contexts. For the full set of reduction rules and evaluation contexts, see the technical report accompanying $L^3$ \cite{l3TechReport}.
\begin{eqnarray*}
\semrule{new} & (\sigma, \lnew{v}) \Rightarrow (\sigma \uplus \{l \mapsto v\},
  \qpair{l}{\capa{l} \otimes {!(\ptr l)}})
\\
\semrule{free} & (\sigma \uplus \{l \mapsto v\}, \lfree{\qpair{l}{\capa{l} \otimes {!(\ptr{l}})}}) \Rightarrow (\sigma, \qpair{l}{v})
\\
\semrule{swap} & (\sigma \uplus \{ l \mapsto v_1 \}, \lswap{(\ptr{l})}{(\capa{l} \otimes v_2)})
  \Rightarrow (\sigma \uplus \{ l \mapsto v_2 \}, \capa{l} \otimes v_1)
\\
\semrule{let-lpack} & (\sigma, \letbe{\qpair{\rho}{x}}{\qpair{l}{v}}{t})
  \Rightarrow
  (\sigma, t[l/\rho][v/x])
\end{eqnarray*}

These rules codify the intuition for the constructs given above. For example, (new) introduces a new location $l$ into the store, mapped to the appropriate value, whilst (free) performs the inverse. Capabilities are threaded throughout to ensure that exclusive rights to update the memory location are held.

For their proof of soundness, the $L^3$ authors don't use a syntactic proof. Instead, they provide a \i{semantic interpretation} of the types and prove that well-typed expressions correspond to true logical statements about the interpretations of the types. A consequence of this approach is that the typing rules don't contain rules for intermediate values like $\capa{l}$, which makes a syntactic proof more complicated, as discussed in \cref{towards-l3}.

%the mechanical verification of a small language -- \i{The Linear Language with Locations}, $L^3$. Although formal semantics and semi-formal proofs of correctness exist for $L^3$, no computer-based proofs exist. We verify $L^3$ because it is typical of modern resource-aware calculi in its use of capabilities -- whilst remaining simple. $L^3$'s concepts of ownership make its mechanisation relevant to the complete verification of low-level systems like garbage collectors and operating systems, which are at the cutting edge of verification research.

%The Linear Language with Locations $L^3$ \cite{ahmed05}, makes use of concepts from linear typing to manage \i{capabilities} -- 

%$L^3$ is a \i{low-level} language by design and features primitive operations for allocating and deallocating memory. By virtue of linear capabilities, \i{strong updates} are supported, whereby the type of a value in a memory location may change upon writing. Strong updates enable staged value initialisation, and simulation of register type-changes throughout program execution. For example, a pointer initially pointing to an \c{Int} can be overwritten with a \c{Bool} and the type system can statically track this change.

%$L^3$'s definition includes a description of syntax, operational semantics and typing rules. Notably, the operational semantics include a store type which maps locations to values. Similarly, the typing rules include a location context that tracks which locations are in scope.

%These two rules from the operational semantics show the behaviour of the \c{new} keyword for allocating memory, and the \c{let} construct for unpacking pointers and capabilities. Note how the store $\sigma$ is extended with a mapping from a new location $l$ to the value $v$ in the case of the rule for \c{new}. The $\lquine l, \langle \capa{l}, \ptr l \rangle \rquine$ notation represents a value of \i{existential type} that witnesses the existence of a pair containing a capability for, and pointer to, some location $l$ which is hidden from the programmer.

%$L^3$'s rules for managing capabilities are closer to standard linear typing than de Vries' rules for modelling Clean's uniqueness typing. Typing assumptions are treated linearly, there are context-splitting operations and contraction and weakening are permitted for values of bang (!) type. This is in contrast to the use of kinds and attribute type constructors in de Vries' model.

%Conceptually, because the rules that introduce capabilities are baked into the type system, capabilities are guaranteed \i{not to have been shared} (uniqueness) and \i{not to be shared in the future} (linearity). The use of capabilities also yields all of the same properties yielded by substructural and uniqueness typing -- notably destructive updates and the potential to eliminate garbage collection. It is typical of many of the later systems which we cover in the next section.

\section{Uniqueness Typing}

Although linear and affine type theory capture the essence of uniquely referenced values, they are insufficient to describe the concept of \i{uniqueness} as it appears in languages like Clean. In his 2007 paper, de Vries \cite{deVries07} notes that terms of a unique type should be \i{guaranteed to never have been shared}, which is sufficient to guarantee a unique pointer at runtime. In contrast, terms of linear (or affine) type are \i{guaranteed not to be shared in the future}, which is insufficient to guarantee a unique pointer.

The distinctness of linearity and uniqueness is highlighted by the contrast between \i{dereliction} and the rule that we'll refer to as \i{uniqueness removal} present in Clean's type system. In a linearly typed language, dereliction refers to the ability to convert intuitionistic values into linear ones, for example by constructing a function with type $({!A} \lolly A)$. In DILL, the following dereliction function is well-typed:
\begin{eqnarray*}
\emptyset; \emptyset \types (\lam{x : {!A}} \letbe{!y : A}{x}{y}) : {!A} \lolly A
\end{eqnarray*}

Now, imagine that we treat linear terms as unique, and intuitionistic ones as non-unique. As noted by Edsko de Vries \cite{deVriesPhD08}, the dereliction function above is \b{not sound} under this proposed equivalence. The ``unique" value resulting from dereliction is not necessarily unique because other shared references (intuitionistic values of type ${!A}$) may still exist.

Further, the uniqueness removal rule in Clean allows unique values to be transformed into non-unique ones. In a linear context, this is analogous to writing a function with type $A \lolly {!A}$, which violates the guarantee that linear variables are only used once, and is impossible in DILL. A unique value may sacrifice its uniqueness to become shared, but a linear value which models the existence of a single resource should not be transformed into an unlimited supply of that resource. Further note that if uniqueness is to be exploited to make garbage collection unnecessary -- as in the case of Rust -- then the uniqueness removal rule is undesirable as it prevents values from having a unique owner.

Due to the non-equivalence of linearity and uniqueness, de Vries constructed a distinct set of semantics and typing rules to model Clean's type system \cite{deVries07}.

One key component of his approach is the use of the \i{kind} (type of types) system to track uniqueness and non-uniqueness. As in Haskell, de Vries' uniqueness system includes a kind for data (\c{*}) which is the kind of all inhabited types (and \c{Void}). In addition, there is a uniqueness kind $\mathcal{U}$ inhabited by two types $\bullet$ and $\times$ representing uniqueness and non-uniqueness respectively. A third kind, $\mathcal{T}$ is the kind of base types (like \c{Int}). These kinds are brought together by a type constructor $\c{Attr} ::_k \mathcal{T} \rightarrow \mathcal{U} \rightarrow *$ which applies a uniqueness attribute to a base type to form a type that is inhabited. For example, \c{Attr} $\bullet$ \c{Int} or $\c{Int}^\bullet$ is the type of uniquely referenced integers.

The other main technique employed by de Vries' model is the use of arbitrary boolean expressions as uniqueness attributes, with $\bullet$ as true and $\times$ as false. Clean's type system allows uniqueness polymorphism, which results in constraint relationships between uniqueness variables, which are represented in de Vries' system as simple boolean expressions that can be handled by a standard unification algorithm.

Despite uniqueness being distinct from linearity, Edsko's formalism also makes use of a context splitting operation to enforce constraints on the usage of variables. It differs slightly from the standard approach in that non-unique variables can be split onto both sides. A discussion about expressing de Vries' context splitting in terms of the standard approach is given in \cref{sec:generality}.

Finally, de Vries' work includes the only known mechanical proof of type soundness for a type system similar to Clean's. The proof is syntactic, and is encoded in Coq. It makes use of the \i{locally nameless} approach to variable naming that is discussed in \cref{sec:de-bruijn}.

\section{Systems of Capabilities}

\subsection{Cyclone}

Several systems extend and generalise the capability-based approach employed in $L^3$. Fluet, Morrisett and Ahmed followed up their paper on $L^3$ with a region-based system that borrows many ideas from $L^3$, called \rgnUL \cite{fluet06}. Notably, it makes use of linear capabilities to provide safe access to \i{dynamic regions}, which are first-class abstractions for the allocation of memory. Dynamic regions extend simpler lexical regions by allowing regions to exist independent of lexical scopes. Accompanying the \rgnUL paper is a mechanised proof of type soundness using the Twelf proof assistant \cite{pfenning99}.

The same authors are also responsible for the Cyclone project \cite{grossman05}, which extends the C programming language with regions and uniqueness typing in order to achieve safe memory management without garbage collection or manual intervention. The \rgnUL calculus models Cyclone's core features, and there exists a translation from Cyclone to \rgnUL via an intermediate language $F^\text{RGN}$ which makes use of a generalised ST monad \cite{fluet06, fluet04}. No mechanised proof of correctness for this work exists, although an earlier semi-formal proof of type soundness for Cyclone \cite{jim01} is structured in a way that looks amenable to mechanised verification. On their webpage \cite{cycloneWeb}, the creators of Cyclone note that work on the project has stopped, with many of the ideas living on in Rust. Future formalisations of Rust can hopefully make use of this work.

\subsection{Pottier's Type-and-Capability System with Hidden State}
\label{sec:ssphs}

A mechanical formalisation for a system even more similar to $L^3$ than \rgnUL is given in a 2013 article by \Francois Pottier \cite{pottier13}. Pottier's system, \SSPHS, uses affine capabilities in the style of $L^3$, but adds polymorphism and support for \i{hidden state}. Hidden state allows an object to completely conceal mutable internal state from its clients. Pottier gives a memory manager as an example where such a feature is useful -- clients care only about the memory allocated or de-allocated, and not about internal data-structures modified in the process. Hidden state is realised via a typing rule called the \i{anti-frame rule}, which makes terms with hidden state subtypes of the type sans hidden state.

The concept of hidden state is distinct from, yet related to, the existential types that $L^3$ uses to conceal exact locations. \SSPHS also employs hidden state for the purpose of general resource management, rather than just memory management. The ability to express memory management in the language obsoletes $L^3$ and similar systems' explicit rules for memory management, which Pottier describes as ``magic" \cite{pottier13}.

All of $L^3$'s features, including strong updates, are covered by Pottier's system. It also subsumes \rgnUL, with support for polymorphism and regions. Unlike previous systems it also guarantees the runtime-irrelevance of capabilities, which are proved to be erasable.

For context splitting, \SSPHS makes use of a \i{multiplicity environment} which records the number of available copies of each variable. Each variable is marked as having 0, 1 or $\infty$ copies available, encoding unavailable linear variables, available linear variables and intuitionistic variables respectively. Pottier exploits the fact that multiplicity environments form a \i{separation algebra}, and unifies their treatment with the treatment of general resources (including regions). In this system, context splitting is the division of a context such that the number of copies of each variable is preserved.

Pottier's formalisation is done within the Coq proof assistant and makes use of de Bruijn indices for variable binding (a pre-cursor to his DbLib library, discussed in \cref{sec:de-bruijn}). The formalisation consists of 20,000 lines of Coq source and follows the syntactic approach to proving type soundness via progress and preservation. Pottier notes that the formalisation took around 6 months to complete.

\subsection{Mezzo}

Together with Thibaut Balabonski and Jonathan Protzenko, Pottier is also responsible for the Mezzo programming language and its associated Coq formalisation \cite{mezzo14}. Mezzo differs from \SSPHS and \rgnUL in that it is designed to be high-level and expressive. Like the other systems examined, its system of ownership is based around linear \i{permissions}, which allow programmers to design diverse usage \i{protocols} for functions and data. Mezzo's model of concurrency leverages ownership to guarantee that well-typed programs do not contain data-races, a property that is also formalised in Coq.

Mezzo includes mechanisms for deferring permissions checks to runtime in order to gain more expressive power, at the cost of some synchronisation overhead. Its surface syntax is also designed to be more minimal than languages like $L^3$ which favour explicit annotations. Both of these aspects reflect Mezzo's ambition to be a user-facing programming language that provides control over resources.

The prototypical compiler for Mezzo uses untyped OCaml as its target language and as such requires garbage collection at runtime. Further, due to OCaml's lack of parallelism, concurrent and race-free Mezzo programs are currently unable to take advantage of multiple cores. One can imagine further work to compile Mezzo to a low-level language with similar semantics, in order to take advantage of its full feature set.

Mezzo's Coq formalisation consists of 14,000 lines of code and makes use of a 2000 line library called DbLib for handling de Bruijn indices. Like the proof for \SSPHS, it uses progress and preservation to prove type soundness.

\section{Linear Dependent Types}
\label{sec:linear-dep}

Conor McBride's recent work on combining linear and dependent typing \cite{mcbride16} is strikingly similar to Pottier's \SSPHS in its treatment of typing contexts. In Conor's system, variables in the context are annotated by the number of occurrences available at run-time. Linear variables are annotated with a $\b{1}$ if available, or a $\b{0}$ if they have already been used in a neighbouring part of the term. This allows types to depend upon linear values, by referring to the $\b{0}$ available copies if necessary -- a process Conor calls \i{contemplation}. As in Pottier's work, an infinity $\infty$ annotation is used for intuitionistic variables, and context splitting is the resource-preserving division of a context into two pieces.

\section{Typed Assembly Languages and Trustworthy Compilers}

Strong updates can be used to model the storage of type-distinct values in a single register through-out program execution. As such, low-level calculi like $L^3$ and \rgnUL are conceptually linked to \i{typed assembly languages} (TALs), which extend regular assembly languages with type annotations.

Well-typed TAL programs typically guarantee memory safety given an axiomatisation of a machine architecture. In the TALx86 \cite{morrisett99, crary99} system, blocks are annotated with pre-conditions that place requirements on the types of registers. This approach to typing is substantially different from the operational semantics and inductive typing judgements used to describe the semantics of the other languages we've surveyed ($L^3$, \rgnUL, \SSPHS). However, recent work by Amal Ahmed \i{et al.}  has successfully resulted in a more traditional model for typed assembly languages \cite{ahmed10}. This model still differs from the others considered in this thesis in that it uses denotational semantics, Hoare logic and several interconnecting layers in order to minimise the number of axioms required. Ahmed's paper includes a Twelf formalisation of soundness for the TAL semantic framework and an example language.

Another take on the typed assembly language concept is Bedrock from Adam Chlipala's research group \cite{chlipala11}. Bedrock uses a domain-specific assembly language embedded within Coq to express low-level programs. Aided by user-provided annotations, Bedrock can prove properties about these assembly programs in an automated way using custom Coq tactics. The block annotations resemble the block pre-conditions of TALx86.

More broadly it is worth noting the contribution of the CompCert \cite{leroy09} project to program verification. Through a series of semantics-preserving translations through intermediate languages, CompCert compiles a variant of C to multiple assembly languages. CompCert is programmed and verified in Coq. Verification of programs written in a low-level linearly-typed language could use parts of CompCert, perhaps with a language like $L^3$ or \SSPHS as an intermediate language.

\section{Variable Naming and Binding}
\label{sec:var-naming}

One problem that arises frequently in the formalisation of language semantics is that of \i{capture-avoiding substitution}. Substitution operations, whereby a value is substituted for a variable in a term, form the core computational component of the operational semantics in many languages. In the simply-typed (and untyped) lambda calculus, the $\beta$-rule uses substitution (denoted $e[v/x]$) to describe the semantics of function application:
\begin{eqnarray*}
(\lambda x : \tau. \; e) \; v \Longrightarrow_\beta e[v/x]
\end{eqnarray*}

The problem of \i{variable capture}, which we wish to avoid, is demonstrated by the following example:

\begin{eqnarray*}
(\lambda x. \; \lambda y. \; x + y) \; y \; {\centernot\Longrightarrow}_\beta \; (\lambda y. \; y + y)
\end{eqnarray*}

Here the parameter $y$ is a free variable acting as a place-holder for a value in the environment. After substitution however, the $y$ replacing $x$ in the abstraction body $x + y$ becomes bound due to the name collision between the free $y$ and the binder $y$. This altering of the meaning of terms during substitution is something we would like to avoid.

One way to avoid variable capture is to forbid the substitution of any terms containing free variables. In such a system, free variables like $y$ are never considered values and as such cannot be used in variable capturing substitutions. This is the approach taken by \i{Software Foundations} \cite{pierce15} in formalisations of the simply-typed lambda calculus and its variants. A further consequence of this approach is that globally-shared integers or strings for variable names are sufficient to guarantee soundness. Although it's tempting to embrace this approach for its simplifying properties, it doesn't help our overall goal of creating a \i{general} framework for substructural languages, in which substitution of open terms should be possible.

To attain capture-avoiding substitution we consider three main approaches from the literature which all exploit the observation that the exact names of bound variables are insignificant at the level of language formalisation. In other words, although the names of variables may hold meaning for the authors of programs, they do not impact the meanings of programs themselves.

\subsection{Higher-order Abstract Syntax}

When using Higher-order Abstract Syntax (HOAS) to handle variable binding, the binders of the host language (in our case Coq) are used to represent binding constructs in the object language. Twelf encourages use of HOAS through its light-weight syntax (this example adapted from \cite{twelf08}):

\begin{verbatim}
exp : type.
let : exp -> (exp -> exp) -> exp.
\end{verbatim}

The full definition for \c{exp} is omitted, but this example demonstrates that a let-binding in the \i{object language}, can be considered in the \i{meta-language} as a value representing the expression being bound, and a function that accepts that bound expression as input. For example, the object language expression \c{let x = 1 + 2 in x + 3} would be encoded as \c{let (plus 1 2) ([x] plus x 3)}, where \c{plus : nat -> nat -> expr} and \c{([x] e)} is syntax for $(\lambda x. \; e)$.

This sort of encoding becomes problematic in Coq due to the difficulty of encoding types involving \i{negative occurrences} inductively. A type appears as a negative occurrence if it would appear below an odd number of negations in a translation to classical logic \cite{tapl}. In our example, the argument to \c{let}'s higher-order function is a negative occurrence: \c{exp -> (\underline{exp} -> exp) -> exp}. An (invalid) inductive Coq definition for the above Twelf example would be:
\begin{coqcode}
Inductive exp : Set :=
  | exp_plus : nat -> nat -> exp
  | exp_let : exp -> (exp -> exp) -> exp.
\end{coqcode}

Coq rejects this definition with the error: \c{Non strictly positive occurrence of "exp" in
 "exp -> (exp -> exp) -> exp"}, as expected.

There are ways to simulate HOAS-like systems in Coq by either limiting the expressiveness and defining filters on the inductive types obtained \cite{despeyroux95} or by mixing de Bruijn indices and HOAS \cite{capretta07}. As HOAS is entirely absent from the Coq formalisations surveyed we choose to look past it in favour of plain de Bruijn indices.

\subsection{De Bruijn Indices and the Locally Nameless Approach}
\label{sec:de-bruijn}

Building on the idea that the exact names of bound variables are irrelevant, de Bruijn indices represent variables as \i{distances from their binding occurrence} \cite{deBruijn72}. For example, the identity function $(\lambda x. \; x)$ is encoded as $(\lambda . \; \hat{0})$, where a natural number annotated with a hat represents a de Bruijn index.

For terms that contain free variables, a fixed naming context is used to map free variables to indices \cite{tapl}. For example, with the naming context $\Gamma = x, y, z$ which maps $\{x \mapsto \hat{2}, y \mapsto \hat{1}, z \mapsto \hat{0}\}$, the term $(\lambda x. \; (x \; y) \; z)$ would be encoded as $(\lambda. \; (\hat{0} \; \hat{2}) \; \hat{1})$. We can imagine the context prepended to the term as an ordered list of binders, so that the use of $z$ ends up being separated from its binding occurrence by \b{1} -- the lambda.

Capture-avoiding substitution with de Bruijn indices can be defined as a recursive function that makes use of a \i{lifting} operation. Lifting a term by $d$ conceptually renumbers free variables for the introduction of $d$ elements at the end of the naming context. To avoid renumbering bound variables, a cut-off parameter $c$ is threaded through the computation. We denote lifting a term $t$ by $d$ with cut-off $c$ as $\uparrow^d_c t$.

\begin{eqnarray*}
\uparrow^d_c k & = &
  \begin{cases}
  k \quad & \text{if} \quad k < c \\
  k + d \quad & \text{if} \quad k \geq c
  \end{cases}\\
\uparrow^d_c (\lambda. \; t_1) & = & \lambda. \; \uparrow^d_{c + 1} t_1\\
\uparrow^d_c (t_1 \; t_2) & = & (\uparrow^d_c t_1) \; (\uparrow^d_c t_2)
\end{eqnarray*}

With lifting defined, the definition of substitution is straight-forward -- we simply lift the free variables of the substituted term by 1 each time we move under a lambda.

\begin{eqnarray*}
k[s/j] & = &
  \begin{cases}
  s \quad & \text{if} \quad j = k \\
  k \quad & \text{otherwise}
  \end{cases}\\
(\lambda. \; t_1)\left[s/j\right] & = & \lambda. \; t_1 \; [(\uparrow^1_0 s)/(j + 1)]\\
(t_1 \; t_2)\left[s/j\right] & = & (t_1 \; [s/j]) \; (t_2 \; [s/j])
\end{eqnarray*}

These equations for lifting and substitution are due to \cite{tapl}.

Unlike HOAS, the recursive functions for de Bruijn indices are well-suited for use with the Coq proof assistant. Of the Coq formalisations surveyed in our literature review, two of the largest use de Bruijn indices. The first, \SSPHS \cite{pottier13} defines a module with several lemmas about substitution, while the Mezzo formalisation \cite{mezzo14} makes use of a stand-alone library called DbLib \cite{dblib13}. This library uses Coq's type-classes to provide useful substitution lemmas, given the definitions of a few basic operations on terms of the object language. Our library is an extension of DbLib, and our proof-of-concept makes successful use of it for substitution, as discussed in \cref{sec:dblib-int}.

The alternative to DbLib would have been to use Arthur Chargu\'{e}raud's \i{Engineering Formal Metatheory} (EFMT) library \cite{aydemir08} for binding using a \i{locally nameless} representation. The locally nameless representation uses de Bruijn indices for bound variables and traditional names for free variables. In his formalisation of uniqueness typing Edsko de Vries notes that use of the LN library \i{``meant that little of our subject reduction proof needs to be concerned with alpha-equivalence or freshness"} \cite{deVries07}.

However, EFMT depends on Chargu\'{e}raud's TLC library for \i{non-constructive} logic within Coq \cite{tlc15}. We elected not to use this library, in order to keep the set of axioms minimal and to allow us to explore constructive logic.

\section{Summary of Mechanisation Techniques}

The following table (Figure \ref{mech-summ}) contains a summary of languages and type systems and their mechanisations. A tick (\tick) indicates that a property is true for a given language, a cross (\cross) indicates that it is false and a dash (-) indicates that the property is not applicable. Note that we also write ``Clean" here to mean Edsko de Vries' uniqueness typing system \cite{deVries07}.

\begin{figure}[h]
\caption{Summary of Mechanisation Techniques}
\label{mech-summ}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{System} & \textbf{DU} & \textbf{SU} & \textbf{Cp} & \textbf{Poly} & \textbf{Other} & \textbf{Mechanised?} & \textbf{Naming}\\
\hline
Clean \cite{deVries07} & \tick & \cross & \cross & \tick & - & \tick (Coq) & LN\\
\hline
Rust \cite{rustWeb} & \tick & \cross & - & \tick 	& No GC & \cross & - \\
\hline
$L^3$ \cite{ahmed05} & \tick & \tick & \tick & $\times$ & - & \cross &  - \\
\hline
\rgnUL \cite{fluet06} & \tick & - & \tick & \tick & Cyclone base & \tick (Twelf) & HOAS\\
\hline
\SSPHS \cite{pottier13} & \tick & \tick & \tick & \tick & Hidden state & \tick (Coq) & DB\\
\hline
Mezzo \cite{mezzo14} &  \tick & \tick & \tick & \tick & Data-race free & \tick (Coq) & DB\\
\hline
\end{tabular}
\begin{flushleft}
Key: DU=Destructive Updates, SU=Strong Updates, Cp=Capabilities, Poly=Polymorphism, LN=Locally Nameless, DB=De Bruijn Indices, HOAS=Higher-order Abstract Syntax.
\end{flushleft}
\end{figure}

\newpage

\section{Summary of Previous Work}

In summary, previous work on the formalisation of resource-aware type systems has culminated in the wide-spread use of capabilities. The basic ideas of linear and affine logic have been adapted to form the core of these systems, with some extra features and approaches mixed in (e.g. hidden state and de Vries' use of kinds). \b{Context splitting} plays a key role in almost all systems surveyed. The use of mechanical verification in proofs of type soundness has gained popularity, with most recent works including a formalisation in Coq or Twelf. Other mainstream proof assistants like Isabelle seem to be less used in this space, but we suspect this is primarily due to the limited number of research groups performing this kind of research, and their personal preferences. Several capability systems mention Alias Types \cite{smith00} and separation logic \cite{reynolds02} as foundational concepts, but we defer in-depth discussion of these to future work.

\section{Evaluation Framework}
\label{sec:eval-framework}

In this section we describe some criteria for assessing the quality of work completed as part of this thesis. Given our goal of creating a general Coq library for the verification of linearly-typed languages, what properties should our library ideally possess? We break the criteria into two subsections: \b{Conceptual Goals}, relating to the theoretical content of the library, and its generality and applicability to other languages; and \b{Implementation Goals} regarding the quality of the Coq library itself. Dividing the criteria in this way provides broad coverage of the quality of the work whilst allowing us to separate concerns.

\subsection{Conceptual Goals}

Our main conceptual goal is that the library be applicable and useful in the mechanical formalisation of numerous languages with substructural typing. We say \i{numerous}, rather than \i{all}, because new type systems with substructural influences are still being developed \cite{mcbride16}. The library should be:

\begin{enumerate}
\item Applicable to the formalisation of $L^3$ and related systems.
\item Usable without modifications to the core definitions.
\item Usable with the addition of a minimal number of lemmas about the library's content.
\end{enumerate}

Point (1) is a restatement of our main overarching goal specialised to $L^3$, which we argue is representative of a class of similar languages.

Point (2) expresses the ideal that the verifier of a new linearly-typed language should be able to use the library without making bespoke modifications to the core definitions and lemmas. Such modifications would indicate a lack of generality in the library's content. \i{Usable} here also means that the potential user of the library should not have to work around the library's inadequacies in convoluted ways.

Point (3) relates to the coverage provided by the library's lemmas. Ideally, all of the interesting relationships between its parts should be chronicled as lemmas within the library. We allow some flexibility, to acknowledgement that the ideal is typically very time-consuming to achieve. We believe it is reasonable for users of the library to discover a small number of missing relationships between the library's components which they can then contribute \i{upstream} for general use.

\subsection{Implementation Goals}
\label{sec:impl-goals}

The construction of a large Coq proof, as in the development of any complex piece of software, demands attention to quality of design and implementation. Due to the lack of well established engineering techniques for interactive proofs we describe and justify some of our own criteria here. Discussion of Adam Chlipala's automated style of theorem proving is given as part of a larger discussion about proof automation in the Evaluation chapter (\cref{sec:automation}).

Firstly, our Coq proofs should be \b{easy to read and understand}. By this we mean that additional complexity or obfuscation that detracts from the \i{intent} of the proof should be kept to a minimum. Coq proofs differ from most other bits of code in that they are often difficult to read without knowing the goal and hypotheses at each step, so our criteria for readability must take this into account.

Some heuristics we can use to assess \b{readability} are:

\begin{itemize}
\item Different levels of indentation for goals and sub-goals.
\item Minimal nesting of cases and sub-cases; prefer lemmas.
\item Meaningful variable names at every step; avoid auto-generated names.
\item Short proofs of lemmas.
\end{itemize}

These heuristics are adapted from common software engineering practice, and should be familiar to anyone with a programming background. Short proofs of lemmas are like short functions, and preferring a small lemma to more code in the same lemma is like preferring a utility function to more code in the same function body. The meaningful naming of variables \i{at every step} links back to our altered definition of readability where we imagine that the reader is stepping through the proof examining the goals and hypotheses at each step.

One aim of readable proofs is to convey key insights, but readability also aids \b{maintainability}. Proofs, particularly libraries of proofs, are not static entities and must be constructed so that they can be updated as easily as possible as features are added and new versions of related software are released. Between different versions of the theorem prover the behaviour of tactics can change in ways that are not backwards compatible. References to automatically named variables are considered fragile as a change in the automatic name generation can invalidate the reference and all subsequent proof steps. Some additional criteria to aid \b{maintainability} are:

\begin{itemize}
\item No repetition of proof script.
\item Markers to enforce different cases and sub-cases.
\end{itemize}

Avoiding copied sections of proof script aids maintainability in an obvious way -- changing one section of code doesn't require changing all of its copies.

Enforced case markers improve the clarity of error messages and the general debugging experience. By \i{enforced} we mean that the case markers only allow a new case to begin if the existing case has already been solved. Without case markers a tactic failure early in a proof script can cause tactics on subsequent lines to be applied to the wrong goals, leading to confusing error messages. With case markers, the cases that fail are isolated from their surroundings.

An assessment of the work according to this evaluation framework is given in the Evaluation chapter (\cref{chap:eval}). We turn our attention now to a detailed description of the work.

\chapter{Own Work}

\section{Research Questions}

The primary research question that this thesis attempts to answer relates to the efficient development of mechanically verified proofs about languages with substructural typing. Specifically,

\begin{itemize}
\item What are the common properties of all proofs about substructural languages, and can these properties be exploited to avoid duplicated work in the context of interactive theorem proving?
\end{itemize}

In the context of Coq, this is can concretely be phrased:

\begin{itemize}
\item Is it possible to write Coq lemmas and definitions which are useful for the verification of numerous substructural type systems?
\end{itemize}

\section{Outline of Own Work}

As part of this thesis, the following work has been completed:

\begin{itemize}
\item Development of general definitions and lemmas about context splitting and other actions on typing environments.
\item Proof of type soundness for a linear lambda calculus.
\end{itemize}

The proof of type soundness for the linear lambda calculus consists of a collection of lemmas and definitions, culminating in proof of \b{progress} and \b{preservation} which together establish soundness.

We begin by defining the language for which we have established type soundness, and continue with an in-depth discussion of how it was formalised in Coq. The definitions and lemmas provided by the library are motivated by the proof and are discussed as they arise.

\section{Purely Linear Lambda Calculus}

The language formalised is a wholly linear subset of DILL, extended with uninterpreted primitive types. For brevity, call this language PLLC, for the Purely Linear Lambda Calculus. PLLC provides just enough features to reason about linearity in interesting ways, while not being too labour-intensive to formalise. Its syntax is:
\begin{eqnarray*}
x & \in & \b{Vars} \\
s & \in & \b{Strings} \\
A,B & ::= & \mathds{I} \sor A \lolly B \sor \TyPrim{s} \\
t,u & ::= & * \sor x \sor (\lam{x : A} t) \sor \app{t}{u} \sor \TPrim{s} \\
v & ::= & * \sor x \sor (\lam{x : A} t) \sor \TPrim{s}
\end{eqnarray*}

The primitive types provide some types other than the unit type for the function type constructor to act on. Their inclusion has minimal impact on the proof of soundness and they can be removed without altering the outcome. All of the other terms have the same meaning as in DILL. The typing rules for PLLC are shown in Figure \ref{pllc-typing-rules}.

\begin{figure}[h]
\caption{Typing Rules for PLLC}
\label{pllc-typing-rules}
\begin{displaymath}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{cc}
\infer[\text{(Lin-Var)}]{x : A \types x : A}{} &
\infer[\text{(Unit)}]{\emptyset \types * : \mathds{I}}{} \\
\infer[\text{($\lolly$-I)}]
  {\Delta \types \lam{x : A} t : A \lolly B}
  {\Delta, x : A \types t : B} &
\infer[\text{($\lolly$-E)}]
  {\Delta_1, \Delta_2 \types \app{u}{t} : B}
  {\Delta_1 \types u : A \lolly B  & \Delta_2 \types t : A} \\
\infer[\t{(Prim)}]{\emptyset \types \TPrim{s_1} : \TyPrim{s_2}}{} \\
\end{array}
\end{displaymath}
\end{figure}

With the exception of the rule for primitives, PLLC's typing rules are a subset of DILL's, with the intuitionistic contexts $\Gamma$ removed. Hence, in PLLC, every variable is linear and every function consumes its input. In the ($\lolly$-E) rule, note the use of context splitting $(\Delta_1, \Delta_2)$ to divide the variables available to an application. The rule for primitives allows any primitive term to be assigned any primitive type, the intention being that primitive types are handled by a separate mechanism, as in a compiler.

The operational semantics for PLLC are derived from call-by-value semantics for the Simply-Typed Lambda Calculus \cite{pierce15}. The three rules, shown in Figure \ref{pllc-op-sems} include the $(\beta)$ rule for computation by substitution, and two rules for evaluating applications. The (StepApp1) rule states that if the first half of an application $u$ can step to a term $u'$, then the entire application can take a step by stepping $u$: $\app{u}{t} \steps \app{u'}{t}$. The (StepApp2) rule is a similar rule for the second sub-term of an application, and applies only once the first sub-term has been reduced to a value.

By enforcing the restriction that the first sub-term of the application in (StepApp2) is a value, an evaluator never needs to choose which side should be evaluated first, thus removing non-determinism. If we include the $(\beta)$ rule in our consideration we see that the stepping relation as a whole is deterministic, although this property isn't verified in our Coq proofs.

\begin{figure}[h]
\caption{Operational Semantics for PLLC}
\label{pllc-op-sems}
\begin{displaymath}
\arraycolsep=1.4pt\def\arraystretch{2.2}
\begin{array}{cc}
\infer[(\beta)]{\app{(\lam{x : A} t)}{v} \steps t[v / x]}{} \\
\infer[\t{(StepApp1)}]{\app{u}{t} \steps \app{u'}{t}}{u \steps u'} \\
\infer[\t{(StepApp2)}]{\app{v}{t} \steps \app{v}{t'}}{t \steps t'} \\
\end{array}
\end{displaymath}
\end{figure}

As is somewhat standard in syntax-driven Coq proofs about programming languages \cite{pierce15}, we use inductive definitions for the terms, types and typing judgements of the language. Translating the semi-formal description of the language into Coq definitions is straight-forward once a few key parts of the representation are decided upon. Specifically:

\begin{itemize}
\item How do we represent the names of variables? As discussed in \cref{sec:var-naming}, we elect to use \Francois Pottier's DbLib library for de Bruijn indices.
\item How is substitution implemented? DbLib provides a substitution function given a few basic properties of a language. The implementation of this is discussed in the next section, \cref{sec:dblib-int}.
\item How are typing contexts represented? How is context splitting implemented? These questions are the topic of the upcoming sections: \cref{sec:repr-ty-contexts}, \cref{sec:emptiness}, \cref{sec:context-splitting}.
\end{itemize}

Discussion of the exact representation of terms, typing judgements and operational semantics is given in the Appendix, sections \cref{app:pllc-syntax}, \cref{app:pllc-typing-rules}, \cref{app:pllc-op-sems}.

Type soundness is the property that we would like to establish to show that PLLC is a well-behaved programming language. We do this by proving progress and preservation lemmas, as defined in \cref{sec:op-sems-soundness}. The statement of these lemmas in Coq is given in \cref{sec:progress} and \cref{sec:preservation}, as well as consideration of how they were proved.

The proofs of progress and preservation are \i{language-specific} in that they only apply to PLLC. We are primarily interested in the \i{universal} lemmas which can be used independently of PLLC and may be applicable to the verification of other linearly typed languages. In general, any lemmas that reference the terms, semantics or typing rules of PLLC are language specific, while lemmas about context splitting and typing contexts are universal and are part of our library.

\section{Integrating with DbLib}
\label{sec:dblib-int}

DbLib provides functions for substitution and lifting that abstract over the manipulation of de Bruijn indices. Client libraries wanting to make use of DbLib need only implement a few fundamental operations via Coq's \i{type-classes} \cite{coqTypeClasses}. For the linear lambda calculus we can use essentially the same definitions as for the simply typed lambda calculus, which are provided as an example with DbLib.

First, we must inform DbLib which of our term constructors is for variables. DbLib allows the types of \i{values} (\c{V}) and \i{terms} (\c{T}) to differ, but we don't make use of this capability, instead using \c{term}s everywhere and the \c{value} predicate. The type-class has the following definition in DbLib:

\begin{coqcode}
Class Var (V : Type) := {
  var: nat -> V
}.
\end{coqcode}

Our instance is straight-forward:

\begin{coqcode}
Instance Var_term : Var term := {
  var := TVar
}.
\end{coqcode}

To convey how variables are bound and scoped, we must implement DbLib's \c{Traverse} type-class, which has a single function called \c{traverse}. From the DbLib documentation:

\begin{displayquote}
\c{traverse} can be thought of as a semantic substitution function. The idea is, \c{traverse f l t} traverses the term \c{t}, incrementing the index \c{l} whenever a binder is entered, and, at every variable \c{x}, it invokes \c{f l x}. This produces a value, which is grafted instead of \c{x}.
\end{displayquote}

The only binders in our linear lambda calculus are lambda abstractions, so our implementation of traverse only has to increment \c{l} when recursing below a \c{TAbs} constructor. This is the same as for the simply typed lambda calculus. If more substitution functions were required, as would be the case in a language with polymorphism and type substitutions, distinct named instances of \c{Traverse} could be created for each use.

\begin{coqcode}
Fixpoint traverse_term (f : nat -> nat -> term) l t :=
  match t with
  | TVar x =>
      f l x
  | TAbs t e =>
      TAbs t (traverse_term f (1 + l) e)
  | TApp e1 e2 =>
      TApp (traverse_term f l e1) (traverse_term f l e2)
  | _ => t
  end.
\end{coqcode}

To ensure that the client's implementation of traverse behaves sensibly and can be manipulated accordingly, DbLib requires the implementation of five further type-classes that establish semantic properties of traverse. Also provided are five tactics for proving these properties automatically, which we found to be sufficient for our simple use-case.

Given these type-class definitions, DbLib provides a substitution function that we can make use of in the operational semantics for our language. The type of the substitution function is \c{V -> nat -> T -> T}, which is specialised to \c{term -> nat -> term -> term}. The Coq implementation of the $(\beta)$ rule makes use of it, as shown in \cref{app:pllc-op-sems}.

\section{Representing Typing Contexts}
\label{sec:repr-ty-contexts}

Before defining an inductive predicate for our typing relation, a representation for typing environments must be selected. In semi-formal proofs, typing judgements are written $H \vdash e : \tau$, and the environment $H$ is assumed to permit various operations such as looking up the type of a variable $x$ (denoted $H[x]$) and (re)assigning a type to a variable $x$, (denoted $H[x \mapsto \tau]$).

As noted in the Background chapter, control over the actions permitted on typing contexts is at the core of substructural typing. Together with our choice to use de Bruijn indices for variable naming, this narrows down our choice of representation. We have the following options to consider:

\begin{itemize}
\item \textbf{Functions}: Some Coq formalisations of languages with structural typing \cite{pierce15} make use of functions to encode partial maps from variables to types. Assigning a type involves wrapping the existing environment in another conditional statement, as in \c{insert x $\tau$ H := fun y => if x = y then Some $\tau$ else H y}. This approach is unsuitable for substructural type systems because the function is opaque and can't be disassembled into two functions which are equivalent when combined. Given an arbitrary function, it is impossible to know that it is always going to consist of a conditional of the form shown, and therefore it is also impossible to extract any of the information about x, $\tau$ or the original $H$.
\item \textbf{Lists of Types}: We could consider using a list of types so that the type for variable $\hat{i}$ is at index $i$. This is preferable to using a function because we can inspect and destructure a list, and can also perform induction. However, splitting an environment becomes problematic because we need to keep the type for variable $\hat{i}$ at index $i$, even if some or all of the types at indices less than $i$ should no longer be available because they were assigned to the other side of the split. Essentially, if we are to use a list, we need a filler value to occupy the evacuated positions. This leads to our next option:
\item \textbf{Lists of Optional Types}: What if we rather than using a list of types, we use a list of \c{option type}, so that types which are no longer available are represented by \c{None} entries? This fulfils all of our requirements: we can look-up types, alter them, add new entries and split an environment so that variables and their types are divided between the two new environments.
\end{itemize}

For these reasons, our formalisation makes use of a list of optional types, as provided by the \c{env} type from DbLib. However, the lemmas about \c{env} provided by DbLib proved to be insufficient for reasoning about substructural typing rules. For example, DbLib treats the empty list \c{[] (nil)} as the only empty environment, when it is often useful to treat any number of \c{None}s as an empty environment. Further, context splitting defined on \c{Env} is general enough to be of use in multiple DbLib-based formalisations, so why not re-use this effort? The next two sections describe these two aspects of our formalisation, and this thesis's contribution to a general framework for substructural languages.

\section{Emptiness}
\label{sec:emptiness}

We define the following predicate for environments which is compatible with any environment from DbLib. We say that an environment is \i{empty} if it contains no typing information. Hence, the empty list environment (\c{nil}) is empty, as is any number of \c{None} values.

\begin{coqcode}
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons E (EmptyTail : is_empty E) : is_empty (None :: E).
\end{coqcode}

The necessity of a predicate for emptiness arises from several uses of \i{inductive loading} in proofs of lemmas related to substitution and type preservation. Using \c{nil} environments proved to be too limiting, and there are several cases in the proof where an induction generates an empty environment like \c{raw_insert x None nil}, and has to apply an inductive hypothesis about empty contexts to it.

In order to be useful in the verification of languages, lemmas about the properties of the \c{is_empty} predicate and its interaction with other parts of the system are required. Of these lemmas, the ones involving interaction with DbLib were slightly more difficult to prove than those about simple Coq constructs. Examples of lemmas about emptiness are shown below, the full set can be found in the file \c{Linear/Empty.v} in the accompanying source code (\cref{sec:src}).

\begin{coqcode}
Lemma empty_repeat : forall A (E : env A),
  is_empty E ->
  E = repeat (length E) None.
(* Proof by induction on is_empty E *)

Lemma empty_lookup : forall A x (E : env A),
  is_empty E ->
  lookup x E = None.
(* Proof by induction on x *)
\end{coqcode}

\section{Context Splitting}
\label{sec:context-splitting}

% You've met context splitting before.
% High-level description.
% Coq code.
% Inductive definition vs function definition (nah).
% Discussion of split single.
% Discussion of difficulty of existence lemmas -> worth having in a library.
% GOOD REASONS WHY insert/tl is frustrating!
% Discussion of similarity to other approaches (de Vries mixin approach).
%    Justification that the definition is general enough.

Context splitting is the operation by which a typing environment, implemented as a \c{list (option T)}, is split so that it may be used to type two expressions. We use the notation $E = E_1 \circ E_2$ to represent the splitting of $E$ into two environments $E_1$ and $E_2$ such that each variable from $E$ appears in only one of $E_1$ and $E_2$.

In PLLC, the typing rule for application requires that the context used to type $(e_1 e_1)$ can be split into two contexts that type $e_1$ and $e_2$ individually. This splitting causes variables from the typing context to be used at most once in the expression $(e_1 e_2)$, as each element of the context must be assigned to either the left or right sub-expression.

As we are working with a list, it makes sense to preserve the length of the context when splitting. As a base case we have $[] = [] \circ []$. If the list contains one or more elements, then that element can either be assigned to the left or to the right. In Coq, we define an inductive predicate over contexts so that \c{context_split E E1 E1} $\equiv E = E_1 \circ E_2$.

\begin{coqcode}
Inductive split_single {A} : option A -> option A -> option A -> Prop :=
  | split_none : split_single None None None
  | split_left (v : A) : split_single (Some v) (Some v) None
  | split_right (v : A) : split_single (Some v) None (Some v).

Inductive context_split {A} : env A -> env A -> env A -> Prop :=
  | split_nil : context_split nil nil nil
  | split_cons E E1 E2 v v1 v2
      (SplitElem : split_single v v1 v2)
      (SplitPre : context_split E E1 E2) :
      context_split (v :: E) (v1 :: E1) (v2 :: E2).
\end{coqcode}

\subsection{Single Element Splits}

One may wonder if the presence of an accompanying \c{split_single} predicate is necessary, as the same meaning can be achieved with the following definition that places the splitting of single elements in-line:

\begin{coqcode}
Inductive context_split : env A -> env A -> env A -> Prop :=
  | split_nil : context_split nil nil nil
  | split_left E E1 E2 v
  	  (SplitPre : context_split E E1 E2) :
  	  context_split (v :: E) (v :: E1) (None :: E2)
  | split_right E E1 E2 v
  	  (SplitPre : context_split E E1 E2) :
  	  context_split (v :: E) (None :: E1) (v :: E2).
\end{coqcode}

Although for the purposes of forward reasoning both definitions are equally convenient, when performing inversion on terms of type \c{context_split E E1 E2} it is often useful to be able to abstract over the splitting of individual elements, particularly as this means there are less cases generated by the inversion.

For example, in the proof of \c{insert_none_split_backwards}, using context splitting on single elements saves duplicating or creating automation for the main part of the proof. The statement of the lemma is:

\begin{coqcode}
Lemma insert_none_split_backwards : forall A (E : env A) E1 E2 x,
  context_split (raw_insert x None E) E1 E2 ->
  exists E1' E2',
    E1 = raw_insert x None E1' /\
    length E1' = length E /\
    E2 = raw_insert x None E2' /\
    length E2' = length E /\
    context_split E E1' E2'.
\end{coqcode}

Intuitively, this lemma states that if we have inserted \c{None} into the typing context at the position for variable \c{x}, then the two environments resulting from the split of this environment must also contain \c{None} at position \c{x}. The proof is by case analysis on whether or not the variable inserted is past the end of the environment (\c{x >= length E}). In the case where it \i{is not} past the end, and \c{x} is greater than 0, we reach a goal of the form:

\begin{coqcode}
exists X1 X2 : env A,
  e1 :: E1' = raw_insert (S x') None X1 /\
  length X1 = S (length E') /\
  e2 :: E2' = raw_insert (S x') None X2 /\
  length X2 = S (length E') /\
  context_split (e :: E') X1 X2
\end{coqcode}

Here, we have \c{E = e :: E'}, \c{E1 = e1 :: E1'}, \c{E2 = e2 :: E2'}, \c{x = S x'} relative to the statement of the lemma.

Without a \c{split_single} definition, inversion of \c{context_split (e :: raw_insert x' None E') (e1 :: E1') (e2 :: E2')} forces us to consider the cases where \c{e1 = None, e2 = Some t} and \c{e1 = Some t, e2 = None} separately. In this proof, this isn't an important distinction, because we really just need to apply the inductive hypothesis to obtain \c{E1''} and \c{E2''} such that \c{E1' = raw_insert x' None E1''} and \c{E2' = raw_insert x' None E2''}. We can then instantiate \c{X1} and \c{X2} with \c{exists (e1 :: E1''), (e2 :: E2'')} and knock over the remaining goals with facts from the proof context. This all works with a \c{split_single} definition, but without one, the two cases have to be handled with two different calls to the \c{exists} tactic; requiring duplication of proof script, or custom Ltac to parametrise the calls.

Hence, \c{split_single} is a more general and useful way for dealing with context splitting, as it allows us to prove goals of the form \c{context_split (e :: E) (e1 :: E1) (e2 :: E2)} without knowing the exact values of \c{e}, \c{e1} and \c{e2}. If the exact values \i{are} required for a proof, they can still be considered by performing inversion on the \c{split_single e e1 e2} fact.

\subsection{Properties of Context Splitting}

We prove several basic properties of context splitting, such as commutativity, as well as lemmas motivated by the proof of soundness for the linear lambda calculus.

\textbf{Length} If $E = E_1 \circ E_2$, then the lengths of all three contexts are the same.

\textbf{Commutativity}

If we can split $E = E_1 \circ E_2$, then we can also split $E = E_2 \circ E_1$. In Coq, proof is by a straight-forward induction on the structure of the \c{context_split}, and depends on a similar commutative property of \c{split_single}.

\begin{coqcode}
Lemma split_commute : forall A (E : env A) E1 E2,
  context_split E E1 E2 -> context_split E E2 E1.
Proof with boom.
  intros A E E1 E2 Split.
  induction Split...
Qed.
\end{coqcode}

\textbf{Associativity} If $E = E_0 \circ (E_1 \circ E_2)$ then $E = (E_0 \circ E_1) \circ E_2$.

In Coq we need an existential to express the existence of a context that splits into $E_0$ and $E_1$.

\begin{coqcode}
Lemma split_assoc : forall A (E E0 E1 E2 E12 : env A),
  context_split E E0 E12 ->
  context_split E12 E1 E2 ->
  (exists E01, context_split E E01 E2 /\ context_split E01 E0 E1).
\end{coqcode}

The justification for calling this operation associativity is the view of $(\circ)$ as a \i{partial} binary operator for combining contexts. From this view, context splitting is a partial commutative monoid, with \c{repeat n None} as the identity element. Another name for this structure is a \i{separation algebra} (assuming that $(\circ)$ is also cancellative, which it is). Further discussion of this is given later when evaluating our approach, in \cref{sec:sep-logic}.

\textbf{4-way Splits}

In his proofs about uniqueness typing Edsko de Vries \cite{deVries07, deVriesPhD08} establishes all of these properties, but proves the associativity lemma using a 4-way split lemma like this: $E = (E_{1a} \circ E_{1b}) \circ (E_{2a} \circ E_{2b}) \longrightarrow E = (E_{1a} \circ E_{2a}) \circ (E_{2a} \circ E_{2b})$. In addition to associativity being provable from this lemma, the opposite is also true, and we prove a version of this lemma called \c{split_swap} using a few simple applications of commutativity and associativity.

\textbf{Rotation} The proof of preservation requires a lemma of the form: $E = E_0 \circ (E_1 \circ E_2) \Rightarrow E = E_1 \circ (E_0 \circ E_2)$, which is provable using the other lemmas above.

\textbf{Emptiness} Lemmas about empty contexts as the identity element for context splitting are provable using a few lines of proof script. For details, see \c{Linear/Context.v} in the accompanying source code (\cref{sec:src}).

\subsection{Lemmas Required for Soundness}

In order to prove soundness, several lemmas about the interaction between inserts and context splitting were required. Although many of the lemmas are conceptually simple, some required significant effort and represent a large portion of the work involved in the proof of soundness for the linear lambda calculus. Many lemmas also involve the insertion of \c{None} values into contexts, which is a consequence of the strengthened induction required to prove the substitution lemma (see the next section on substitution, \cref{sec:subst}).

The first, and perhaps most difficult to prove, was \c{insert_none_split_backwards}, discussed previously in the section about \c{split_single}. We attribute the difficulty in proving the lemma to the fact that we initially lacked ways to express many of the intuitive reasons for the lemma's truth, described below. With the right definitions and lemmas in place, the proof became quite straight-forward.

DbLib's \c{insert} function behaves in two different ways depending on the relationship between the variable \c{x} being inserted and the length of the existing environment, \c{E}. If \c{x < length E}, then the type for \c{x} is inserted \i{between} existing elements, with subsequent elements being shunted along. Conversely, if \c{x >= length E}, then the type of \c{x} is inserted \i{after} existing elements, with the intervening space padded by \c{None} values. These two cases are quite different to reason about, and any attempt at a direct inductive proof on \c{x} or one of the environments inevitably leads to wanting to know which of the two cases one is considering. For this reason, the first step of our proof is to split on the comparison between \c{x} and \c{length E}.

To handle the case where \c{x >= length E} and padding \c{None} values are inserted, we note that the values past the end of the old environment will all be \c{None}. Equipped with a simple definition of a \c{repeat} function on lists, and some lemmas about list append, the proof of this case is simple. Determining the right abstractions is the hardest part.

\newpage
\begin{coqcode}
Lemma insert_none_def : forall A x (E : env A),
  x >= length E ->
  raw_insert x None E = E ++ repeat (S (x - length E)) None.

Lemma split_app : forall A (E : env A) E1 E2 n,
    context_split (E ++ repeat n None) E1 E2 ->
    exists E1' E2',
      E1 = E1' ++ repeat n None /\
      E2 = E2' ++ repeat n None /\
      context_split E E1' E2'.
\end{coqcode}

The other case where the new \c{None} value is inserted in between existing elements is handled by an induction on \c{x}, which is made simpler by knowing the bound on \c{x}, i.e. \c{x < length E}. For example, it absolves us from having to deal with the case where the environment is empty, which proved to be a nuisance in early versions of the proof. Particularly as \c{length (tl E1) = length (tl E2)} does not imply that \c{length E1 = length E2} as one may expect outside a total programming language.

A few other lemmas about context splitting and interactions with DbLib's \c{insert} were required for the proof of soundness. They can be found in the accompanying proof sources, see \cref{sec:src}. In general, the lemmas involving existentials required more cunning to prove, and subsequently more cunning to apply, as discussed in \cref{sec:named-destruct}.

\section{Progress}
\label{sec:progress}

With typing rules and small step semantics established we can state the progress lemma which contributes to the proof of type soundness.

\begin{coqcode}
Theorem progress : forall E e t,
  is_empty E ->
  E |- e ~: t ->
  (exists e', step e e') \/ value e.
\end{coqcode}

This theorem states that any closed term $e$ that is well-typed can either be stepped using the small step semantic relation, or is already a value and cannot be evaluated further.

Proof is by induction on the typing derivation \c{E |- e \textasciitilde: t}. Unlike the proof of preservation presented in the next section, the proof of progress requires very few supporting lemmas about context splitting or substitution. Most of the cases in the induction require reasoning about the interaction between typing and term structure, which can be handled by simple inversions. For example, one of the supporting lemmas states that any value assigned a function type under an empty environment must necessarily be a $\lambda$-abstraction:

\begin{coqcode}
Lemma fun_value_is_abs : forall E e t1 t2,
  is_empty E ->
  E |- e ~: TyFun t1 t2 ->
  value e ->
  (exists e', e = TAbs t1 e').
\end{coqcode}

Given that most of the verification effort was expended reasoning about context splitting, substitution and other lemmas required for preservation, the effort required to prove progress represents a relatively small fraction of the total effort. The fact that progress and preservation each form ``half" of the soundness proof does not imply that the difficulty of proving soundness is split evenly between them.

\section{Preservation}
\label{sec:preservation}

The preservation lemma states that if a well-typed closed term $e$ can take a step to another term $e'$, then $e'$ is also well-typed. In other words, the type of a term is \i{preserved} as it is evaluated in accordance with the small-step semantics.

\newpage
\begin{coqcode}
Theorem preservation : forall E e e' t,
  is_empty E ->
  E |- e ~: t ->
  step e e' ->
  E |- e' ~: t.
\end{coqcode}

Proof is by induction on the stepping of $e$ to $e'$, \c{step e e'}. In the three cases that result from the three stepping rules, the two for applications are proved directly from the inductive hypothesis. The remaining case for $\beta$-reduction involves interaction between substitution, typing and context splitting, and is proved via a supporting \i{substitution} lemma.

\subsection{Substitution}
\label{sec:subst}

The substitution lemma states that the result of a substitution is well-typed if the terms involved are well-typed. With substructural typing, we must also consider the supply of free variables to both terms using context splitting, so the lemma takes the form:

\begin{eqnarray*}
\infer[\text{Substitution}]{
    E_1 \circ E_2 \types e_2 [e_1/x] : \tau_2
}{
    E_1 \types e_1 : \tau_1 \quad E_2, x : \tau_1 \types e_2 : \tau_2
}
\end{eqnarray*}

Substitution lemmas similar to this are common in proofs of similar complexity, as in \i{Software Foundations} \cite{pierce15} and the examples accompanying DbLib. In Software Foundations, a weakening lemma is used in the proof of the substitution lemma, but with substructural typing this technique is unavailable.

In Coq the lemma is:

\begin{coqcode}
Lemma substitution: forall E2 e2 t1 t2 x,
  insert x t1 E2 |- e2 ~: t2 ->
  forall E E1 e1, E1 |- e1 ~: t1 ->
  context_split E E1 E2 ->
  E |- (subst e1 x e2) ~: t2.
\end{coqcode}

Proof is by \i{dependent induction} on the judgement \c{insert x t1 E2 |- e2 \textasciitilde: t2}. Dependent induction allows us to take into account the fact that the environment is \c{insert x t1 E2} rather than an unadorned variable (e.g. $E$). This is achieved by replacing instantiated variables with general ones, and then adding constraining equalities. In this case, the only instantiated variable is \c{insert x t1 E2}, which dependent induction will replace by a new universally quantified variable $E_\text{new}$ and the equation $E_\text{new} = \c{insert x t1 E2}$. With the goal in the form described the dependent induction tactic then applies the induction principle for \c{has_type} to generate sub-goals for each of the cases, whilst preserving the newly added equality constraints. Coq's dependent induction is based on Conor McBride's \c{BasicElim} tactic \cite{mcbride00} which makes use of Conor's humorously named ``John Major" heterogeneous equality (\c{JMeq}). Heterogeneous equality requires the addition of an axiom, but our use of it is restricted to the proof-of-concept and it isn't required to use the library.

Simple inversion removes the cases that are absurd due to the \c{insert x t1 E2} environment, leaving three cases for variables, $\lambda$-abstractions and applications. The variable case is handled by some straight-forward reasoning about empty contexts, and requires no new supporting lemmas, while the other two cases form the motivation for several of the lemmas about inserts and context splitting.

The $\lambda$-abstraction case requires a proof that the term being substituted is well-typed under the environment for the abstraction sans binder, i.e. \c{(None :: E1) |- shift 0 e1 ~: t1}. This motivates \c{typing_insert_none}, which in turn motivates \c{insert_none_split}. Although \c{typing_insert_none} is more general in that it allows us to prove facts of the form \c{raw_insert x None E |- e ~: t} and not just \c{raw_insert 0 None E |- e ~: t}, generalising for all $x$ makes related lemmas easier to prove by enabling induction on $x$. This technique is sometimes referred to as \i{inductive loading}.

The application case requires that one side of the application be well-typed without referring to the substitution variable $x$. This motivates the following lemma, \c{typing_insert_none_subst}:

\begin{coqcode}
Lemma typing_insert_none_subst : forall E e x junk t,
  raw_insert x None E |- e ~: t ->
  E |- subst junk x e ~: t.
\end{coqcode}

This in turn motivates the rest of the lemmas about inserting none into a typing context, including the difficult to prove \c{insert_none_split_backwards} -- discussed above.

To prove this lemma, DbLib was extended with a \i{lowering} operation that is conceptually inverse of lifting. In the case where variables are lowered by 1 we call the operation \i{unshifting}, by analogy with lifting by 1 (shifting). The lemma is proved by establishing that a similar lemma holds for \c{unshift x e}, and an equivalence of \c{unshift x e} and \c{subst junk x e} when \c{x} does not appear free in \c{e} (see \c{contains_var}).

The addition of lowering to DbLib was the only substantial change required for our proof of soundness for PLLC. A few other minor changes to use \c{raw_insert} instead of \c{insert} in some lemmas were also made. Although the lowering operation might be useful upstream, further work is required to unify it with lifting to reduce the maintenance burden for DbLib \cite{dblibPullRequest}.

\section{Summary}

Our Coq formalisation of PLLC was able to make use of universal lemmas about emptiness and context splitting developed as part of this thesis. These lemmas represent a significant portion of the effort required to establish type preservation for PLLC. Further, the formalisation was able to make successful use of \Francois Pottier's DbLib library, for substitution operations and basic actions on typing environments. The creation of a generic library for context splitting provides a positive answer to our primary research question about exploiting commonalities in formalisations of languages with substructural typing. The extent to which the library is applicable to other formalisations is discussed in the next chapter.

\chapter{Evaluation}
\label{chap:eval}

In this section we assess the quality of the work according to the Evaluation Framework of Section \cref{sec:eval-framework}. We discuss the success of the library according to its conceptual goals, primarily by outlining how a formalisation of $L^3$ may make use of the library. Further, we evaluate the library's implementation according to the implementation goals and discuss possible improvements.

\section{Generality and Applicability}
\label{sec:generality}

According to the conceptual goals of our evaluation framework, we would like the library to be applicable to the formalisation of languages more complex than the linear lambda calculus. In concrete terms, this means that the context splitting operation provided by the library should be useful in constructing syntactic proofs of soundness for some class of languages with substructural typing. We argue that this class of languages includes those based on DILL -- which could act as a foundation for further formalisations.

Extending the proof-of-concept LLC proof to a complete formalisation of DILL would require the addition of an intuitionistic context, product types and bang types. The paper for DILL \cite{barber96} includes proofs of substitution lemmas similar to the one used in the proof of preservation for our LLC, which suggests that it would admit a complete proof of soundness in a syntactic style. This is in contrast to several earlier systems based on linear logic, which Philip Wadler showed \i{do not} have substitution lemmas \cite{wadlerNoSubst}. Wadler's result is our primary motivation for using DILL as a foundation, motivated further by Pottier's success with DILL-based syntactic soundness proofs in SSHPS \cite{pottier13}. Interestingly, a draft version of the $L^3$ paper from 2001 cites Wadler's result as a reason to avoid a syntactic proof, but this claim is absent from the final paper \cite{ahmed05}.

The proofs of progress and preservation for DILL could re-use much of the proof effort for PLLC. For example, the ($\otimes$-I) rule is similar to PLLC's existing ($\lolly$-E) rule, implying that inductive cases related to ($\otimes$-I) could be handled using the same library lemmas used for ($\lolly$-E). Further, the lemmas provided by the library about \c{insert} and context splitting would aid in reasoning about ($\otimes$-E) and (!-E), which both include augmented contexts in their premises.

Our approach may also be applicable to a modified version of Edsko de Vries' uniqueness type system \cite{deVries07, deVriesPhD08}. Edsko's soundness proof is syntactic and makes use of an inductive context splitting relation that is identical to ours except that it allows non-unique (intuitionistic) types to be split to both sides. To make his system compatible with purely linear context splitting a stand-alone contraction rule could be added, in the style of Wadler's linear lambda calculus of 1993 \cite{wadler93}. The contraction rule would provide the ability to duplicate non-unique values, which would previously have been provided by context splitting.

\section{Towards a Coq Formalisation of $L^3$}
\label{towards-l3}

\i{The Linear Language with Locations}, $L^3$, is a good candidate for assessing the applicability of our approach to languages beyond the linear lambda calculus. As argued in the Background chapter, its use of linear capabilities and shared pointers is typical of other modern calculi supporting destructive updates.

$L^3$ seems like it may be amenable to mechanical verification with our library because like DILL and Wadler's linear lambda calculus, contexts are split in a purely linear way. Intuitionistic terms are handled by the $\dupl{t}$ and $\drop{t}$ primitives, rather than implicit or explicit contraction and weakening, which means the \c{context_split} predicate could be employed unaltered. However, as noted in the $L^3$ paper \cite{ahmed05}, the operational semantics and typing rules are not set-up for a syntactic proof of soundness. As an example, consider the preservation lemma for the memory allocation primitive $\lnew{v}$. Under the operational semantics the term steps unconditionally: $(\sigma, \lnew{v}) \steps (\sigma \uplus \{l \to v\}, \qpair{l}{\capa \otimes !(\ptr{l})})$, hence we should have:

\begin{eqnarray*}
\infer{\Delta; \Gamma \types \qpair{l}{\capa \otimes !(\ptr{l})} : \Exists{\rho} A}{
  \Delta; \Gamma \types \lnew{v} : \Exists{\rho} A  & (\sigma, \lnew{v}) \steps (\sigma \uplus \{l \to v\}, \qpair{l}{\capa \otimes !(\ptr{l})})
}
\end{eqnarray*}

The premises are both true, by the rules (New) and (new), but the conclusion is unprovable. It might seem that we could apply the (LPack) rule, but this would require $l \in \Delta$, which is nonsensical because $l$ is a location constant, not a location variable. Further, we can't type the pair $\capa \otimes !(\ptr{l})$ because there are no typing rules for lone capabilities or pointers. To do a syntactic proof of soundness would therefore require altering $L^3$'s typing rules. The authors of the $L^3$ paper suggest using \i{store typing}, as in Alias Types \cite{smith00}, whereby constraints on the runtime store $\sigma$ are expressed in the typing rules. In this case, reasoning about context splitting would likely only comprise a small fraction of the proof effort. Pottier's proof for \SSPHS and the proof for \rgnUL are around 20,000 lines of Coq and Twelf code respectively, implying that the complexity of reasoning about these language features (destructive strong updates) increases proof complexity significantly. For comparison, our linear lambda calculus formalisation is around 1,300 lines of Coq code including the library for context splitting.

\section{Further Work: Separation Logic}
\label{sec:sep-logic}

Conor McBride's work on linear dependent types (\cref{sec:linear-dep}), and \Francois Pottier's on \SSPHS (\cref{sec:ssphs}) both make use of typing contexts that record the number of available occurrences of each variable, and use concepts from separation logic to handle context splitting. This approach is strictly more general than our context splitting library, and a library based on these ideas would likely form a better foundation for general reasoning about typing contexts in linear languages. In this sense, our library is lacking in generality. Further, ideas from separation logic are cited as foundational in much of the literature surveyed, and could be exploited for multiple purposes, as Pottier did with \SSPHS \cite{pottier13}. Indeed, work on creating generalised libraries for separation algebras has already been completed, in Coq \cite{dockins09} and Isabelle/HOL \cite{klein12}.

\section{Quality of Coq Proofs}

As identified in the Implementation Goals (\cref{sec:impl-goals}) section of our evaluation criteria, we would like the library to be both \b{readable} and \b{maintainable}.

\subsection{Case Analysis and Indentation}

To separate different cases when performing induction and destructuring, the proofs make use of Benjamin Pierce's \c{Case} markers from Software Foundations \cite{pierce15}. Combined with indentation, these markers fulfil our desire to create readable proofs that are also resistant to corruption upon refactoring. The \c{Case} tactic ensures that the proof remains structured by failing if an existing case at the same level remains unproven, as can happen if an earlier tactic fails. The following example taken from the proof of \c{insert_none_def} demonstrates our usage of the markers and our indentation scheme:

\begin{coqcode}
induction x as [|x']; intros.
Case "x = 0".
  destruct E as [|e E'].
  SCase "E = []".
    rewrite raw_insert_zero...
  SCase "E = e :: E'".
    solve by inversion.
\end{coqcode}

If, for example, the tactic \c{rewrite raw_insert_zero...} fails to prove the goal for the case where \c{E = []}, then the \c{SCase "E = e :: E'"} tactic will fail and prevent the proof from proceeding, clearly signalling that the error lies in the previous case. The string arguments are uninterpreted but provide useful documentation.

The style of proof shown above was followed meticulously throughout the entire development, aiding both readability and maintainability. Excessive nesting of cases was also successfully avoided, with the deepest level of nesting being 3 sub-cases deep (an \c{SSCase}), occurring only once in the proof-of-concept formalisation.

An alternative to using Pierce's \c{Case} markers would have been to use Coq's \i{bullets}, which are available as part of Coq's core since version 8.4 \cite{coqRefMan}. They function identically to the case markers except that documentation strings can't be embedded as case descriptions.

\subsection{Avoiding Auto-Generated Variable Names}

Avoiding references to automatically generated names is an important part of creating a readable and maintainable Coq proof. Generated names are subject to change between Coq versions, potentially rendering all pieces of proof script reliant on them in need of upgrading. Furthermore, updating the proof can be difficult if the exact values that the names were referring to have been forgotten, effectively requiring old goals to be solved anew.

Although there are several straight-forward techniques that can be used to avoid generated names, in practice we didn't manage to avoid them entirely, notably when using dependent induction.

\subsubsection{Introduction Patterns}

When performing induction it is often necessary to leave some hypotheses as premises of the inductive hypothesis. In order to avoid generating names for these hypotheses we followed a pattern whereby the \c{intros} tactic would be used to name every hypothesis, \c{generalize dependent} would be used to re-quantify the necessary variables, and then induction performed. This leads to hypotheses being re-introduced with the names used by \c{intros}, rather than generated ones. For example:

\begin{coqcode}
Lemma empty_lookup : forall A x (E : env A), is_empty E ->
  lookup x E = None.
Proof.
  intros A x E Empty.
  generalize dependent E.
  induction x as [|x'].
  Case "x = 0".
    intros. inversion Empty; auto.
  (* Proof continues... *)
\end{coqcode}

Here the fact \c{is_empty E} is named \c{Empty} by \c{intros}, abstracted over, and then automatically re-introduced with the name \c{Empty}. This relies partly on Coq's name generator to remember the name from the first application of \c{intros}, but this is less fragile than relying on entirely automatic naming.

\subsubsection{Named Constructor Arguments}

A cosmetic variation of the above pattern for introductions could use named function arguments instead of an explicit \c{forall}. This is the approach taken for all inductive constructors, with the aim of generating unique names during inversion. For example, we can declare the \c{is_empty} predicate in two semantically equivalent ways:

\newpage
\begin{coqcode}
(* With implicit argument names *)
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons : forall E, is_empty E -> is_empty (None :: E).

(* With explicit argument names (preferred) *)
Inductive is_empty {A} : env A -> Prop :=
  | is_empty_nil : is_empty nil
  | is_empty_cons E (EmptyTail : is_empty E) : is_empty (None :: E).
\end{coqcode}

With the second variant, when inverting a fact of the form \c{is_empty E} the fact about the environment's tail will be named \c{EmptyTail} if that name isn't already taken. In practice this was found to be quite effective, as most proofs required only a single inversion per data-type. In cases where more control over naming is needed, names can be provided to the tactic, as discussed in the next section.

\subsubsection{Named Destructuring}
\label{sec:named-destruct}

Named destructuring allows names to be provided to tactics like \c{induction}, \c{destruct} and \c{inversion} which would otherwise generate names automatically. For simple cases like induction on natural numbers and lists, we found it to be highly effective, e.g. \c{induction n as [|n']}. However, destructuring large chains of existentials, conjunctions and disjunctions in this manner quickly becomes unwieldy.

For example, our proof of soundness for PLLC contains a line: \c{destruct AppPreSplit as [E1' [E2' [? [? [? [? ?]]]]]]}. This is to the detriment of both readability and maintainability, in particular because changes to the lemma that creates \c{AppPreSplit} would require digging into several layers of brackets to ensure the names are correct and that the chain of conjunctions is fully decomposed. This is an area where the library falls short of the evaluation criteria. Further work could seek to eliminate maintenance burdens such as these, possibly using Arthur Charguéraud's improved tactics library, \c{LibTactics} \cite{tlc15}.

\subsubsection{Dependent Induction}

Dependent induction is used at one point in the proof of soundness for PLLC, as discussed in \cref{sec:subst}. Unlike the regular induction tactic, Coq's dependent induction tactic doesn't provide a way to explicitly name the variables introduced by the inversion -- there is no \c{dependent induction x as [..]}. In our proof, we fall back on using the auto-generated names, which is less than ideal. We experienced the fragility of this approach when refactoring, which demonstrates the utility of our other work to avoid generated names.

\subsection{Automation and Repetition}
\label{sec:automation}

We found that repeated sections of proof script could be de-duplicated in one of two ways: either by writing a lemma to encapsulate the common truth, or by using Coq's tactic language (Ltac) to repeat the same steps of reasoning. Of these, we found writing a lemma to be preferable as we found Ltac code more difficult to debug. In this section we discuss the efficacy of the automation employed by our proofs, and suggest possible improvements.

In his book \i{Certified Programming with Dependent Types} \cite{cpdt}, Adam Chlipala advocates:

\i{``The more uninteresting drudge work a proof domain involves, the more important it is to work to prove theorems with single tactics."}
\i{``I like to say that if you find yourself caring about indentation in a proof script, it is a sign that the script is structured poorly."}

This is contrary to many of the steps taken by our proof to remain readable, including the use of case markers and indentation discussed above. This is primarily because constructing a proof in Chlipala's style is easiest when following his approach from the outset. Our proof began in a primarily manual style, with modest uses of the \c{auto} tactic with explicit hint lemmas. Mid-way through the development a hint database was created with an accompanying specialised version of \c{auto} called \c{boom}.

Adapting the proof to use \c{boom} did remove repetition in some places, but was quite time-consuming for the savings achieved. The auto tactic works by repeatedly calling the \c{apply} tactic with lemmas from the hint databases and the current set of hypotheses. Unfortunately, many manual proofs are not structured as a straight-forward series of lemma applications, which means significant effort is often required to adapt proofs for use with auto. In our proofs, rewrites and inversion were the most significant barriers to automation with auto because they don't map cleanly onto stand-alone lemmas. To simulate a rewrite with a lemma requires restating the goal before and after the rewrite, which leads to a lemma for each goal and rewrite-rule pair. Similarly, the outcome of an inversion is usually a set of equalities about the components of the term being inverted, which can be used in a myriad of ways.

That said, the number of rewrites in our proof-of-concept is exacerbated by our choice to keep DbLib's definitions \i{opaque}. By default, DbLib exports the functions for lookups and inserts as opaque to the Coq simplifier. This means that an explicit rewrite is required every time a lookup or insert needs simplification -- the \c{simpl} tactic has no effect. DbLib makes the definitions opaque by default to prevent \i{``fragile simplifications"}, and it is possible to selectively make definitions transparent again. If starting the proof again from scratch or reworking the proof to be more automated, we believe it would be best to make the definitions transparent for the whole project, with opt-in opacity where it is required to deal with odd simplification behaviour. This would remove rewrites and make \c{auto} applicable in more places.

As well as \c{auto}, Coq also includes an \c{autorewrite} tactic for repeatedly rewriting using a collection of rewrite rules. Use of this tactic could be investigated as an alternative to making definitions transparent, although it wouldn't solve the problem of automatically simplifying hypotheses, which we get for free with transparent definitions and the simplify-everywhere tactic, \c{simpl in *}.

Although automating proofs extensively reduces proof-effort and therefore makes larger developments feasible \cite{cpdt}, it is not without downsides. We found debugging Ltac code quite difficult and labour-intensive. For example, Ltac's semicolon operator which makes it possible to ``pipe" the results of tactics into each other, simultaneously makes debugging the middle of a pipeline difficult. The debugging mode for Ltac (\c{Set Ltac Debug}) doesn't show intermediate results in semicolon pipelines and also isn't available in CoqIDE or Vim. This leaves one with no option but to break the pipeline apart into individual tactic applications. If a tactic is failing in only a few branches this also requires the temporary use of the \c{admit} tactic to navigate to the failing cases. Once the error is found and fixed, one then has to glue the tactics back together. This debugging experience is unarguably less than ideal, and is an inevitable consequence of automating heavily using current tools. Our proof walks the line between difficult-to-debug Ltac code and repetitive manual proof, erring slightly on the side of manual proof. Further work could develop improved debugging facilities for Coq tactics, possibly using a graphical interface to convey the branching into different cases.

\section{Summary of Evaluation}

In summary, the library partially satisfies our evaluation criteria for quality of concept and implementation. Conceptually, the library is a success in that it aided us in the formalisation of PLLC. However, PLLC is extremely simple, and consideration of more complex languages like $L^3$ and \SSPHS reveals that the library isn't general enough to be useful in the formalisation of extensions to the linear lambda calculus involving strong destructive updates. That said, we have argued that it is general enough to be of use in a proof of soundness for DILL, and there may be ways to construct compatible new languages with the same features as $L^3$. Further, it may be possible that reasoning about context splitting as we have defined it could be used in a non-syntactic proof of soundness, possibly following the semantic interpretation of types for $L^3$.

From an implementation perspective, the library and proof-of-concept are rather satisfactory. As discussed above, the fragility of automatic variable naming was avoided in all but one case involving the use of dependent induction. Readability of the proofs was simplest to achieve, through case markers and indentation, although the result may still not satisfy Chlipala. More automation and the use of third-party convenience tactics are two areas identified as areas of slight deficiency. It's possible that with these improvements the development would also have been less time consuming.

%\section{Time Efficiency}

%The library and proof-of-concept were both very time consuming to create. We attribute this both to inexperience with Coq's best practices and the fact that DbLib's environment type is ill-suited for proofs about context splitting. The first point, of inexperience, can only be mitigated by practice and study, which have both been provided by this project. The second deserves a more thorough discussion.

\chapter{Conclusion}

We have identified context splitting as a common trait of languages with substructural typing, and have created a Coq library for context splitting, building upon DbLib. This library aided in a syntactic proof of type soundness for a simple variant of the linear lambda calculus, and we are confident that it could be scaled to handle all of Dual Intuitionistic Linear Logic.

Overall we were able to achieve a satisfactory level of quality and expressiveness in the mechanised Coq proofs, although some deficiencies in the use of automatic variable naming and automation could be improved. Further work could look at using custom tactics for dealing with these problems in Coq, or address the more fundamental challenge of building readable and maintainable facilities for proof automation. Following Adam Chlipala's mostly-automated philosophy from the start would be beneficial from an automation standpoint, whilst also solving the issue of variable naming as a side-effect.

Although it was initially hoped that the context splitting library would be useful in the formalisation of more advanced languages, this is probably unrealistic. The first reason for this is that more advanced languages tend to involve type system extensions that obsolete context splitting as we have defined it, or contain so many other features that context splitting only makes up a small fraction of the overall proof effort. Secondly, the approach to context splitting whereby unavailable values are entirely erased is subsumed by an approach based on separation algebras whereby the number of occurrences of a variable is tracked in the typing context. This approach is used to great effect by Pottier for \SSPHS \cite{pottier13}, and McBride for a fusion of linear and dependent typing \cite{mcbride16}.

Pottier's DbLib library for reasoning about de Bruijn indices was found to be highly effective at abstracting over the details of capture-avoiding substitution. Our proofs were able to make effective use of the library by defining only a few language-specific functions. DbLib's environment type, which is not used in Pottier's own work, was also found to be adequate for use with our simple linear language. Some minor additions were made to DbLib while constructing the proof of soundness for PLLC, and further work is required to integrate them into the upstream repository. Alternatively, the proof for PLLC could be restructured so as not to rely on lowering, which was the main addition to DbLib.

In a broad sense, the literature on substructural typing has reached a point of maturity where the creation of practical tools is becoming feasible. The next generation of operating systems, device drivers and language run-times can hopefully be built on the foundation of formal verification. Projects like Adam Chlipala's Bedrock \cite{chlipala11} and Cyclone \cite{grossman05} are already a move in this direction. Further work could build a compiler for a substructural language based on Pottier's \SSPHS \cite{pottier13}, with accompanying verification tools. Cross-over with the impressive world of C verification also seems like it would be productive.

Further, Mozilla's Rust programming language represents an opportunity for substructural typing to break into the mainstream. Although its semantics are yet to be formally specified, the foundations of Cyclone and the other systems surveyed could possibly be adapted for this purpose. Considering this alongside the possibility of verification tools built on linear dependent types as described in the work of McBride \cite{mcbride16}, these are surely exciting times for substructural type systems.

%% chapters in the ``backmatter'' section do not have chapter numbering
%% text in the ``backmatter'' is single spaced
\backmatter
\pagebreak
\bibliographystyle{alpha}
\bibliography{pubs}

\chapter{Appendix}

\section{PLLC Syntax in Coq}
\label{app:pllc-syntax}

To define the terms of PPLC we must first define the set of types. The reason for this is that lambda abstractions are explicitly annotated with the type of their parameter in order to avoid type-inference when writing proofs. A lambda abstraction $(\lambda x : \tau. e)$ is represented by the Coq expression \c{TAbs $\tau$ e}, which hides the name of the binding variable $x$ through the use of de Bruijn indices. The inductive definition for types is:

\begin{coqcode}
Inductive ty : Set :=
  | TyUnit
  | TyPrim : String.string -> ty
  | TyFun : ty -> ty -> ty.
\end{coqcode}

Now, with PLLC's types defined, we can define the set of terms:

\begin{coqcode}
Inductive term : Set :=
  | TUnit
  | TPrim : String.string -> term
  | TVar : nat -> term
  | TAbs : ty -> term -> term
  | TApp : term -> term -> term.
\end{coqcode}

Variables are represented by the \c{TVar} constructor which takes a de Bruijn index representing the variable and constructs a \c{term}. Later when defining the typing judgement we will see that a variable \c{TVar i} is well-typed only if the typing context contains a type at index $i$.

In order to state the progress and preservation lemmas, Coq also needs an idea of which terms are considered \i{values}, i.e. those terms in a form that can not be simplified further. For this, we use an inductive predicate \c{value t} with the following definition:

\begin{coqcode}
Inductive value : term -> Prop :=
  | VUnit : value TUnit
  | VPrim : forall s, value (TPrim s)
  | VVar : forall x, value (TVar x)
  | VAbs : forall t e, value (TAbs t e).
\end{coqcode}

Note that the type of \c{value} is \c{term -> Prop}. Given a \c{term}, \c{t}, a Coq term with type \c{value t} \i{witnesses} the truth of the proposition \c{value t}, which itself has type \c{Prop}. This is in contrast to \c{ty} and \c{term} which have type \c{Set}. Types in Prop are intended to represent proof terms, while those in Set are meant to represent data. We use Prop for all predicates to minimise friction with Coq's standard library, which provides definitions for basic logical connectives operating only on \c{Prop}s.

\section{PLLC Typing Rules in Coq}
\label{app:pllc-typing-rules}

Typing rules determine which terms are considered well-formed. We define an inductive predicate \c{has_type : (env ty) -> term -> ty -> Prop} so that \c{has_type E e t} is inhabited if the environment $E$ determines $e : t$. In standard mathematical notation this is $E \types e : t$, which we mirror using the Coq notation \c{E |- e \textasciitilde: t}.

\begin{coqcode}
Reserved Notation "E '|-' e '~:' t" (at level 40).

Inductive has_type : (env ty) -> term -> ty -> Prop :=
  | HasTyUnit E
      (UnitPre : is_empty E) :
      E |- TUnit ~: TyUnit
  | HasTyPrim E s t
      (PrimPre : is_empty E) :
      E |- TPrim s ~: TyPrim t
  | HasTyVar E x t
      (VarPre : is_empty E) :
      insert x t E |- TVar x ~: t
  | HasTyAbs E e t1 t2
      (AbsPre : (insert 0 t1 E) |- e ~: t2) :
      E |- TAbs t1 e ~: (TyFun t1 t2)
  | HasTyApp E E1 E2 e1 e2 t1 t2
      (AppPreSplit : context_split E E1 E2)
      (AppPreWT1 : E1 |- e1 ~: TyFun t1 t2)
      (AppPreWT2 : E2 |- e2 ~: t1) :
      E  |- TApp e1 e2 ~: t2

where "E '|-' e '~:' t" := (has_type E e t).
\end{coqcode}

Here we see the definitions for emptiness and context splitting coming into play. In linear lambda calculus variables must be used exactly once, so a single variable \c{x} is well-typed only under an environment containing a type for \c{x} and nothing else, as captured by the \c{HasTyVar} rule. Similarly, primitive values must be typed under environments containing no free variables.

A lambda abstraction $(\lambda \hat{0} : \tau. \; e)$ is well-typed if the body can be typed under an environment extended by the type for its binder, $(\hat{0} : \tau)$. This is the only rule that requires the \i{input} context (\c{insert 0 t1 E}) to differ in length to the \i{output} context (\c{E}).

Function applications make use of the \c{context_split} operation to ensure that the variables used to type an application $(e_1 e_2)$ are split between $e_1$ and $e_2$ without duplication. The rule also ensures that the type of the application (\c{t2}) is consistent with the type of the function being applied (\c{t1 -> t2}) and the type of the argument (\c{t1}).

\section{PLLC Operational Semantics in Coq}
\label{app:pllc-op-sems}

The following inductive definition encodes $e \steps e'$ as \c{step e e'}.

\begin{coqcode}
Inductive step : term -> term -> Prop :=
  | StepAppAbs e e' v t
      (BetaPreVal : value v)
      (BetaPreSubst : subst v 0 e = e') :
      step (TApp (TAbs t e) v) e'
  | StepApp1 e1 e1' e2
      (App1Step : step e1 e1') :
      step (TApp e1 e2) (TApp e1' e2)
  | StepApp2 v1 e2 e2'
      (App2Val : value v1)
      (App2Step : step e2 e2') :
      step (TApp v1 e2) (TApp v1 e2').
\end{coqcode}

The first rule, \c{StepAppAbs} is the familiar $\beta$-rule for evaluating the application of an abstraction to a value. It states that function application is equivalent to the substitution of the argument value for the bound variable in the body of the function. For substitution, note the use of DbLib's \c{subst} function.

We are using a \i{call-by-value} evaluation strategy, which means that only the outer-most reducible expressions (\i{redexes}) are reduced, and only when their argument is a value \cite{tapl}. Note that as a result there is no rule for stepping a $\lambda$-abstraction when its body is capable of stepping, as in: $e \steps e' \longrightarrow (\lambda \hat{0} : \tau. \, e) \steps (\lambda \hat{0} : \tau. \, e')$.

\newpage

\section{Complete Source Code}
\label{sec:src}

The complete source code for both the library and the proof of soundness for PLLC is available on GitHub.

\url{https://github.com/michaelsproul/dblib-linear}

The commit-hash for the master branch at the time of writing is:

\c{f4e6fb9becdaa208943ce0d180f2a30ef2f381de}

\end{document}
