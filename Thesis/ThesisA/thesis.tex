\documentclass[]{unswthesis}

\usepackage{hyperref}
\usepackage{proof}
\usepackage{amsmath}

% Disable hideous fluorescent links
\hypersetup{hidelinks = true}

%%% Class options:

%  undergrad (default)
%  hdr

%  11pt (default)
%  12pt

%  final (default)
%  draft

%  oneside (default for hdr)
%  twoside (default for undergrad)


%% Thesis details
\thesistitle{Computer-verified proof of type soundness for the Linear Language with Locations, $L^3$}
\thesisschool{School of Computer Science and Engineering}
\thesisauthor{Michael Alexander Sproul}
\thesisZid{z3484357}
\thesistopic{} % TODO: Find out what this is meant to be?
\thesisdegree{Bachelor of Science (Honours)}
\thesisdate{October 2015}
\thesissupervisor{Dr. Ben Lippmeier}

%% My own LaTeX macros, definitions, etc
\include{definitions}

\begin{document}

%% pages in the ``frontmatter'' section have roman numeral page number
\frontmatter  
\maketitle

% \include{abstract}
% \include{acknowledgements}
% Do I need these?
% \include{abbreviations}

\tableofcontents
%\listoffigures
%\listoftables

%% pages in the ``mainmatter'' section have arabic page numbers and chapters are numbered
\mainmatter

% \include{introduction}
\chapter{Introduction}
\label{ch:intro}

Computer systems form an integral part of modern society, both in the form of personal devices and critical infrastructure. Ensuring the correct operation of computer hardware and software is therefore required. One emerging technique for the construction of robust software systems is the use of mathematical formalisations and proofs of correctness. In this paradigm, desirable properties of the software can be proven true using a computer-based \textit{proof assistant}, which itself relies only on a minimal amount of trusted code. For software formalisation and verification to be truly effective, the objects under consideration must have precise mathematical models associated with them. Typically these models are created based on the \textit{semantics} (meaning) of the programming language that the software is written in. Unfortunately for the would-be software verifier, most popular programming languages lack formal semantics and are therefore not amenable to verification techniques. The focus of this thesis is the computer-based formalisation of language semantics for a specific language (\textit{The Linear Language with Locations} -- $L^3$), as a pre-requisite for further verification of software written in this language.

\section{Operational Semantics}

\section{Type Systems and Type Safety}

\section{Logic, Type Theory and the Coq proof assistant}

\section{Verification Aims}

The $L^3$ language was specified in a 2001 (2005? 2007?) paper by Ahmed et al (REF). The paper includes a hand-written proof of type soundness for $L^3$ core, spanning 8 pages.

Include motivation for linear/uniqueness typing here.

% Background.
\chapter{Background}
\label{ch:intro}

% Overall picture (verified all the way down, focus on low-level languages).
% Focus on "normal" semantics and type systems.
% Notes about TAL

If software verification is to propagate through modern software stacks, verification of components at different layers is a necessity. Proofs about programs written in high-level languages offer only superficial assurances if during compilation down to executable machine code* the program is corrupted by an unverified transformation. Verification of entire computing systems including compilers, operating systems and file-systems is a huge undertaking however, so we restrict our attention here to the type systems of low-level languages.

Historically, low-level \textit{systems software} has been written primarily in the C programming language, which was originally designed without a formal semantics. Attempts to assign semantics to C have been quite successful, and have resulted in numerous impressive verification projects -- notably the CompCert verified compiler (REF) and the seL4 micro-kernel (REF). In this work we consider type systems for languages that could potentially act as verification-friendly successors to the domains where C currently excels (operating systems, language runtimes, embedded devices). Our core hypothesis is that \textit{uniqueness types} and the destructive updates they enable should simplify the verification of systems software.

% FIXME: make this a footnote.
*We don't consider other methods of computation in the present paper, e.g. logic gates, qubits.

\section{Previous Work}

\subsection{Linear and Uniqueness Typing}

Linear, affine and uniqueness typing are closely-related features of type systems that enforce rules about the number of times values may be used and referenced. These restrictions are motivated by several desirable features that can be obtained by enforcing them. The Clean programming language (REF) uses uniqueness typing to ensure that values in memory have at most one reference to them, thus enabling \textit{destructive updates} whilst preserving referential transparency. The Rust programming language (REF) uses uniqueness typing to track and free heap-allocated memory, thus allowing it to achieve memory safety without garbage collection. This makes it suitable for writing systems software where a garbage collector isn't available (like a garbage collector itself, or an operating system).

Linear and affine type systems are based on linear and affine logic respectively (REF Girard and Grishin), via the standard mapping from logic to type systems (see previous section?). In classical and intuitionistic logic there are deduction rules equivalent to the following, called \textit{Contraction} and \textit{Weakening}.

\begin{eqnarray*}
\infer[\text{Contraction}]{\Gamma, A \vdash B}{
	\Gamma, A, A \vdash B
}
\qquad
\infer[\text{Weakening}]{\Gamma, A \vdash B}{
    \Gamma \vdash B
}
\end{eqnarray*}

Contraction allows duplicate assumptions to be discarded, whilst Weakening allows a non-vital extra assumption to be introduced from nowhere. Linear logic bans the use of both rules, such that every assumption is used \textit{exactly once}. Affine logic on the other hand bans only the use of Contraction, which results in the requirement that every assumption be used \textit{at most once}. When transformed into typing rules, Contraction and Weakening take the form:

\begin{eqnarray*}
\infer[\text{Contraction}]{\Gamma, x : A \vdash u[x/y, x/z] :: B}{
	\Gamma, y : A, z : A \vdash u :: B
}
\qquad
\infer[\text{Weakening}]{\Gamma, x : A \vdash y :: B}{
    \Gamma\vdash y :: B
}
\end{eqnarray*}

(Note that the use of substitutions in the rule for Contraction is required so that no variable appears more than once in a typing context $\Gamma$).

As in linear and affine logic, linear type theory prevents the use of Contraction and Weakening, whilst affine type theory prevents just the use of Contraction. Our previous observations about the number of times an assumption is used now translate into observations about the number of times variables are used.

Without Contraction, \textit{variables can only appear once in a term}. The dual substitution of $x$ for $y$ and $x$ for $z$ allows a term containing one $y$ and one $z$ to become a term containing two occurrences of $x$. None of the other rules of intuitionistic type theory allow this (REF Wadler).

Similarly, without Weakening, \textit{all variables in the context must be used in the term}. This follows from the fact that Weakening introduces an unused variable to the context and no other rule of intuitionistic type theory allows this.

From these two observations we can conclude that linear type theory requires all variables to be used \textit{exactly once} in terms, whilst affine type theory requires all variables to be used \textit{at most once} in terms.

\subsubsection{Uniqueness Typing}

% Linear + affine logic and control over contraction and weakening. DONE.

% Uniqueness typing as an alternative (mention dereliction, non guarantees).

% de Vries formalisation of Clean's uniqueness typing using attributes.

\subsection{Systems of Capabilities}

% rgnURAL as a basis for Cyclone.

% Francois Pottier's low-level thing.

% Mezzo. Strong updates, etc.

\subsection{Trust-worthy compilers, typed assembly languages and other similar systems}

\section{The Linear Language with Locations, $L^3$}

\chapter{Proposal}

\section{Variable naming and binding}

One problem that arises frequently in the formalisation of language semantics is that of \textit{capture-avoiding substitution}. Substitution operations, whereby a value is substituted for a variable in a term, form the core computational component of the operational semantics in many languages. In the simply-typed (and untyped) lambda calculus, the $\beta$-rule uses substitution (denoted $e[v/x]$) to describe the semantics of function application:
% FIXME: formatting, long right arrow and space.
\begin{eqnarray*}
(\lambda x : \tau. e) v \Rightarrow_\beta e[v/x]
\end{eqnarray*}

The problem of \textit{variable capture}, which we wish to avoid, is demonstrated by the following example:
% FIXME: formatting
\begin{eqnarray*}
(\lambda x. \lambda y. x + y) y \Rightarrow_\beta (\lambda y. y + y)
\end{eqnarray*}

Here the parameter $y$ is a free variable acting as a place-holder for a value in the environment. After substitution however, the $y$ replacing $x$ in the abstraction body $x + y$ becomes bound due to the name collision between the free $y$ and the binder $y$. Intuitively, drastically altering the meaning of terms during substitution is something we would like to avoid.

One way to avoid variable capture is to forbid the substitution of any terms containing free variables. In such a system, free variables like $y$ are never considered values and as such cannot be used in (variable capturing) substitutions. This is the approach taken by \textit{Software Foundations} (REF) in formalisations of the simply-typed lambda calculus and its variants. A further consequence of this approach is that globally-shared integers or strings for variable names are sufficient to guarantee soundness. Although it's tempting to embrace this approach for its simplifying properties, it doesn't accurately capture the behaviour of common functional languages like Haskell and ML, which perform substitutions whilst avoiding variable capture. (might need a stronger argument here).

In our Coq formalisation of $L^3$ we would therefore like to include \textit{capture avoiding substitution} as part of the definitions of variable names and substitution operations. For this we consider three main approaches from the literature which all exploit the observation that the exact names of bound variables are insignificant at the level of language formalisation. In other words, although the names of variables may hold meaning for the authors of programs, they do not impact the meaning of the programs themselves.

\subsection{Higher-order Abstract Syntax}

\subsection{de Bruijn indices}

\subsection{The Locally Nameless approach}



%\include{background}
%\include{proposal}
%\include{mywork}
%\include{evaluation}
%\include{conclusion}

%% chapters in the ``backmatter'' section do not have chapter numbering
%% text in the ``backmatter'' is single spaced
\backmatter
\bibliographystyle{alpha}
\bibliography{pubs}

%\include{appendix1}
%\include{appendix2}

\end{document}
